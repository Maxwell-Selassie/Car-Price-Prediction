{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e44d6075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ab4afdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded with 4751 rows and 19 features\n",
      "Data successfully loaded with 1356 rows and 19 features\n",
      "Data successfully loaded with 910 rows and 19 features\n"
     ]
    }
   ],
   "source": [
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f'Data successfully loaded with {df.shape[0]} rows and {df.shape[1]} features')\n",
    "    return df\n",
    "\n",
    "train_set = load_data('../data/processed/train_processed_v1.csv')\n",
    "dev_set = load_data('../data/processed/dev_processed_v1.csv')\n",
    "test_set = load_data('../data/processed/test_processed_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ea6e6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_shape: (4751,) - feature_shape: (4751, 18)\n",
      "target_shape: (1356,) - feature_shape: (1356, 18)\n",
      "target_shape: (910,) - feature_shape: (910, 18)\n"
     ]
    }
   ],
   "source": [
    "def split_feature_target(df, target_variable: str = 'selling_price_log'):\n",
    "    y = df[target_variable]\n",
    "    x = df.drop(columns=[target_variable])\n",
    "    print(f'target_shape: {y.shape} - feature_shape: {x.shape}')\n",
    "    return x, y\n",
    "\n",
    "x_train, y_train = split_feature_target(train_set)\n",
    "x_dev, y_dev = split_feature_target(dev_set)\n",
    "x_test, y_test = split_feature_target(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a08cab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import optuna\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db98a0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_duration(seconds: float) -> str:\n",
    "    if seconds < 60:\n",
    "        return f\"{seconds:.2f}s\"\n",
    "    elif seconds < 3600:\n",
    "        minutes = int(seconds // 60)\n",
    "        secs = seconds % 60\n",
    "        return f\"{minutes}m {secs:.2f}s\"\n",
    "    else:\n",
    "        hours = int(seconds // 3600)\n",
    "        minutes = int((seconds % 3600) // 60)\n",
    "        secs = seconds % 60\n",
    "        return f\"{hours}h {minutes}m {secs:.2f}s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9725566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model training\n",
    "baseline_models = {\n",
    "    'Ridge' : Ridge(alpha=0.3, max_iter=2000, random_state=1),\n",
    "    'XGBoost' : XGBRegressor(random_state=42, n_estimators=200, max_depth=3),\n",
    "    'LightGBM' : LGBMRegressor(max_depth=5, learning_rate=0.3, n_estimators=200, random_state=42),\n",
    "    'RandomForest' : RandomForestRegressor(n_estimators=200, max_depth=4, random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16e8423f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "BASELINE TRAINING INITIALIZED\n",
      "==================================================\n",
      "Training Ridge model...\n",
      "Training XGBoost model...\n",
      "Training LightGBM model...\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000315 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 870\n",
      "[LightGBM] [Info] Number of data points in the train set: 4751, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training RandomForest model...\n",
      "Ridge\n",
      "--------------------\n",
      "Time elapsed:  0.00s\n",
      "R^2_Score:  0.842\n",
      "RMSE:  0.3975\n",
      "==============================\n",
      "XGBoost\n",
      "--------------------\n",
      "Time elapsed:  0.19s\n",
      "R^2_Score:  0.947\n",
      "RMSE:  0.2302\n",
      "==============================\n",
      "LightGBM\n",
      "--------------------\n",
      "Time elapsed:  0.16s\n",
      "R^2_Score:  0.9626\n",
      "RMSE:  0.1933\n",
      "==============================\n",
      "RandomForest\n",
      "--------------------\n",
      "Time elapsed:  1.50s\n",
      "R^2_Score:  0.8355\n",
      "RMSE:  0.4056\n",
      "==============================\n",
      "BASELINE MODEL TRAINING COMPLETED\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "print(\"=\"*50)\n",
    "print(f\"BASELINE TRAINING INITIALIZED\")\n",
    "print(\"=\"*50)\n",
    "for name, model in baseline_models.items():\n",
    "    print(f'Training {name} model...')\n",
    "    start_time = time.time()\n",
    "    model.fit(x_train, y_train)\n",
    "    time_elapsed = time.time() - start_time\n",
    "\n",
    "    preds_train = model.predict(x_train)\n",
    "\n",
    "    results[name] = {\n",
    "        \"Time_elapsed\" : format_duration(time_elapsed),\n",
    "        \"R^2_score\" : round(r2_score(y_train, preds_train),4),\n",
    "        \"RMSE\" : round(root_mean_squared_error(y_train, preds_train), 4)\n",
    "    }\n",
    "\n",
    "for name, result in results.items():\n",
    "        print(f\"{name}\")\n",
    "        print('-'*20)\n",
    "        print(\"Time elapsed: \",result['Time_elapsed'])\n",
    "        print(\"R^2_Score: \", result['R^2_score'])\n",
    "        print(\"RMSE: \",result['RMSE'])\n",
    "        print (\"=\"*30)\n",
    "\n",
    "print(f'BASELINE MODEL TRAINING COMPLETED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19b2b5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "CROSS VALIDATION INITIALIZED\n",
      "==================================================\n",
      "Model_name: Ridge\n",
      "========================================\n",
      "CV Scores:  [0.41716765 0.3952506  0.39828129 0.39223018 0.40086055]\n",
      "CV scores_mean:  0.4008\n",
      "CV score_std: +/- 0.0087\n",
      "Time_elapsed:  6.56s\n",
      "========================================\n",
      "Model_name: XGBoost\n",
      "========================================\n",
      "CV Scores:  [0.29129551 0.30042265 0.2986982  0.27874239 0.29366269]\n",
      "CV scores_mean:  0.2926\n",
      "CV score_std: +/- 0.0077\n",
      "Time_elapsed:  4.62s\n",
      "========================================\n",
      "Model_name: LightGBM\n",
      "========================================\n",
      "CV Scores:  [0.28973033 0.30676242 0.3128978  0.29391002 0.30420172]\n",
      "CV scores_mean:  0.3015\n",
      "CV score_std: +/- 0.0085\n",
      "Time_elapsed:  3.87s\n",
      "========================================\n",
      "Model_name: RandomForest\n",
      "========================================\n",
      "CV Scores:  [0.44352197 0.41886004 0.40985105 0.42159388 0.41798678]\n",
      "CV scores_mean:  0.4224\n",
      "CV score_std: +/- 0.0113\n",
      "Time_elapsed:  1.57s\n",
      "========================================\n",
      "CROSS-VALIDATION MODEL TRAINING COMPLETED\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "print(\"=\"*50)\n",
    "print(f\"CROSS VALIDATION INITIALIZED\")\n",
    "print(f\"=\"*50)\n",
    "\n",
    "for name, model in baseline_models.items():\n",
    "    start_time = time.time()\n",
    "    cv_scores = cross_val_score(model, x_train, y_train, cv=KFold(5), scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "    time_elapsed = time.time() - start_time\n",
    "\n",
    "    abs_cv_score = abs(cv_scores)\n",
    "    \n",
    "    results[name] = {\n",
    "        'cv_scores_across_folds' : abs_cv_score,\n",
    "        'cv_score_mean' : round(abs_cv_score.mean(), 4),\n",
    "        'cv_score_std' : round(abs_cv_score.std(), 4),\n",
    "        'Time_elapsed' : format_duration(time_elapsed)\n",
    "    }\n",
    "\n",
    "for name, result in results.items():\n",
    "    print(f'Model_name: {name}')\n",
    "    print(f'='*40)\n",
    "    print(f'CV Scores: ',result['cv_scores_across_folds'])\n",
    "    print(f'CV scores_mean: ',result['cv_score_mean'])\n",
    "    print(f'CV score_std: +/-',result['cv_score_std'])\n",
    "    print(f'Time_elapsed: ',result['Time_elapsed'])\n",
    "    print(f'='*40)\n",
    "\n",
    "print(f'CROSS-VALIDATION MODEL TRAINING COMPLETED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99311f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad658dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "            n_estimators = trial.suggest_int('n_estimators', 100,1000)\n",
    "            max_depth = trial.suggest_int('max_depth', 3, 15)\n",
    "            learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 0.3)\n",
    "            min_child_weight = trial.suggest_int('min_child_weight', 1 , 10)\n",
    "\n",
    "            xgbr = XGBRegressor(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                learning_rate=learning_rate,\n",
    "                min_child_weight=min_child_weight,\n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "\n",
    "            scores = cross_val_score(\n",
    "                    estimator=xgbr,\n",
    "                    X=x_train, y=y_train,\n",
    "                    cv=KFold(5),scoring='neg_mean_squared_error',\n",
    "                    n_jobs=-1,\n",
    "            )\n",
    "\n",
    "            return -scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53922965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-01-21 16:07:13,991]\u001b[0m A new study created in memory with name: no-name-2b535705-1135-4482-8846-3e83ab50ba39\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:07:20,520]\u001b[0m Trial 0 finished with value: 0.08539019019252586 and parameters: {'n_estimators': 425, 'max_depth': 4, 'learning_rate': 0.08978658960089328, 'min_child_weight': 8}. Best is trial 0 with value: 0.08539019019252586.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:07:26,413]\u001b[0m Trial 1 finished with value: 0.09495197683593737 and parameters: {'n_estimators': 662, 'max_depth': 12, 'learning_rate': 0.04225670585219596, 'min_child_weight': 9}. Best is trial 0 with value: 0.08539019019252586.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:07:26,957]\u001b[0m Trial 2 finished with value: 0.08561203246811563 and parameters: {'n_estimators': 390, 'max_depth': 6, 'learning_rate': 0.09152415967258339, 'min_child_weight': 10}. Best is trial 0 with value: 0.08539019019252586.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:07:29,506]\u001b[0m Trial 3 finished with value: 0.10226171316013058 and parameters: {'n_estimators': 911, 'max_depth': 12, 'learning_rate': 0.08499608012413307, 'min_child_weight': 9}. Best is trial 0 with value: 0.08539019019252586.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:07:30,188]\u001b[0m Trial 4 finished with value: 0.09436835513192324 and parameters: {'n_estimators': 753, 'max_depth': 4, 'learning_rate': 0.017248301793842766, 'min_child_weight': 8}. Best is trial 0 with value: 0.08539019019252586.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:07:30,500]\u001b[0m Trial 5 finished with value: 0.08442866030728638 and parameters: {'n_estimators': 419, 'max_depth': 3, 'learning_rate': 0.14454227768925998, 'min_child_weight': 5}. Best is trial 5 with value: 0.08442866030728638.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:07:31,587]\u001b[0m Trial 6 finished with value: 0.0882806271297641 and parameters: {'n_estimators': 684, 'max_depth': 6, 'learning_rate': 0.013405067306329557, 'min_child_weight': 6}. Best is trial 5 with value: 0.08442866030728638.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:07:32,219]\u001b[0m Trial 7 finished with value: 0.09259418891274053 and parameters: {'n_estimators': 915, 'max_depth': 3, 'learning_rate': 0.02678845034771573, 'min_child_weight': 9}. Best is trial 5 with value: 0.08442866030728638.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:07:35,433]\u001b[0m Trial 8 finished with value: 0.0936506610503796 and parameters: {'n_estimators': 335, 'max_depth': 15, 'learning_rate': 0.01097092238863852, 'min_child_weight': 8}. Best is trial 5 with value: 0.08442866030728638.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:07:36,077]\u001b[0m Trial 9 finished with value: 0.08905616282810194 and parameters: {'n_estimators': 195, 'max_depth': 8, 'learning_rate': 0.07777355373762264, 'min_child_weight': 2}. Best is trial 5 with value: 0.08442866030728638.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:07:36,856]\u001b[0m Trial 10 finished with value: 0.10070957535224619 and parameters: {'n_estimators': 110, 'max_depth': 10, 'learning_rate': 0.27267565225837204, 'min_child_weight': 2}. Best is trial 5 with value: 0.08442866030728638.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:07:37,219]\u001b[0m Trial 11 finished with value: 0.0849354185113468 and parameters: {'n_estimators': 473, 'max_depth': 3, 'learning_rate': 0.19045697560007604, 'min_child_weight': 5}. Best is trial 5 with value: 0.08442866030728638.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:07:37,942]\u001b[0m Trial 12 finished with value: 0.09538829459268595 and parameters: {'n_estimators': 518, 'max_depth': 6, 'learning_rate': 0.22790075635283982, 'min_child_weight': 4}. Best is trial 5 with value: 0.08442866030728638.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:07:38,178]\u001b[0m Trial 13 finished with value: 0.08532982608602123 and parameters: {'n_estimators': 276, 'max_depth': 3, 'learning_rate': 0.1617740873548449, 'min_child_weight': 5}. Best is trial 5 with value: 0.08442866030728638.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:07:39,453]\u001b[0m Trial 14 finished with value: 0.09802117955542877 and parameters: {'n_estimators': 530, 'max_depth': 8, 'learning_rate': 0.1505512620673958, 'min_child_weight': 4}. Best is trial 5 with value: 0.08442866030728638.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:07:39,973]\u001b[0m Trial 15 finished with value: 0.08861113027563677 and parameters: {'n_estimators': 460, 'max_depth': 5, 'learning_rate': 0.154721973803218, 'min_child_weight': 6}. Best is trial 5 with value: 0.08442866030728638.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:07:40,454]\u001b[0m Trial 16 finished with value: 0.08782249655960989 and parameters: {'n_estimators': 645, 'max_depth': 3, 'learning_rate': 0.05256557470451341, 'min_child_weight': 4}. Best is trial 5 with value: 0.08442866030728638.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:07:41,423]\u001b[0m Trial 17 finished with value: 0.09671218874633744 and parameters: {'n_estimators': 273, 'max_depth': 8, 'learning_rate': 0.19186452535726453, 'min_child_weight': 1}. Best is trial 5 with value: 0.08442866030728638.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:07:43,184]\u001b[0m Trial 18 finished with value: 0.10056915412131459 and parameters: {'n_estimators': 587, 'max_depth': 10, 'learning_rate': 0.11733138927856819, 'min_child_weight': 5}. Best is trial 5 with value: 0.08442866030728638.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:07:44,033]\u001b[0m Trial 19 finished with value: 0.08463494265724372 and parameters: {'n_estimators': 805, 'max_depth': 5, 'learning_rate': 0.03726403894408621, 'min_child_weight': 7}. Best is trial 5 with value: 0.08442866030728638.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:07:44,940]\u001b[0m Trial 20 finished with value: 0.08550715408794303 and parameters: {'n_estimators': 756, 'max_depth': 5, 'learning_rate': 0.02520961766152917, 'min_child_weight': 7}. Best is trial 5 with value: 0.08442866030728638.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:07:45,688]\u001b[0m Trial 21 finished with value: 0.08624021489163447 and parameters: {'n_estimators': 853, 'max_depth': 4, 'learning_rate': 0.035327354565561804, 'min_child_weight': 7}. Best is trial 5 with value: 0.08442866030728638.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:07:46,267]\u001b[0m Trial 22 finished with value: 0.08421050029548485 and parameters: {'n_estimators': 492, 'max_depth': 5, 'learning_rate': 0.0610564159249591, 'min_child_weight': 3}. Best is trial 22 with value: 0.08421050029548485.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:07:47,468]\u001b[0m Trial 23 finished with value: 0.0894877666245822 and parameters: {'n_estimators': 591, 'max_depth': 7, 'learning_rate': 0.05832186435832847, 'min_child_weight': 3}. Best is trial 22 with value: 0.08421050029548485.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:07:48,575]\u001b[0m Trial 24 finished with value: 0.08522084111774414 and parameters: {'n_estimators': 975, 'max_depth': 5, 'learning_rate': 0.054532236488827504, 'min_child_weight': 3}. Best is trial 22 with value: 0.08421050029548485.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:07:49,972]\u001b[0m Trial 25 finished with value: 0.08664087579640814 and parameters: {'n_estimators': 780, 'max_depth': 7, 'learning_rate': 0.02967135026923937, 'min_child_weight': 7}. Best is trial 22 with value: 0.08421050029548485.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:07:50,470]\u001b[0m Trial 26 finished with value: 0.09277247714187424 and parameters: {'n_estimators': 369, 'max_depth': 5, 'learning_rate': 0.0222935837699687, 'min_child_weight': 3}. Best is trial 22 with value: 0.08421050029548485.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:07:50,898]\u001b[0m Trial 27 finished with value: 0.08890925915171902 and parameters: {'n_estimators': 310, 'max_depth': 4, 'learning_rate': 0.06690915411666694, 'min_child_weight': 6}. Best is trial 22 with value: 0.08421050029548485.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:07:52,320]\u001b[0m Trial 28 finished with value: 0.08679846383946271 and parameters: {'n_estimators': 482, 'max_depth': 7, 'learning_rate': 0.04223229255158766, 'min_child_weight': 1}. Best is trial 22 with value: 0.08421050029548485.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:07:52,727]\u001b[0m Trial 29 finished with value: 0.08434924485492348 and parameters: {'n_estimators': 412, 'max_depth': 4, 'learning_rate': 0.12148859170414321, 'min_child_weight': 4}. Best is trial 22 with value: 0.08421050029548485.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:07:53,142]\u001b[0m Trial 30 finished with value: 0.08332985614930138 and parameters: {'n_estimators': 423, 'max_depth': 4, 'learning_rate': 0.11249292670209345, 'min_child_weight': 2}. Best is trial 30 with value: 0.08332985614930138.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:07:53,584]\u001b[0m Trial 31 finished with value: 0.08381487607339143 and parameters: {'n_estimators': 431, 'max_depth': 4, 'learning_rate': 0.115050939635253, 'min_child_weight': 2}. Best is trial 30 with value: 0.08332985614930138.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:07:54,105]\u001b[0m Trial 32 finished with value: 0.08381121898840153 and parameters: {'n_estimators': 433, 'max_depth': 4, 'learning_rate': 0.10282474929939976, 'min_child_weight': 2}. Best is trial 30 with value: 0.08332985614930138.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:07:54,578]\u001b[0m Trial 33 finished with value: 0.08492498104511295 and parameters: {'n_estimators': 215, 'max_depth': 6, 'learning_rate': 0.1023454873561578, 'min_child_weight': 2}. Best is trial 30 with value: 0.08332985614930138.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:07:55,163]\u001b[0m Trial 34 finished with value: 0.08351886332682175 and parameters: {'n_estimators': 580, 'max_depth': 4, 'learning_rate': 0.07595117182921536, 'min_child_weight': 1}. Best is trial 30 with value: 0.08332985614930138.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:02,263]\u001b[0m Trial 35 finished with value: 0.1065370630575128 and parameters: {'n_estimators': 628, 'max_depth': 14, 'learning_rate': 0.07723971680912345, 'min_child_weight': 1}. Best is trial 30 with value: 0.08332985614930138.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:02,904]\u001b[0m Trial 36 finished with value: 0.08407738487249838 and parameters: {'n_estimators': 569, 'max_depth': 4, 'learning_rate': 0.10283928424506285, 'min_child_weight': 2}. Best is trial 30 with value: 0.08332985614930138.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:07,010]\u001b[0m Trial 37 finished with value: 0.10418987847943002 and parameters: {'n_estimators': 372, 'max_depth': 11, 'learning_rate': 0.12182892124864252, 'min_child_weight': 1}. Best is trial 30 with value: 0.08332985614930138.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:07,570]\u001b[0m Trial 38 finished with value: 0.08315042884967586 and parameters: {'n_estimators': 692, 'max_depth': 3, 'learning_rate': 0.09093076376412754, 'min_child_weight': 2}. Best is trial 38 with value: 0.08315042884967586.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:08,169]\u001b[0m Trial 39 finished with value: 0.08360267337033281 and parameters: {'n_estimators': 716, 'max_depth': 3, 'learning_rate': 0.09024050537160284, 'min_child_weight': 1}. Best is trial 38 with value: 0.08315042884967586.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:08,809]\u001b[0m Trial 40 finished with value: 0.08344470964379247 and parameters: {'n_estimators': 720, 'max_depth': 3, 'learning_rate': 0.08504677843784278, 'min_child_weight': 1}. Best is trial 38 with value: 0.08315042884967586.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:09,552]\u001b[0m Trial 41 finished with value: 0.08381392901444998 and parameters: {'n_estimators': 711, 'max_depth': 3, 'learning_rate': 0.08526438478797285, 'min_child_weight': 1}. Best is trial 38 with value: 0.08315042884967586.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:10,453]\u001b[0m Trial 42 finished with value: 0.0857432511381672 and parameters: {'n_estimators': 700, 'max_depth': 3, 'learning_rate': 0.07171610758179825, 'min_child_weight': 1}. Best is trial 38 with value: 0.08315042884967586.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:11,143]\u001b[0m Trial 43 finished with value: 0.08371432932413264 and parameters: {'n_estimators': 732, 'max_depth': 3, 'learning_rate': 0.09038952555685499, 'min_child_weight': 1}. Best is trial 38 with value: 0.08315042884967586.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:11,749]\u001b[0m Trial 44 finished with value: 0.08615883393220121 and parameters: {'n_estimators': 836, 'max_depth': 3, 'learning_rate': 0.050090028121262106, 'min_child_weight': 2}. Best is trial 38 with value: 0.08315042884967586.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:12,408]\u001b[0m Trial 45 finished with value: 0.08428574598953223 and parameters: {'n_estimators': 626, 'max_depth': 4, 'learning_rate': 0.0680388176581034, 'min_child_weight': 3}. Best is trial 38 with value: 0.08315042884967586.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:13,430]\u001b[0m Trial 46 finished with value: 0.0897804337144529 and parameters: {'n_estimators': 678, 'max_depth': 6, 'learning_rate': 0.135195239675003, 'min_child_weight': 10}. Best is trial 38 with value: 0.08315042884967586.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:18,849]\u001b[0m Trial 47 finished with value: 0.10368442923134472 and parameters: {'n_estimators': 893, 'max_depth': 13, 'learning_rate': 0.08695949298730145, 'min_child_weight': 2}. Best is trial 38 with value: 0.08315042884967586.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:19,385]\u001b[0m Trial 48 finished with value: 0.08419051036454686 and parameters: {'n_estimators': 667, 'max_depth': 3, 'learning_rate': 0.18448885399020545, 'min_child_weight': 1}. Best is trial 38 with value: 0.08315042884967586.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:20,384]\u001b[0m Trial 49 finished with value: 0.08666228717253156 and parameters: {'n_estimators': 546, 'max_depth': 6, 'learning_rate': 0.07610270793434759, 'min_child_weight': 2}. Best is trial 38 with value: 0.08315042884967586.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:21,039]\u001b[0m Trial 50 finished with value: 0.08539823364234111 and parameters: {'n_estimators': 607, 'max_depth': 4, 'learning_rate': 0.0461730771330967, 'min_child_weight': 1}. Best is trial 38 with value: 0.08315042884967586.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:21,649]\u001b[0m Trial 51 finished with value: 0.08398322451750445 and parameters: {'n_estimators': 731, 'max_depth': 3, 'learning_rate': 0.09265727991103724, 'min_child_weight': 1}. Best is trial 38 with value: 0.08315042884967586.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:22,337]\u001b[0m Trial 52 finished with value: 0.08324196548934296 and parameters: {'n_estimators': 765, 'max_depth': 3, 'learning_rate': 0.09489057507106707, 'min_child_weight': 1}. Best is trial 38 with value: 0.08315042884967586.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:22,981]\u001b[0m Trial 53 finished with value: 0.08498755749166624 and parameters: {'n_estimators': 778, 'max_depth': 3, 'learning_rate': 0.06180117209397851, 'min_child_weight': 2}. Best is trial 38 with value: 0.08315042884967586.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:23,826]\u001b[0m Trial 54 finished with value: 0.08592689223502362 and parameters: {'n_estimators': 831, 'max_depth': 4, 'learning_rate': 0.13813118130903948, 'min_child_weight': 1}. Best is trial 38 with value: 0.08315042884967586.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:24,719]\u001b[0m Trial 55 finished with value: 0.08643366899645041 and parameters: {'n_estimators': 644, 'max_depth': 5, 'learning_rate': 0.09583258825663805, 'min_child_weight': 3}. Best is trial 38 with value: 0.08315042884967586.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:25,400]\u001b[0m Trial 56 finished with value: 0.0835047463204986 and parameters: {'n_estimators': 762, 'max_depth': 3, 'learning_rate': 0.07947726297966977, 'min_child_weight': 2}. Best is trial 38 with value: 0.08315042884967586.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:26,763]\u001b[0m Trial 57 finished with value: 0.09288435334111321 and parameters: {'n_estimators': 894, 'max_depth': 5, 'learning_rate': 0.1722617953681521, 'min_child_weight': 2}. Best is trial 38 with value: 0.08315042884967586.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:27,441]\u001b[0m Trial 58 finished with value: 0.08424549859482734 and parameters: {'n_estimators': 762, 'max_depth': 3, 'learning_rate': 0.07739471993573048, 'min_child_weight': 4}. Best is trial 38 with value: 0.08315042884967586.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:28,020]\u001b[0m Trial 59 finished with value: 0.08502893317695907 and parameters: {'n_estimators': 517, 'max_depth': 4, 'learning_rate': 0.1075160121036574, 'min_child_weight': 3}. Best is trial 38 with value: 0.08315042884967586.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:29,400]\u001b[0m Trial 60 finished with value: 0.0890072310285759 and parameters: {'n_estimators': 861, 'max_depth': 5, 'learning_rate': 0.13028686825818497, 'min_child_weight': 2}. Best is trial 38 with value: 0.08315042884967586.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:30,111]\u001b[0m Trial 61 finished with value: 0.08354751438906698 and parameters: {'n_estimators': 795, 'max_depth': 3, 'learning_rate': 0.08460950328385586, 'min_child_weight': 1}. Best is trial 38 with value: 0.08315042884967586.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:30,867]\u001b[0m Trial 62 finished with value: 0.08481137014593011 and parameters: {'n_estimators': 817, 'max_depth': 3, 'learning_rate': 0.06569259144917551, 'min_child_weight': 1}. Best is trial 38 with value: 0.08315042884967586.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:31,635]\u001b[0m Trial 63 finished with value: 0.08285306771082904 and parameters: {'n_estimators': 800, 'max_depth': 3, 'learning_rate': 0.08077242352373684, 'min_child_weight': 2}. Best is trial 63 with value: 0.08285306771082904.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:32,734]\u001b[0m Trial 64 finished with value: 0.08435363262967716 and parameters: {'n_estimators': 976, 'max_depth': 4, 'learning_rate': 0.05685210698664559, 'min_child_weight': 3}. Best is trial 63 with value: 0.08285306771082904.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:33,583]\u001b[0m Trial 65 finished with value: 0.08440017469452545 and parameters: {'n_estimators': 746, 'max_depth': 4, 'learning_rate': 0.11135592890465919, 'min_child_weight': 2}. Best is trial 63 with value: 0.08285306771082904.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:36,326]\u001b[0m Trial 66 finished with value: 0.10234792567968147 and parameters: {'n_estimators': 693, 'max_depth': 9, 'learning_rate': 0.22364142672437978, 'min_child_weight': 2}. Best is trial 63 with value: 0.08285306771082904.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:37,414]\u001b[0m Trial 67 finished with value: 0.08439105115445075 and parameters: {'n_estimators': 929, 'max_depth': 4, 'learning_rate': 0.07219916763828681, 'min_child_weight': 3}. Best is trial 63 with value: 0.08285306771082904.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:38,236]\u001b[0m Trial 68 finished with value: 0.08367204082049616 and parameters: {'n_estimators': 805, 'max_depth': 3, 'learning_rate': 0.08238202829404043, 'min_child_weight': 2}. Best is trial 63 with value: 0.08285306771082904.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:39,228]\u001b[0m Trial 69 finished with value: 0.08674702124077736 and parameters: {'n_estimators': 859, 'max_depth': 4, 'learning_rate': 0.15527121101810207, 'min_child_weight': 9}. Best is trial 63 with value: 0.08285306771082904.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:39,935]\u001b[0m Trial 70 finished with value: 0.08364722764310903 and parameters: {'n_estimators': 650, 'max_depth': 3, 'learning_rate': 0.09802483535771658, 'min_child_weight': 1}. Best is trial 63 with value: 0.08285306771082904.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:40,724]\u001b[0m Trial 71 finished with value: 0.08339834509471708 and parameters: {'n_estimators': 793, 'max_depth': 3, 'learning_rate': 0.08405483104230632, 'min_child_weight': 1}. Best is trial 63 with value: 0.08285306771082904.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:41,516]\u001b[0m Trial 72 finished with value: 0.08458083145212651 and parameters: {'n_estimators': 786, 'max_depth': 3, 'learning_rate': 0.0675263469060003, 'min_child_weight': 1}. Best is trial 63 with value: 0.08285306771082904.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:42,755]\u001b[0m Trial 73 finished with value: 0.08730137187390588 and parameters: {'n_estimators': 758, 'max_depth': 5, 'learning_rate': 0.12299342371480639, 'min_child_weight': 2}. Best is trial 63 with value: 0.08285306771082904.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:43,503]\u001b[0m Trial 74 finished with value: 0.08347106168854283 and parameters: {'n_estimators': 577, 'max_depth': 4, 'learning_rate': 0.07981864573742056, 'min_child_weight': 1}. Best is trial 63 with value: 0.08285306771082904.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:44,323]\u001b[0m Trial 75 finished with value: 0.10497466632612777 and parameters: {'n_estimators': 726, 'max_depth': 3, 'learning_rate': 0.014092989635293521, 'min_child_weight': 2}. Best is trial 63 with value: 0.08285306771082904.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:44,566]\u001b[0m Trial 76 finished with value: 0.09500932690462209 and parameters: {'n_estimators': 105, 'max_depth': 4, 'learning_rate': 0.1086039143559855, 'min_child_weight': 1}. Best is trial 63 with value: 0.08285306771082904.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:45,414]\u001b[0m Trial 77 finished with value: 0.08627061663570071 and parameters: {'n_estimators': 689, 'max_depth': 3, 'learning_rate': 0.060595925185273916, 'min_child_weight': 2}. Best is trial 63 with value: 0.08285306771082904.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:46,511]\u001b[0m Trial 78 finished with value: 0.08361465461377242 and parameters: {'n_estimators': 771, 'max_depth': 4, 'learning_rate': 0.05198491090905934, 'min_child_weight': 1}. Best is trial 63 with value: 0.08285306771082904.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:48,072]\u001b[0m Trial 79 finished with value: 0.0862346237864177 and parameters: {'n_estimators': 829, 'max_depth': 5, 'learning_rate': 0.0817185794344923, 'min_child_weight': 3}. Best is trial 63 with value: 0.08285306771082904.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:48,617]\u001b[0m Trial 80 finished with value: 0.08576719136060315 and parameters: {'n_estimators': 451, 'max_depth': 3, 'learning_rate': 0.09718350156459914, 'min_child_weight': 1}. Best is trial 63 with value: 0.08285306771082904.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:49,521]\u001b[0m Trial 81 finished with value: 0.08392691309042451 and parameters: {'n_estimators': 577, 'max_depth': 4, 'learning_rate': 0.07307146616511602, 'min_child_weight': 1}. Best is trial 63 with value: 0.08285306771082904.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:50,100]\u001b[0m Trial 82 finished with value: 0.08667835916016219 and parameters: {'n_estimators': 511, 'max_depth': 3, 'learning_rate': 0.07916804669993499, 'min_child_weight': 2}. Best is trial 63 with value: 0.08285306771082904.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:50,933]\u001b[0m Trial 83 finished with value: 0.0832016218464397 and parameters: {'n_estimators': 560, 'max_depth': 4, 'learning_rate': 0.08640028784086477, 'min_child_weight': 1}. Best is trial 63 with value: 0.08285306771082904.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:51,817]\u001b[0m Trial 84 finished with value: 0.08360618571881825 and parameters: {'n_estimators': 536, 'max_depth': 4, 'learning_rate': 0.09448692513835313, 'min_child_weight': 1}. Best is trial 63 with value: 0.08285306771082904.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:58,386]\u001b[0m Trial 85 finished with value: 0.10570571526403402 and parameters: {'n_estimators': 604, 'max_depth': 15, 'learning_rate': 0.11139774869374261, 'min_child_weight': 2}. Best is trial 63 with value: 0.08285306771082904.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:08:59,297]\u001b[0m Trial 86 finished with value: 0.08912974625484643 and parameters: {'n_estimators': 493, 'max_depth': 3, 'learning_rate': 0.06425382491006983, 'min_child_weight': 1}. Best is trial 63 with value: 0.08285306771082904.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:09:01,062]\u001b[0m Trial 87 finished with value: 0.08794829675893581 and parameters: {'n_estimators': 745, 'max_depth': 5, 'learning_rate': 0.12268207362193763, 'min_child_weight': 2}. Best is trial 63 with value: 0.08285306771082904.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:09:07,080]\u001b[0m Trial 88 finished with value: 0.10369587843510582 and parameters: {'n_estimators': 562, 'max_depth': 11, 'learning_rate': 0.14473459322293603, 'min_child_weight': 1}. Best is trial 63 with value: 0.08285306771082904.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:09:08,407]\u001b[0m Trial 89 finished with value: 0.08450059575739881 and parameters: {'n_estimators': 620, 'max_depth': 3, 'learning_rate': 0.08869978895266002, 'min_child_weight': 1}. Best is trial 63 with value: 0.08285306771082904.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:09:11,272]\u001b[0m Trial 90 finished with value: 0.08564046243395204 and parameters: {'n_estimators': 667, 'max_depth': 6, 'learning_rate': 0.04681430509551698, 'min_child_weight': 5}. Best is trial 63 with value: 0.08285306771082904.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:09:12,899]\u001b[0m Trial 91 finished with value: 0.08425998493872724 and parameters: {'n_estimators': 552, 'max_depth': 4, 'learning_rate': 0.07118296299886646, 'min_child_weight': 1}. Best is trial 63 with value: 0.08285306771082904.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:09:14,019]\u001b[0m Trial 92 finished with value: 0.08328057039506313 and parameters: {'n_estimators': 405, 'max_depth': 4, 'learning_rate': 0.10047759061246937, 'min_child_weight': 1}. Best is trial 63 with value: 0.08285306771082904.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:09:15,468]\u001b[0m Trial 93 finished with value: 0.08414219287159663 and parameters: {'n_estimators': 393, 'max_depth': 4, 'learning_rate': 0.09958471917536217, 'min_child_weight': 2}. Best is trial 63 with value: 0.08285306771082904.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:09:16,791]\u001b[0m Trial 94 finished with value: 0.08939263864603345 and parameters: {'n_estimators': 364, 'max_depth': 3, 'learning_rate': 0.08865499678568776, 'min_child_weight': 1}. Best is trial 63 with value: 0.08285306771082904.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:09:19,635]\u001b[0m Trial 95 finished with value: 0.0846428294515632 and parameters: {'n_estimators': 448, 'max_depth': 5, 'learning_rate': 0.10271048638980614, 'min_child_weight': 1}. Best is trial 63 with value: 0.08285306771082904.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:09:22,216]\u001b[0m Trial 96 finished with value: 0.08165961576392188 and parameters: {'n_estimators': 708, 'max_depth': 3, 'learning_rate': 0.1305406029660337, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:09:23,471]\u001b[0m Trial 97 finished with value: 0.08488512104472777 and parameters: {'n_estimators': 249, 'max_depth': 4, 'learning_rate': 0.13579984952374738, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:09:24,693]\u001b[0m Trial 98 finished with value: 0.0864821733201315 and parameters: {'n_estimators': 303, 'max_depth': 3, 'learning_rate': 0.128351553549392, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:09:27,106]\u001b[0m Trial 99 finished with value: 0.0844953596275288 and parameters: {'n_estimators': 391, 'max_depth': 4, 'learning_rate': 0.16336367009525457, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:09:29,546]\u001b[0m Trial 100 finished with value: 0.08312414300199703 and parameters: {'n_estimators': 711, 'max_depth': 3, 'learning_rate': 0.11684755278935727, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:09:32,225]\u001b[0m Trial 101 finished with value: 0.08302348043185244 and parameters: {'n_estimators': 707, 'max_depth': 3, 'learning_rate': 0.11487771270054614, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:09:34,985]\u001b[0m Trial 102 finished with value: 0.08349016143148591 and parameters: {'n_estimators': 709, 'max_depth': 3, 'learning_rate': 0.1160674269691832, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:09:37,477]\u001b[0m Trial 103 finished with value: 0.08273551529377218 and parameters: {'n_estimators': 731, 'max_depth': 3, 'learning_rate': 0.10363236931060575, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:09:38,767]\u001b[0m Trial 104 finished with value: 0.08712664607608565 and parameters: {'n_estimators': 349, 'max_depth': 3, 'learning_rate': 0.10486902407350014, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:09:41,307]\u001b[0m Trial 105 finished with value: 0.08335508002367598 and parameters: {'n_estimators': 740, 'max_depth': 3, 'learning_rate': 0.11565471390482743, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:09:43,634]\u001b[0m Trial 106 finished with value: 0.0833304450341904 and parameters: {'n_estimators': 676, 'max_depth': 3, 'learning_rate': 0.1467339355171277, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:09:45,783]\u001b[0m Trial 107 finished with value: 0.08274977982394807 and parameters: {'n_estimators': 650, 'max_depth': 3, 'learning_rate': 0.14032232137911704, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:09:48,343]\u001b[0m Trial 108 finished with value: 0.08723091108735959 and parameters: {'n_estimators': 688, 'max_depth': 4, 'learning_rate': 0.1716096310857956, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:09:56,435]\u001b[0m Trial 109 finished with value: 0.10129893784474617 and parameters: {'n_estimators': 635, 'max_depth': 9, 'learning_rate': 0.13197299451598388, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:09:58,003]\u001b[0m Trial 110 finished with value: 0.08328120589468982 and parameters: {'n_estimators': 651, 'max_depth': 3, 'learning_rate': 0.19686211973582013, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:09:59,938]\u001b[0m Trial 111 finished with value: 0.08518771642746378 and parameters: {'n_estimators': 704, 'max_depth': 3, 'learning_rate': 0.21284438682818743, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:01,629]\u001b[0m Trial 112 finished with value: 0.08417473164618214 and parameters: {'n_estimators': 718, 'max_depth': 3, 'learning_rate': 0.2573362514157516, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:03,621]\u001b[0m Trial 113 finished with value: 0.08544436252688223 and parameters: {'n_estimators': 663, 'max_depth': 4, 'learning_rate': 0.18470150049441122, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:05,117]\u001b[0m Trial 114 finished with value: 0.084259756018591 and parameters: {'n_estimators': 654, 'max_depth': 3, 'learning_rate': 0.12342352819145197, 'min_child_weight': 6}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:06,408]\u001b[0m Trial 115 finished with value: 0.08477727688870212 and parameters: {'n_estimators': 603, 'max_depth': 3, 'learning_rate': 0.19835927604465456, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:07,541]\u001b[0m Trial 116 finished with value: 0.0843732621596494 and parameters: {'n_estimators': 420, 'max_depth': 4, 'learning_rate': 0.15550711222825636, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:09,158]\u001b[0m Trial 117 finished with value: 0.08238179744888834 and parameters: {'n_estimators': 697, 'max_depth': 3, 'learning_rate': 0.1417590430315001, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:10,924]\u001b[0m Trial 118 finished with value: 0.0835201737237383 and parameters: {'n_estimators': 733, 'max_depth': 3, 'learning_rate': 0.1411043833680087, 'min_child_weight': 8}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:12,409]\u001b[0m Trial 119 finished with value: 0.08440188316439623 and parameters: {'n_estimators': 700, 'max_depth': 3, 'learning_rate': 0.25715681280501174, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:13,708]\u001b[0m Trial 120 finished with value: 0.0826963136261387 and parameters: {'n_estimators': 642, 'max_depth': 3, 'learning_rate': 0.1077450816752893, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:15,022]\u001b[0m Trial 121 finished with value: 0.08325320855427412 and parameters: {'n_estimators': 642, 'max_depth': 3, 'learning_rate': 0.10724059566697194, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:16,234]\u001b[0m Trial 122 finished with value: 0.08232280947837918 and parameters: {'n_estimators': 670, 'max_depth': 3, 'learning_rate': 0.10542742723185726, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:17,558]\u001b[0m Trial 123 finished with value: 0.08313899416292488 and parameters: {'n_estimators': 624, 'max_depth': 3, 'learning_rate': 0.10997590253386248, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:18,660]\u001b[0m Trial 124 finished with value: 0.0827606041580992 and parameters: {'n_estimators': 619, 'max_depth': 3, 'learning_rate': 0.11668457280589149, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:19,826]\u001b[0m Trial 125 finished with value: 0.08345781219298537 and parameters: {'n_estimators': 619, 'max_depth': 3, 'learning_rate': 0.1292886889574429, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:20,890]\u001b[0m Trial 126 finished with value: 0.082856239988547 and parameters: {'n_estimators': 601, 'max_depth': 3, 'learning_rate': 0.11809413583643358, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:21,985]\u001b[0m Trial 127 finished with value: 0.08315158199964165 and parameters: {'n_estimators': 594, 'max_depth': 3, 'learning_rate': 0.1180279161431575, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:25,941]\u001b[0m Trial 128 finished with value: 0.09996173825297669 and parameters: {'n_estimators': 683, 'max_depth': 8, 'learning_rate': 0.1481997044308352, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:26,973]\u001b[0m Trial 129 finished with value: 0.08287132257374849 and parameters: {'n_estimators': 617, 'max_depth': 3, 'learning_rate': 0.16403809237444936, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:27,882]\u001b[0m Trial 130 finished with value: 0.08399392866757854 and parameters: {'n_estimators': 622, 'max_depth': 3, 'learning_rate': 0.16573585552214937, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:28,930]\u001b[0m Trial 131 finished with value: 0.08329801970738117 and parameters: {'n_estimators': 664, 'max_depth': 3, 'learning_rate': 0.11104707563596589, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:29,955]\u001b[0m Trial 132 finished with value: 0.08218916379001612 and parameters: {'n_estimators': 675, 'max_depth': 3, 'learning_rate': 0.13801112479702166, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:30,944]\u001b[0m Trial 133 finished with value: 0.08272626181044221 and parameters: {'n_estimators': 635, 'max_depth': 3, 'learning_rate': 0.14191624976621722, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:31,908]\u001b[0m Trial 134 finished with value: 0.08356949324059257 and parameters: {'n_estimators': 642, 'max_depth': 3, 'learning_rate': 0.1537375807154451, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:32,970]\u001b[0m Trial 135 finished with value: 0.08344809825525434 and parameters: {'n_estimators': 674, 'max_depth': 3, 'learning_rate': 0.1369706909144448, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:33,966]\u001b[0m Trial 136 finished with value: 0.08412450105439928 and parameters: {'n_estimators': 587, 'max_depth': 3, 'learning_rate': 0.12395820780479369, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:34,971]\u001b[0m Trial 137 finished with value: 0.08436250442465335 and parameters: {'n_estimators': 706, 'max_depth': 3, 'learning_rate': 0.17785803488799526, 'min_child_weight': 5}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:36,063]\u001b[0m Trial 138 finished with value: 0.08543737126852116 and parameters: {'n_estimators': 605, 'max_depth': 4, 'learning_rate': 0.14353716350915044, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:36,990]\u001b[0m Trial 139 finished with value: 0.08315415930339068 and parameters: {'n_estimators': 675, 'max_depth': 3, 'learning_rate': 0.13405820894560508, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:37,916]\u001b[0m Trial 140 finished with value: 0.08328874654472859 and parameters: {'n_estimators': 653, 'max_depth': 3, 'learning_rate': 0.15869826973665974, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:38,819]\u001b[0m Trial 141 finished with value: 0.08377063932211924 and parameters: {'n_estimators': 633, 'max_depth': 3, 'learning_rate': 0.11658539958968088, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:39,625]\u001b[0m Trial 142 finished with value: 0.08374662836316327 and parameters: {'n_estimators': 618, 'max_depth': 3, 'learning_rate': 0.1277263961665088, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:40,596]\u001b[0m Trial 143 finished with value: 0.08432312077904969 and parameters: {'n_estimators': 721, 'max_depth': 3, 'learning_rate': 0.10853179618895935, 'min_child_weight': 5}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:41,464]\u001b[0m Trial 144 finished with value: 0.08334078110772783 and parameters: {'n_estimators': 640, 'max_depth': 3, 'learning_rate': 0.12110494404716916, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:46,935]\u001b[0m Trial 145 finished with value: 0.10451366567669003 and parameters: {'n_estimators': 687, 'max_depth': 14, 'learning_rate': 0.14507074038584247, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:47,999]\u001b[0m Trial 146 finished with value: 0.08396547011835669 and parameters: {'n_estimators': 613, 'max_depth': 4, 'learning_rate': 0.13639171825799357, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:48,303]\u001b[0m Trial 147 finished with value: 0.09866471516554136 and parameters: {'n_estimators': 132, 'max_depth': 3, 'learning_rate': 0.11238564435102358, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:49,278]\u001b[0m Trial 148 finished with value: 0.0840355188769745 and parameters: {'n_estimators': 584, 'max_depth': 4, 'learning_rate': 0.10322045668079496, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:50,212]\u001b[0m Trial 149 finished with value: 0.08313315216040826 and parameters: {'n_estimators': 750, 'max_depth': 3, 'learning_rate': 0.1660799124374714, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:51,136]\u001b[0m Trial 150 finished with value: 0.0833273896064651 and parameters: {'n_estimators': 751, 'max_depth': 3, 'learning_rate': 0.1651366333583015, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:52,169]\u001b[0m Trial 151 finished with value: 0.08380846367977608 and parameters: {'n_estimators': 777, 'max_depth': 3, 'learning_rate': 0.15286807372382835, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:53,088]\u001b[0m Trial 152 finished with value: 0.08301209213271031 and parameters: {'n_estimators': 731, 'max_depth': 3, 'learning_rate': 0.12802760864145787, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:53,989]\u001b[0m Trial 153 finished with value: 0.08302693484535764 and parameters: {'n_estimators': 727, 'max_depth': 3, 'learning_rate': 0.12784958699998758, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:55,119]\u001b[0m Trial 154 finished with value: 0.08513153125053821 and parameters: {'n_estimators': 726, 'max_depth': 4, 'learning_rate': 0.1261944095331, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:55,951]\u001b[0m Trial 155 finished with value: 0.08294988420046369 and parameters: {'n_estimators': 696, 'max_depth': 3, 'learning_rate': 0.13532433933688193, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:56,805]\u001b[0m Trial 156 finished with value: 0.0828023224844447 and parameters: {'n_estimators': 695, 'max_depth': 3, 'learning_rate': 0.14085302275617623, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:57,785]\u001b[0m Trial 157 finished with value: 0.08576151350462309 and parameters: {'n_estimators': 662, 'max_depth': 4, 'learning_rate': 0.14069482415142037, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:58,602]\u001b[0m Trial 158 finished with value: 0.08313593893209155 and parameters: {'n_estimators': 691, 'max_depth': 3, 'learning_rate': 0.15154086975109562, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:10:59,475]\u001b[0m Trial 159 finished with value: 0.0823890790084934 and parameters: {'n_estimators': 698, 'max_depth': 3, 'learning_rate': 0.1330362377141421, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:03,657]\u001b[0m Trial 160 finished with value: 0.10233081625126154 and parameters: {'n_estimators': 671, 'max_depth': 10, 'learning_rate': 0.1370092278862532, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:04,492]\u001b[0m Trial 161 finished with value: 0.08391457740581108 and parameters: {'n_estimators': 694, 'max_depth': 3, 'learning_rate': 0.13185349868056861, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:05,313]\u001b[0m Trial 162 finished with value: 0.08346778197338994 and parameters: {'n_estimators': 700, 'max_depth': 3, 'learning_rate': 0.17805642639231592, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:06,061]\u001b[0m Trial 163 finished with value: 0.08323672801444174 and parameters: {'n_estimators': 653, 'max_depth': 3, 'learning_rate': 0.12025742635806869, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:06,873]\u001b[0m Trial 164 finished with value: 0.11016564627256034 and parameters: {'n_estimators': 676, 'max_depth': 3, 'learning_rate': 0.010378973569057811, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:07,856]\u001b[0m Trial 165 finished with value: 0.08625986833194307 and parameters: {'n_estimators': 707, 'max_depth': 4, 'learning_rate': 0.14716036895634124, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:08,744]\u001b[0m Trial 166 finished with value: 0.08308838128817571 and parameters: {'n_estimators': 741, 'max_depth': 3, 'learning_rate': 0.09963558574504541, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:09,487]\u001b[0m Trial 167 finished with value: 0.08256675863644616 and parameters: {'n_estimators': 656, 'max_depth': 3, 'learning_rate': 0.13973814936294954, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:10,387]\u001b[0m Trial 168 finished with value: 0.08687355439209934 and parameters: {'n_estimators': 638, 'max_depth': 4, 'learning_rate': 0.1609132460578733, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:11,067]\u001b[0m Trial 169 finished with value: 0.08320375301071112 and parameters: {'n_estimators': 600, 'max_depth': 3, 'learning_rate': 0.14178668903280967, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:11,811]\u001b[0m Trial 170 finished with value: 0.0831569856636962 and parameters: {'n_estimators': 665, 'max_depth': 3, 'learning_rate': 0.1268643147989451, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:12,599]\u001b[0m Trial 171 finished with value: 0.0830861335286607 and parameters: {'n_estimators': 683, 'max_depth': 3, 'learning_rate': 0.13290225573477593, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:13,398]\u001b[0m Trial 172 finished with value: 0.08373209848835807 and parameters: {'n_estimators': 719, 'max_depth': 3, 'learning_rate': 0.12019996757476797, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:14,153]\u001b[0m Trial 173 finished with value: 0.08325772728529475 and parameters: {'n_estimators': 652, 'max_depth': 3, 'learning_rate': 0.15214886960476046, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:14,878]\u001b[0m Trial 174 finished with value: 0.0840249429842295 and parameters: {'n_estimators': 628, 'max_depth': 3, 'learning_rate': 0.11491677371037282, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:15,672]\u001b[0m Trial 175 finished with value: 0.08340173491254706 and parameters: {'n_estimators': 700, 'max_depth': 3, 'learning_rate': 0.09436776330584665, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:16,463]\u001b[0m Trial 176 finished with value: 0.0841523490888042 and parameters: {'n_estimators': 761, 'max_depth': 3, 'learning_rate': 0.10647568348454815, 'min_child_weight': 6}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:17,236]\u001b[0m Trial 177 finished with value: 0.08278875829740572 and parameters: {'n_estimators': 681, 'max_depth': 3, 'learning_rate': 0.14108357812745545, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:18,095]\u001b[0m Trial 178 finished with value: 0.08588354015186887 and parameters: {'n_estimators': 653, 'max_depth': 4, 'learning_rate': 0.1392830063377355, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:18,966]\u001b[0m Trial 179 finished with value: 0.08460117799571931 and parameters: {'n_estimators': 810, 'max_depth': 3, 'learning_rate': 0.17338984425136839, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:19,754]\u001b[0m Trial 180 finished with value: 0.08271264948133934 and parameters: {'n_estimators': 684, 'max_depth': 3, 'learning_rate': 0.15705465604688915, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:20,446]\u001b[0m Trial 181 finished with value: 0.08311842459534399 and parameters: {'n_estimators': 672, 'max_depth': 3, 'learning_rate': 0.158421171065339, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:22,446]\u001b[0m Trial 182 finished with value: 0.09775820704966276 and parameters: {'n_estimators': 684, 'max_depth': 7, 'learning_rate': 0.14615087650234898, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:23,149]\u001b[0m Trial 183 finished with value: 0.08363667644538213 and parameters: {'n_estimators': 638, 'max_depth': 3, 'learning_rate': 0.1338226018140962, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:23,929]\u001b[0m Trial 184 finished with value: 0.08518966582885326 and parameters: {'n_estimators': 736, 'max_depth': 3, 'learning_rate': 0.19077551462452097, 'min_child_weight': 5}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:24,664]\u001b[0m Trial 185 finished with value: 0.08234088467720187 and parameters: {'n_estimators': 664, 'max_depth': 3, 'learning_rate': 0.12495509112417334, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:25,327]\u001b[0m Trial 186 finished with value: 0.08300663028468404 and parameters: {'n_estimators': 618, 'max_depth': 3, 'learning_rate': 0.15032173910311109, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:26,013]\u001b[0m Trial 187 finished with value: 0.08294792493806535 and parameters: {'n_estimators': 651, 'max_depth': 3, 'learning_rate': 0.1419365210881878, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:26,902]\u001b[0m Trial 188 finished with value: 0.09374333088296935 and parameters: {'n_estimators': 658, 'max_depth': 4, 'learning_rate': 0.02080342899711038, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:27,563]\u001b[0m Trial 189 finished with value: 0.08186637706985608 and parameters: {'n_estimators': 632, 'max_depth': 3, 'learning_rate': 0.15890258227803425, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:28,204]\u001b[0m Trial 190 finished with value: 0.08286524474377754 and parameters: {'n_estimators': 629, 'max_depth': 3, 'learning_rate': 0.16926927538063177, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:28,865]\u001b[0m Trial 191 finished with value: 0.08407084466100902 and parameters: {'n_estimators': 593, 'max_depth': 3, 'learning_rate': 0.16904208683351568, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:29,493]\u001b[0m Trial 192 finished with value: 0.08513404749620522 and parameters: {'n_estimators': 608, 'max_depth': 3, 'learning_rate': 0.20773195866623698, 'min_child_weight': 5}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:30,168]\u001b[0m Trial 193 finished with value: 0.08512809066960167 and parameters: {'n_estimators': 619, 'max_depth': 3, 'learning_rate': 0.18043615339507732, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:30,837]\u001b[0m Trial 194 finished with value: 0.08317299290426386 and parameters: {'n_estimators': 633, 'max_depth': 3, 'learning_rate': 0.16509441720922918, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:31,437]\u001b[0m Trial 195 finished with value: 0.08329165608201898 and parameters: {'n_estimators': 567, 'max_depth': 3, 'learning_rate': 0.1538172785739412, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:32,094]\u001b[0m Trial 196 finished with value: 0.09249173931498986 and parameters: {'n_estimators': 636, 'max_depth': 3, 'learning_rate': 0.037374917201836444, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:32,976]\u001b[0m Trial 197 finished with value: 0.08581289239318927 and parameters: {'n_estimators': 670, 'max_depth': 4, 'learning_rate': 0.12330769828374147, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:33,714]\u001b[0m Trial 198 finished with value: 0.08311832057969164 and parameters: {'n_estimators': 659, 'max_depth': 3, 'learning_rate': 0.16042916038914823, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:34,425]\u001b[0m Trial 199 finished with value: 0.08244091747412202 and parameters: {'n_estimators': 684, 'max_depth': 3, 'learning_rate': 0.14385568265471832, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:35,286]\u001b[0m Trial 200 finished with value: 0.0859276097402228 and parameters: {'n_estimators': 682, 'max_depth': 4, 'learning_rate': 0.14224587898891486, 'min_child_weight': 5}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:35,925]\u001b[0m Trial 201 finished with value: 0.08294365575003566 and parameters: {'n_estimators': 637, 'max_depth': 3, 'learning_rate': 0.15046884654817935, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:36,628]\u001b[0m Trial 202 finished with value: 0.08291915631246957 and parameters: {'n_estimators': 674, 'max_depth': 3, 'learning_rate': 0.13023642816500441, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:37,319]\u001b[0m Trial 203 finished with value: 0.08431553209465299 and parameters: {'n_estimators': 710, 'max_depth': 3, 'learning_rate': 0.17093907596417868, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:38,044]\u001b[0m Trial 204 finished with value: 0.08316980756401962 and parameters: {'n_estimators': 687, 'max_depth': 3, 'learning_rate': 0.11869732635990873, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:38,845]\u001b[0m Trial 205 finished with value: 0.08275119431371236 and parameters: {'n_estimators': 605, 'max_depth': 3, 'learning_rate': 0.1414067208791691, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:39,503]\u001b[0m Trial 206 finished with value: 0.08259810148865966 and parameters: {'n_estimators': 650, 'max_depth': 3, 'learning_rate': 0.1381752365021477, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:40,200]\u001b[0m Trial 207 finished with value: 0.08269710428895796 and parameters: {'n_estimators': 660, 'max_depth': 3, 'learning_rate': 0.13736718028084632, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:40,925]\u001b[0m Trial 208 finished with value: 0.0825306928188557 and parameters: {'n_estimators': 662, 'max_depth': 3, 'learning_rate': 0.1382901086616111, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:41,627]\u001b[0m Trial 209 finished with value: 0.08327358623423534 and parameters: {'n_estimators': 662, 'max_depth': 3, 'learning_rate': 0.13812304221937574, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:42,545]\u001b[0m Trial 210 finished with value: 0.08337420215674948 and parameters: {'n_estimators': 695, 'max_depth': 3, 'learning_rate': 0.1286032868551115, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:46,733]\u001b[0m Trial 211 finished with value: 0.10445665012752847 and parameters: {'n_estimators': 659, 'max_depth': 12, 'learning_rate': 0.14618571428673297, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:47,371]\u001b[0m Trial 212 finished with value: 0.08334076815055066 and parameters: {'n_estimators': 649, 'max_depth': 3, 'learning_rate': 0.13808973747749967, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:48,059]\u001b[0m Trial 213 finished with value: 0.08274338024429977 and parameters: {'n_estimators': 686, 'max_depth': 3, 'learning_rate': 0.1304660168185342, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:48,746]\u001b[0m Trial 214 finished with value: 0.08251096914423196 and parameters: {'n_estimators': 680, 'max_depth': 3, 'learning_rate': 0.1293197990960976, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:49,455]\u001b[0m Trial 215 finished with value: 0.08199902606831211 and parameters: {'n_estimators': 680, 'max_depth': 3, 'learning_rate': 0.1282709846134543, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:50,158]\u001b[0m Trial 216 finished with value: 0.08235772891596732 and parameters: {'n_estimators': 672, 'max_depth': 3, 'learning_rate': 0.1249657112280401, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:50,888]\u001b[0m Trial 217 finished with value: 0.08293553119582622 and parameters: {'n_estimators': 718, 'max_depth': 3, 'learning_rate': 0.12899706846722223, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:51,583]\u001b[0m Trial 218 finished with value: 0.08253509322945916 and parameters: {'n_estimators': 680, 'max_depth': 3, 'learning_rate': 0.12265854350792257, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:52,492]\u001b[0m Trial 219 finished with value: 0.08376735221276418 and parameters: {'n_estimators': 670, 'max_depth': 4, 'learning_rate': 0.12142548645862575, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:53,203]\u001b[0m Trial 220 finished with value: 0.08242039156277536 and parameters: {'n_estimators': 707, 'max_depth': 3, 'learning_rate': 0.12695046859378734, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:53,951]\u001b[0m Trial 221 finished with value: 0.08310408342648781 and parameters: {'n_estimators': 716, 'max_depth': 3, 'learning_rate': 0.12782331377936376, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:54,649]\u001b[0m Trial 222 finished with value: 0.08236920723797173 and parameters: {'n_estimators': 698, 'max_depth': 3, 'learning_rate': 0.1233028505972129, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:55,391]\u001b[0m Trial 223 finished with value: 0.08291592089843107 and parameters: {'n_estimators': 705, 'max_depth': 3, 'learning_rate': 0.11133452136965735, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:56,121]\u001b[0m Trial 224 finished with value: 0.08237535912990515 and parameters: {'n_estimators': 686, 'max_depth': 3, 'learning_rate': 0.12369671382064866, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:56,850]\u001b[0m Trial 225 finished with value: 0.08328242745865577 and parameters: {'n_estimators': 699, 'max_depth': 3, 'learning_rate': 0.12113605530093002, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:57,521]\u001b[0m Trial 226 finished with value: 0.08330519127524712 and parameters: {'n_estimators': 674, 'max_depth': 3, 'learning_rate': 0.10990256239727371, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:58,244]\u001b[0m Trial 227 finished with value: 0.08333766438105057 and parameters: {'n_estimators': 720, 'max_depth': 3, 'learning_rate': 0.1240892314689277, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:58,965]\u001b[0m Trial 228 finished with value: 0.08348678243602832 and parameters: {'n_estimators': 686, 'max_depth': 3, 'learning_rate': 0.13384598550855942, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:11:59,833]\u001b[0m Trial 229 finished with value: 0.08346972442740377 and parameters: {'n_estimators': 706, 'max_depth': 3, 'learning_rate': 0.11573371362562394, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:00,543]\u001b[0m Trial 230 finished with value: 0.08291621932822964 and parameters: {'n_estimators': 668, 'max_depth': 3, 'learning_rate': 0.1512250693646204, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:01,226]\u001b[0m Trial 231 finished with value: 0.08231996370906212 and parameters: {'n_estimators': 685, 'max_depth': 3, 'learning_rate': 0.13051819305340795, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:01,937]\u001b[0m Trial 232 finished with value: 0.08231519235453469 and parameters: {'n_estimators': 691, 'max_depth': 3, 'learning_rate': 0.12553229969743437, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:02,653]\u001b[0m Trial 233 finished with value: 0.08261364924345689 and parameters: {'n_estimators': 688, 'max_depth': 3, 'learning_rate': 0.1321980613581436, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:03,349]\u001b[0m Trial 234 finished with value: 0.0825212172002789 and parameters: {'n_estimators': 688, 'max_depth': 3, 'learning_rate': 0.13022799664454113, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:04,036]\u001b[0m Trial 235 finished with value: 0.08282661754470844 and parameters: {'n_estimators': 697, 'max_depth': 3, 'learning_rate': 0.12582295355401366, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:04,723]\u001b[0m Trial 236 finished with value: 0.08280000274572497 and parameters: {'n_estimators': 667, 'max_depth': 3, 'learning_rate': 0.1306461235844909, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:05,421]\u001b[0m Trial 237 finished with value: 0.0822068912312609 and parameters: {'n_estimators': 684, 'max_depth': 3, 'learning_rate': 0.12061138869972476, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:06,145]\u001b[0m Trial 238 finished with value: 0.08306979246734976 and parameters: {'n_estimators': 691, 'max_depth': 3, 'learning_rate': 0.12030460811937331, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:06,846]\u001b[0m Trial 239 finished with value: 0.08280936601250934 and parameters: {'n_estimators': 711, 'max_depth': 3, 'learning_rate': 0.12383216793083743, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:07,546]\u001b[0m Trial 240 finished with value: 0.08323729894382537 and parameters: {'n_estimators': 685, 'max_depth': 3, 'learning_rate': 0.11260067480876691, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:08,198]\u001b[0m Trial 241 finished with value: 0.0827596216791415 and parameters: {'n_estimators': 654, 'max_depth': 3, 'learning_rate': 0.13377565509996925, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:08,920]\u001b[0m Trial 242 finished with value: 0.08254312932582486 and parameters: {'n_estimators': 668, 'max_depth': 3, 'learning_rate': 0.13305597335317693, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:09,625]\u001b[0m Trial 243 finished with value: 0.08260706905635237 and parameters: {'n_estimators': 675, 'max_depth': 3, 'learning_rate': 0.11893581462617447, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:10,339]\u001b[0m Trial 244 finished with value: 0.08219159376324617 and parameters: {'n_estimators': 677, 'max_depth': 3, 'learning_rate': 0.12330753726364783, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:10,984]\u001b[0m Trial 245 finished with value: 0.08332867404127005 and parameters: {'n_estimators': 675, 'max_depth': 3, 'learning_rate': 0.11994707099506251, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:11,705]\u001b[0m Trial 246 finished with value: 0.08245010521755164 and parameters: {'n_estimators': 706, 'max_depth': 3, 'learning_rate': 0.12414401068657961, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:12,410]\u001b[0m Trial 247 finished with value: 0.0830015532855319 and parameters: {'n_estimators': 711, 'max_depth': 3, 'learning_rate': 0.12742768482582006, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:13,119]\u001b[0m Trial 248 finished with value: 0.08311543440998431 and parameters: {'n_estimators': 729, 'max_depth': 3, 'learning_rate': 0.13219496439329864, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:13,854]\u001b[0m Trial 249 finished with value: 0.08354447071482189 and parameters: {'n_estimators': 701, 'max_depth': 3, 'learning_rate': 0.11474885589868798, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:14,765]\u001b[0m Trial 250 finished with value: 0.08459523466839948 and parameters: {'n_estimators': 701, 'max_depth': 4, 'learning_rate': 0.1241502238684362, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:15,479]\u001b[0m Trial 251 finished with value: 0.08279362737753312 and parameters: {'n_estimators': 722, 'max_depth': 3, 'learning_rate': 0.1494991087348143, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:16,357]\u001b[0m Trial 252 finished with value: 0.0832079959920258 and parameters: {'n_estimators': 674, 'max_depth': 3, 'learning_rate': 0.13432866134592425, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:17,050]\u001b[0m Trial 253 finished with value: 0.08271413443324369 and parameters: {'n_estimators': 687, 'max_depth': 3, 'learning_rate': 0.124926024678362, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:17,725]\u001b[0m Trial 254 finished with value: 0.08228669392391635 and parameters: {'n_estimators': 660, 'max_depth': 3, 'learning_rate': 0.14408896612176275, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:18,719]\u001b[0m Trial 255 finished with value: 0.08746037507141037 and parameters: {'n_estimators': 742, 'max_depth': 4, 'learning_rate': 0.14490393078199054, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:19,420]\u001b[0m Trial 256 finished with value: 0.08244453094845948 and parameters: {'n_estimators': 708, 'max_depth': 3, 'learning_rate': 0.11442389434732107, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:20,164]\u001b[0m Trial 257 finished with value: 0.08267614433778225 and parameters: {'n_estimators': 719, 'max_depth': 3, 'learning_rate': 0.10657408998167088, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:20,976]\u001b[0m Trial 258 finished with value: 0.08361087104719155 and parameters: {'n_estimators': 703, 'max_depth': 3, 'learning_rate': 0.11332185614130329, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:21,679]\u001b[0m Trial 259 finished with value: 0.08254682461025294 and parameters: {'n_estimators': 694, 'max_depth': 3, 'learning_rate': 0.11743350889026086, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:22,411]\u001b[0m Trial 260 finished with value: 0.08315199608854278 and parameters: {'n_estimators': 736, 'max_depth': 3, 'learning_rate': 0.12640080137565118, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:23,336]\u001b[0m Trial 261 finished with value: 0.0831869149479296 and parameters: {'n_estimators': 674, 'max_depth': 4, 'learning_rate': 0.10426825177558603, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:24,058]\u001b[0m Trial 262 finished with value: 0.0831621325240794 and parameters: {'n_estimators': 714, 'max_depth': 3, 'learning_rate': 0.11374029478435731, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:24,747]\u001b[0m Trial 263 finished with value: 0.08241511203722858 and parameters: {'n_estimators': 685, 'max_depth': 3, 'learning_rate': 0.12331647684896496, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:25,657]\u001b[0m Trial 264 finished with value: 0.08509378854913294 and parameters: {'n_estimators': 692, 'max_depth': 4, 'learning_rate': 0.12155435884246352, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:27,441]\u001b[0m Trial 265 finished with value: 0.09446901061125146 and parameters: {'n_estimators': 705, 'max_depth': 8, 'learning_rate': 0.1139963114047943, 'min_child_weight': 10}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:28,231]\u001b[0m Trial 266 finished with value: 0.0829485186897416 and parameters: {'n_estimators': 728, 'max_depth': 3, 'learning_rate': 0.12342887071002819, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:28,955]\u001b[0m Trial 267 finished with value: 0.08256420848681219 and parameters: {'n_estimators': 682, 'max_depth': 3, 'learning_rate': 0.10764713410167656, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:29,676]\u001b[0m Trial 268 finished with value: 0.08241588768524585 and parameters: {'n_estimators': 665, 'max_depth': 3, 'learning_rate': 0.1477851086419211, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:30,372]\u001b[0m Trial 269 finished with value: 0.0832236338954678 and parameters: {'n_estimators': 666, 'max_depth': 3, 'learning_rate': 0.15139933317782445, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:31,155]\u001b[0m Trial 270 finished with value: 0.08285732035700681 and parameters: {'n_estimators': 748, 'max_depth': 3, 'learning_rate': 0.1454667316843672, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:32,078]\u001b[0m Trial 271 finished with value: 0.08516721649864492 and parameters: {'n_estimators': 711, 'max_depth': 4, 'learning_rate': 0.13473906654691553, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:32,775]\u001b[0m Trial 272 finished with value: 0.08291222919437333 and parameters: {'n_estimators': 653, 'max_depth': 3, 'learning_rate': 0.15570116711532464, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:33,531]\u001b[0m Trial 273 finished with value: 0.08335991669751068 and parameters: {'n_estimators': 694, 'max_depth': 3, 'learning_rate': 0.1334278205895036, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:34,279]\u001b[0m Trial 274 finished with value: 0.08242883783270984 and parameters: {'n_estimators': 666, 'max_depth': 3, 'learning_rate': 0.14524655842574996, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:35,021]\u001b[0m Trial 275 finished with value: 0.08266547354134668 and parameters: {'n_estimators': 692, 'max_depth': 3, 'learning_rate': 0.14493793627601226, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:35,981]\u001b[0m Trial 276 finished with value: 0.08538467882794278 and parameters: {'n_estimators': 716, 'max_depth': 4, 'learning_rate': 0.12822881844104156, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:36,749]\u001b[0m Trial 277 finished with value: 0.09503155262312986 and parameters: {'n_estimators': 645, 'max_depth': 3, 'learning_rate': 0.029424623795244804, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:37,490]\u001b[0m Trial 278 finished with value: 0.08288635613635478 and parameters: {'n_estimators': 679, 'max_depth': 3, 'learning_rate': 0.14680658631822177, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:38,284]\u001b[0m Trial 279 finished with value: 0.08264974876794147 and parameters: {'n_estimators': 731, 'max_depth': 3, 'learning_rate': 0.11180706672950355, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:39,088]\u001b[0m Trial 280 finished with value: 0.08211371813595908 and parameters: {'n_estimators': 703, 'max_depth': 3, 'learning_rate': 0.1596647076635128, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:40,057]\u001b[0m Trial 281 finished with value: 0.08657470992634171 and parameters: {'n_estimators': 708, 'max_depth': 4, 'learning_rate': 0.1601561323232085, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:40,774]\u001b[0m Trial 282 finished with value: 0.08259439090255191 and parameters: {'n_estimators': 665, 'max_depth': 3, 'learning_rate': 0.1570134056819719, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:41,539]\u001b[0m Trial 283 finished with value: 0.08312408247804894 and parameters: {'n_estimators': 703, 'max_depth': 3, 'learning_rate': 0.1432695740113138, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:42,257]\u001b[0m Trial 284 finished with value: 0.08297372789394555 and parameters: {'n_estimators': 673, 'max_depth': 3, 'learning_rate': 0.11855465621049265, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:43,050]\u001b[0m Trial 285 finished with value: 0.0824331510535781 and parameters: {'n_estimators': 727, 'max_depth': 3, 'learning_rate': 0.13757482304307347, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:45,951]\u001b[0m Trial 286 finished with value: 0.08637316625377117 and parameters: {'n_estimators': 752, 'max_depth': 7, 'learning_rate': 0.013399778495374916, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:50,188]\u001b[0m Trial 287 finished with value: 0.10319183305913875 and parameters: {'n_estimators': 731, 'max_depth': 10, 'learning_rate': 0.15706450307640876, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:51,045]\u001b[0m Trial 288 finished with value: 0.08404118132533886 and parameters: {'n_estimators': 722, 'max_depth': 3, 'learning_rate': 0.18327569199882493, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:51,882]\u001b[0m Trial 289 finished with value: 0.08263370202318032 and parameters: {'n_estimators': 743, 'max_depth': 3, 'learning_rate': 0.14152475896185862, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:52,768]\u001b[0m Trial 290 finished with value: 0.0834927133525499 and parameters: {'n_estimators': 695, 'max_depth': 3, 'learning_rate': 0.09859684146603792, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:53,782]\u001b[0m Trial 291 finished with value: 0.08558875960099446 and parameters: {'n_estimators': 716, 'max_depth': 4, 'learning_rate': 0.14957823486025926, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:54,565]\u001b[0m Trial 292 finished with value: 0.0825962951162357 and parameters: {'n_estimators': 648, 'max_depth': 3, 'learning_rate': 0.16598630304534764, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:55,324]\u001b[0m Trial 293 finished with value: 0.08525988219355811 and parameters: {'n_estimators': 702, 'max_depth': 3, 'learning_rate': 0.13708862791665433, 'min_child_weight': 7}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:56,419]\u001b[0m Trial 294 finished with value: 0.08491634346648637 and parameters: {'n_estimators': 758, 'max_depth': 4, 'learning_rate': 0.12350921715179004, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:57,229]\u001b[0m Trial 295 finished with value: 0.08277226913673 and parameters: {'n_estimators': 665, 'max_depth': 3, 'learning_rate': 0.13438231578427978, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:58,038]\u001b[0m Trial 296 finished with value: 0.0821336454759914 and parameters: {'n_estimators': 689, 'max_depth': 3, 'learning_rate': 0.11800022154866892, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:58,803]\u001b[0m Trial 297 finished with value: 0.08324338368912149 and parameters: {'n_estimators': 646, 'max_depth': 3, 'learning_rate': 0.10442942862659536, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:12:59,837]\u001b[0m Trial 298 finished with value: 0.08389344305054196 and parameters: {'n_estimators': 686, 'max_depth': 4, 'learning_rate': 0.11768636498561107, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:13:00,952]\u001b[0m Trial 299 finished with value: 0.0848766197803248 and parameters: {'n_estimators': 951, 'max_depth': 3, 'learning_rate': 0.1513202508267093, 'min_child_weight': 9}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:13:01,778]\u001b[0m Trial 300 finished with value: 0.0828420257148326 and parameters: {'n_estimators': 666, 'max_depth': 3, 'learning_rate': 0.11033988519544023, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:13:02,588]\u001b[0m Trial 301 finished with value: 0.08199056700519247 and parameters: {'n_estimators': 685, 'max_depth': 3, 'learning_rate': 0.13790133434153026, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:13:04,223]\u001b[0m Trial 302 finished with value: 0.09088192756881397 and parameters: {'n_estimators': 639, 'max_depth': 6, 'learning_rate': 0.14060581484453336, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:13:05,052]\u001b[0m Trial 303 finished with value: 0.08291160072327894 and parameters: {'n_estimators': 680, 'max_depth': 3, 'learning_rate': 0.1476278484118076, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:13:09,664]\u001b[0m Trial 304 finished with value: 0.1049394814041883 and parameters: {'n_estimators': 662, 'max_depth': 13, 'learning_rate': 0.16112454022311065, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:13:10,553]\u001b[0m Trial 305 finished with value: 0.0836179953257 and parameters: {'n_estimators': 687, 'max_depth': 3, 'learning_rate': 0.13108567402429414, 'min_child_weight': 6}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:13:11,379]\u001b[0m Trial 306 finished with value: 0.08265678691618213 and parameters: {'n_estimators': 660, 'max_depth': 3, 'learning_rate': 0.13897800248180486, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:13:12,207]\u001b[0m Trial 307 finished with value: 0.08329057337357557 and parameters: {'n_estimators': 681, 'max_depth': 3, 'learning_rate': 0.12822132343920442, 'min_child_weight': 5}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:13:13,387]\u001b[0m Trial 308 finished with value: 0.08586042141522317 and parameters: {'n_estimators': 726, 'max_depth': 4, 'learning_rate': 0.15310343815449812, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:13:14,193]\u001b[0m Trial 309 finished with value: 0.0831940183020074 and parameters: {'n_estimators': 641, 'max_depth': 3, 'learning_rate': 0.1761957503333402, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:13:15,029]\u001b[0m Trial 310 finished with value: 0.08226786937139183 and parameters: {'n_estimators': 697, 'max_depth': 3, 'learning_rate': 0.13674488871801474, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:13:15,926]\u001b[0m Trial 311 finished with value: 0.08258516295466148 and parameters: {'n_estimators': 699, 'max_depth': 3, 'learning_rate': 0.12434917433904207, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:13:16,848]\u001b[0m Trial 312 finished with value: 0.08418431503900826 and parameters: {'n_estimators': 718, 'max_depth': 3, 'learning_rate': 0.1356332994882256, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:13:17,832]\u001b[0m Trial 313 finished with value: 0.08378835210536532 and parameters: {'n_estimators': 696, 'max_depth': 3, 'learning_rate': 0.12038022904710605, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:13:19,451]\u001b[0m Trial 314 finished with value: 0.08655320541884155 and parameters: {'n_estimators': 999, 'max_depth': 4, 'learning_rate': 0.13011994056216725, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:13:20,356]\u001b[0m Trial 315 finished with value: 0.08267759915260924 and parameters: {'n_estimators': 669, 'max_depth': 3, 'learning_rate': 0.1458280112679509, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:13:25,684]\u001b[0m Trial 316 finished with value: 0.10230479617375414 and parameters: {'n_estimators': 653, 'max_depth': 11, 'learning_rate': 0.13310804067309584, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:13:26,737]\u001b[0m Trial 317 finished with value: 0.08253292069688178 and parameters: {'n_estimators': 737, 'max_depth': 3, 'learning_rate': 0.16058101456692556, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:13:27,895]\u001b[0m Trial 318 finished with value: 0.08360373450105948 and parameters: {'n_estimators': 773, 'max_depth': 3, 'learning_rate': 0.11833799274094756, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:13:28,887]\u001b[0m Trial 319 finished with value: 0.08306322233093005 and parameters: {'n_estimators': 711, 'max_depth': 3, 'learning_rate': 0.1390084446817895, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:13:29,869]\u001b[0m Trial 320 finished with value: 0.08297395306054933 and parameters: {'n_estimators': 678, 'max_depth': 3, 'learning_rate': 0.12550358934501196, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:13:31,092]\u001b[0m Trial 321 finished with value: 0.08502468501565282 and parameters: {'n_estimators': 694, 'max_depth': 4, 'learning_rate': 0.10787972119581064, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:13:32,064]\u001b[0m Trial 322 finished with value: 0.08294150522200303 and parameters: {'n_estimators': 635, 'max_depth': 3, 'learning_rate': 0.15361831708872656, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:13:33,009]\u001b[0m Trial 323 finished with value: 0.08291127575946386 and parameters: {'n_estimators': 660, 'max_depth': 3, 'learning_rate': 0.1183023176608315, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:13:34,441]\u001b[0m Trial 324 finished with value: 0.0845033398062486 and parameters: {'n_estimators': 731, 'max_depth': 4, 'learning_rate': 0.13111286746066964, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:13:35,436]\u001b[0m Trial 325 finished with value: 0.08229269394206133 and parameters: {'n_estimators': 699, 'max_depth': 3, 'learning_rate': 0.14706100859622576, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:13:36,458]\u001b[0m Trial 326 finished with value: 0.08319980298285114 and parameters: {'n_estimators': 679, 'max_depth': 3, 'learning_rate': 0.1483784597945296, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:13:37,719]\u001b[0m Trial 327 finished with value: 0.08355104320136941 and parameters: {'n_estimators': 696, 'max_depth': 3, 'learning_rate': 0.16922405208778718, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:13:38,884]\u001b[0m Trial 328 finished with value: 0.08214051827823698 and parameters: {'n_estimators': 670, 'max_depth': 3, 'learning_rate': 0.16272617069275944, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:13:40,326]\u001b[0m Trial 329 finished with value: 0.08368642417592195 and parameters: {'n_estimators': 703, 'max_depth': 3, 'learning_rate': 0.16690747633113653, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:13:47,523]\u001b[0m Trial 330 finished with value: 0.10056914324523544 and parameters: {'n_estimators': 679, 'max_depth': 9, 'learning_rate': 0.17513492159295752, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:13:49,548]\u001b[0m Trial 331 finished with value: 0.08742187178093777 and parameters: {'n_estimators': 652, 'max_depth': 4, 'learning_rate': 0.201744326145274, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:13:59,931]\u001b[0m Trial 332 finished with value: 0.10750592060617267 and parameters: {'n_estimators': 630, 'max_depth': 15, 'learning_rate': 0.15893696609343366, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:14:02,391]\u001b[0m Trial 333 finished with value: 0.08342305906642027 and parameters: {'n_estimators': 712, 'max_depth': 3, 'learning_rate': 0.11502050021881001, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:14:04,569]\u001b[0m Trial 334 finished with value: 0.08514749289202753 and parameters: {'n_estimators': 693, 'max_depth': 3, 'learning_rate': 0.19199542829851968, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:14:05,966]\u001b[0m Trial 335 finished with value: 0.08306091591721386 and parameters: {'n_estimators': 670, 'max_depth': 3, 'learning_rate': 0.1261119983256676, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:14:07,424]\u001b[0m Trial 336 finished with value: 0.08237992365506012 and parameters: {'n_estimators': 686, 'max_depth': 3, 'learning_rate': 0.13839062525395407, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:14:09,298]\u001b[0m Trial 337 finished with value: 0.08504635372532161 and parameters: {'n_estimators': 650, 'max_depth': 4, 'learning_rate': 0.14777406504294713, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:14:10,810]\u001b[0m Trial 338 finished with value: 0.08217331987854679 and parameters: {'n_estimators': 681, 'max_depth': 3, 'learning_rate': 0.15958875150819574, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:14:12,270]\u001b[0m Trial 339 finished with value: 0.08579425456448286 and parameters: {'n_estimators': 685, 'max_depth': 3, 'learning_rate': 0.28183571682530023, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:14:13,799]\u001b[0m Trial 340 finished with value: 0.08467105207276104 and parameters: {'n_estimators': 685, 'max_depth': 3, 'learning_rate': 0.18339304412257268, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:14:15,292]\u001b[0m Trial 341 finished with value: 0.08242706667986897 and parameters: {'n_estimators': 699, 'max_depth': 3, 'learning_rate': 0.15864922195203376, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:14:17,272]\u001b[0m Trial 342 finished with value: 0.08495171802357053 and parameters: {'n_estimators': 720, 'max_depth': 3, 'learning_rate': 0.22873435311700674, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:14:19,384]\u001b[0m Trial 343 finished with value: 0.08436796376240073 and parameters: {'n_estimators': 672, 'max_depth': 4, 'learning_rate': 0.13726266719431376, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:14:21,026]\u001b[0m Trial 344 finished with value: 0.08324510960531548 and parameters: {'n_estimators': 656, 'max_depth': 3, 'learning_rate': 0.1743939296900131, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:14:22,717]\u001b[0m Trial 345 finished with value: 0.08348012821167246 and parameters: {'n_estimators': 693, 'max_depth': 3, 'learning_rate': 0.0986764267333128, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:14:24,348]\u001b[0m Trial 346 finished with value: 0.08222864482506259 and parameters: {'n_estimators': 677, 'max_depth': 3, 'learning_rate': 0.16323272359212893, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:14:26,024]\u001b[0m Trial 347 finished with value: 0.08360830242929047 and parameters: {'n_estimators': 641, 'max_depth': 3, 'learning_rate': 0.16737418668167678, 'min_child_weight': 8}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:14:28,017]\u001b[0m Trial 348 finished with value: 0.08218972123363966 and parameters: {'n_estimators': 714, 'max_depth': 3, 'learning_rate': 0.16333928423694558, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:14:30,659]\u001b[0m Trial 349 finished with value: 0.0846592336480432 and parameters: {'n_estimators': 744, 'max_depth': 3, 'learning_rate': 0.18682580269285856, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:14:32,780]\u001b[0m Trial 350 finished with value: 0.08296534942665079 and parameters: {'n_estimators': 718, 'max_depth': 3, 'learning_rate': 0.1612367696695066, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:14:34,868]\u001b[0m Trial 351 finished with value: 0.08385594691626566 and parameters: {'n_estimators': 672, 'max_depth': 3, 'learning_rate': 0.1722502401260967, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:14:36,892]\u001b[0m Trial 352 finished with value: 0.08428881746489056 and parameters: {'n_estimators': 627, 'max_depth': 3, 'learning_rate': 0.1791799355169692, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:14:39,542]\u001b[0m Trial 353 finished with value: 0.08514809023965 and parameters: {'n_estimators': 656, 'max_depth': 4, 'learning_rate': 0.15790232018038525, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:14:42,358]\u001b[0m Trial 354 finished with value: 0.08517640762635152 and parameters: {'n_estimators': 713, 'max_depth': 4, 'learning_rate': 0.04139217389343924, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:14:44,563]\u001b[0m Trial 355 finished with value: 0.08374766448669568 and parameters: {'n_estimators': 682, 'max_depth': 3, 'learning_rate': 0.1638557734298231, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:14:46,855]\u001b[0m Trial 356 finished with value: 0.08346478111354153 and parameters: {'n_estimators': 703, 'max_depth': 3, 'learning_rate': 0.1536835958272594, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:14:49,101]\u001b[0m Trial 357 finished with value: 0.08345120363017818 and parameters: {'n_estimators': 674, 'max_depth': 3, 'learning_rate': 0.14247459583800262, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:14:51,699]\u001b[0m Trial 358 finished with value: 0.08341290855897499 and parameters: {'n_estimators': 645, 'max_depth': 3, 'learning_rate': 0.19321798887291278, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:14:54,983]\u001b[0m Trial 359 finished with value: 0.08363018486181899 and parameters: {'n_estimators': 742, 'max_depth': 3, 'learning_rate': 0.15326469897021103, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:14:57,729]\u001b[0m Trial 360 finished with value: 0.08310301346123908 and parameters: {'n_estimators': 722, 'max_depth': 3, 'learning_rate': 0.1654123313909216, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:14:58,427]\u001b[0m Trial 361 finished with value: 0.09272379644340216 and parameters: {'n_estimators': 146, 'max_depth': 3, 'learning_rate': 0.17875733030102187, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:15:01,454]\u001b[0m Trial 362 finished with value: 0.08378574735051028 and parameters: {'n_estimators': 696, 'max_depth': 3, 'learning_rate': 0.14071784533639825, 'min_child_weight': 6}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:15:04,958]\u001b[0m Trial 363 finished with value: 0.0852733325696883 and parameters: {'n_estimators': 667, 'max_depth': 4, 'learning_rate': 0.15365795633244098, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:15:07,494]\u001b[0m Trial 364 finished with value: 0.08216595188325272 and parameters: {'n_estimators': 707, 'max_depth': 3, 'learning_rate': 0.14441597406543358, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:15:20,313]\u001b[0m Trial 365 finished with value: 0.10378265171249632 and parameters: {'n_estimators': 761, 'max_depth': 12, 'learning_rate': 0.1700284729570938, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:15:22,547]\u001b[0m Trial 366 finished with value: 0.08294003021589308 and parameters: {'n_estimators': 712, 'max_depth': 3, 'learning_rate': 0.14850120633406186, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:15:25,314]\u001b[0m Trial 367 finished with value: 0.08326020405580013 and parameters: {'n_estimators': 688, 'max_depth': 3, 'learning_rate': 0.1381824558269524, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:15:27,709]\u001b[0m Trial 368 finished with value: 0.08228238847757531 and parameters: {'n_estimators': 732, 'max_depth': 3, 'learning_rate': 0.11035431570305058, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:15:30,567]\u001b[0m Trial 369 finished with value: 0.08428079796872202 and parameters: {'n_estimators': 740, 'max_depth': 4, 'learning_rate': 0.10673704404460277, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:15:32,766]\u001b[0m Trial 370 finished with value: 0.08311972393512462 and parameters: {'n_estimators': 722, 'max_depth': 3, 'learning_rate': 0.10085546983090601, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:15:35,316]\u001b[0m Trial 371 finished with value: 0.08338968730295256 and parameters: {'n_estimators': 756, 'max_depth': 3, 'learning_rate': 0.1096439095644406, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:15:37,847]\u001b[0m Trial 372 finished with value: 0.08293213450970785 and parameters: {'n_estimators': 729, 'max_depth': 3, 'learning_rate': 0.11362738374211649, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:15:41,844]\u001b[0m Trial 373 finished with value: 0.09393946362590899 and parameters: {'n_estimators': 656, 'max_depth': 7, 'learning_rate': 0.11837369088267372, 'min_child_weight': 7}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:15:43,764]\u001b[0m Trial 374 finished with value: 0.08327466350181753 and parameters: {'n_estimators': 707, 'max_depth': 3, 'learning_rate': 0.12846530317218494, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:15:47,927]\u001b[0m Trial 375 finished with value: 0.08506553342131629 and parameters: {'n_estimators': 671, 'max_depth': 4, 'learning_rate': 0.12159918709159756, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:15:51,526]\u001b[0m Trial 376 finished with value: 0.0826984353792733 and parameters: {'n_estimators': 785, 'max_depth': 3, 'learning_rate': 0.10473485939363336, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:15:54,399]\u001b[0m Trial 377 finished with value: 0.08311464356185652 and parameters: {'n_estimators': 631, 'max_depth': 3, 'learning_rate': 0.1588106470835251, 'min_child_weight': 5}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:15:57,306]\u001b[0m Trial 378 finished with value: 0.08507777430539948 and parameters: {'n_estimators': 701, 'max_depth': 3, 'learning_rate': 0.2131369313519442, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:16:13,357]\u001b[0m Trial 379 finished with value: 0.10664077240786737 and parameters: {'n_estimators': 736, 'max_depth': 14, 'learning_rate': 0.09429658952122823, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:16:14,670]\u001b[0m Trial 380 finished with value: 0.08308505363475249 and parameters: {'n_estimators': 713, 'max_depth': 3, 'learning_rate': 0.1131357865715089, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:16:17,213]\u001b[0m Trial 381 finished with value: 0.0840521759461056 and parameters: {'n_estimators': 655, 'max_depth': 4, 'learning_rate': 0.13215392322762637, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:16:22,595]\u001b[0m Trial 382 finished with value: 0.0984829737053229 and parameters: {'n_estimators': 683, 'max_depth': 8, 'learning_rate': 0.14912066158077822, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:16:23,659]\u001b[0m Trial 383 finished with value: 0.08409671431676492 and parameters: {'n_estimators': 531, 'max_depth': 3, 'learning_rate': 0.12205004952071309, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:16:25,231]\u001b[0m Trial 384 finished with value: 0.08343552651457424 and parameters: {'n_estimators': 671, 'max_depth': 3, 'learning_rate': 0.1774655527365081, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:16:26,526]\u001b[0m Trial 385 finished with value: 0.08218376867519202 and parameters: {'n_estimators': 699, 'max_depth': 3, 'learning_rate': 0.13052937330805023, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:16:27,860]\u001b[0m Trial 386 finished with value: 0.0826176725634861 and parameters: {'n_estimators': 725, 'max_depth': 3, 'learning_rate': 0.13271849711232264, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:16:29,804]\u001b[0m Trial 387 finished with value: 0.087404024246948 and parameters: {'n_estimators': 703, 'max_depth': 4, 'learning_rate': 0.16433111571613443, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:16:31,178]\u001b[0m Trial 388 finished with value: 0.0824579868517901 and parameters: {'n_estimators': 743, 'max_depth': 3, 'learning_rate': 0.14836611835380692, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:16:32,327]\u001b[0m Trial 389 finished with value: 0.08343538851576185 and parameters: {'n_estimators': 623, 'max_depth': 3, 'learning_rate': 0.14125458811777683, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:16:33,602]\u001b[0m Trial 390 finished with value: 0.08632526446250707 and parameters: {'n_estimators': 697, 'max_depth': 3, 'learning_rate': 0.056022746594372785, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:16:34,818]\u001b[0m Trial 391 finished with value: 0.10113468346246568 and parameters: {'n_estimators': 642, 'max_depth': 3, 'learning_rate': 0.020281215100443548, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:16:35,712]\u001b[0m Trial 392 finished with value: 0.08408297959141789 and parameters: {'n_estimators': 483, 'max_depth': 3, 'learning_rate': 0.12900230178141608, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:16:37,088]\u001b[0m Trial 393 finished with value: 0.08455967169326965 and parameters: {'n_estimators': 718, 'max_depth': 3, 'learning_rate': 0.19147390737421194, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:16:38,227]\u001b[0m Trial 394 finished with value: 0.08290135112828734 and parameters: {'n_estimators': 664, 'max_depth': 3, 'learning_rate': 0.15759159883390034, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:16:39,795]\u001b[0m Trial 395 finished with value: 0.08622599585167161 and parameters: {'n_estimators': 772, 'max_depth': 4, 'learning_rate': 0.14358542177433672, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:16:41,262]\u001b[0m Trial 396 finished with value: 0.08256680193136179 and parameters: {'n_estimators': 687, 'max_depth': 3, 'learning_rate': 0.11300629895304438, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:16:42,484]\u001b[0m Trial 397 finished with value: 0.08279304994854082 and parameters: {'n_estimators': 709, 'max_depth': 3, 'learning_rate': 0.13016555058226223, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:16:43,773]\u001b[0m Trial 398 finished with value: 0.08298208106969547 and parameters: {'n_estimators': 673, 'max_depth': 3, 'learning_rate': 0.11711199315704739, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:16:44,383]\u001b[0m Trial 399 finished with value: 0.086487997927399 and parameters: {'n_estimators': 249, 'max_depth': 3, 'learning_rate': 0.17305987730210545, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:16:45,927]\u001b[0m Trial 400 finished with value: 0.08451040655479528 and parameters: {'n_estimators': 651, 'max_depth': 4, 'learning_rate': 0.13612431063346395, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:16:47,362]\u001b[0m Trial 401 finished with value: 0.08312387237224324 and parameters: {'n_estimators': 755, 'max_depth': 3, 'learning_rate': 0.15369809132501225, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:16:48,656]\u001b[0m Trial 402 finished with value: 0.08279913946855727 and parameters: {'n_estimators': 733, 'max_depth': 3, 'learning_rate': 0.1046292265631998, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:16:50,162]\u001b[0m Trial 403 finished with value: 0.08525800338496867 and parameters: {'n_estimators': 694, 'max_depth': 4, 'learning_rate': 0.12429077421281269, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:16:51,405]\u001b[0m Trial 404 finished with value: 0.08306589903344415 and parameters: {'n_estimators': 681, 'max_depth': 3, 'learning_rate': 0.16553152521787645, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:16:52,914]\u001b[0m Trial 405 finished with value: 0.08263009693292171 and parameters: {'n_estimators': 710, 'max_depth': 3, 'learning_rate': 0.14717100904205002, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:16:54,615]\u001b[0m Trial 406 finished with value: 0.08444546632016169 and parameters: {'n_estimators': 659, 'max_depth': 3, 'learning_rate': 0.13525496203868753, 'min_child_weight': 5}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:16:56,009]\u001b[0m Trial 407 finished with value: 0.08282584835510597 and parameters: {'n_estimators': 693, 'max_depth': 3, 'learning_rate': 0.11939313059434521, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:16:57,062]\u001b[0m Trial 408 finished with value: 0.08275273275273365 and parameters: {'n_estimators': 640, 'max_depth': 3, 'learning_rate': 0.14232542742183865, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:16:58,475]\u001b[0m Trial 409 finished with value: 0.08324783184800626 and parameters: {'n_estimators': 671, 'max_depth': 4, 'learning_rate': 0.10955386302155608, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:16:59,658]\u001b[0m Trial 410 finished with value: 0.08293670250894221 and parameters: {'n_estimators': 729, 'max_depth': 3, 'learning_rate': 0.12846222422596343, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:00,810]\u001b[0m Trial 411 finished with value: 0.08291566242729534 and parameters: {'n_estimators': 705, 'max_depth': 3, 'learning_rate': 0.15460480448480116, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:01,871]\u001b[0m Trial 412 finished with value: 0.08471320065483626 and parameters: {'n_estimators': 684, 'max_depth': 3, 'learning_rate': 0.1804718823624195, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:03,131]\u001b[0m Trial 413 finished with value: 0.08267044541148255 and parameters: {'n_estimators': 616, 'max_depth': 3, 'learning_rate': 0.1371759175449779, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:04,488]\u001b[0m Trial 414 finished with value: 0.08447697754338505 and parameters: {'n_estimators': 719, 'max_depth': 4, 'learning_rate': 0.12073543052466343, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:05,515]\u001b[0m Trial 415 finished with value: 0.08390700147066862 and parameters: {'n_estimators': 659, 'max_depth': 3, 'learning_rate': 0.09063822398633373, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:06,540]\u001b[0m Trial 416 finished with value: 0.08333214843043357 and parameters: {'n_estimators': 677, 'max_depth': 3, 'learning_rate': 0.16525694331500834, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:07,585]\u001b[0m Trial 417 finished with value: 0.0828179712937066 and parameters: {'n_estimators': 696, 'max_depth': 3, 'learning_rate': 0.14563648746893948, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:08,658]\u001b[0m Trial 418 finished with value: 0.08299945475657355 and parameters: {'n_estimators': 743, 'max_depth': 3, 'learning_rate': 0.12993002928050323, 'min_child_weight': 10}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:09,614]\u001b[0m Trial 419 finished with value: 0.08281322887122904 and parameters: {'n_estimators': 639, 'max_depth': 3, 'learning_rate': 0.10013008707833468, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:11,103]\u001b[0m Trial 420 finished with value: 0.08395017481762637 and parameters: {'n_estimators': 714, 'max_depth': 4, 'learning_rate': 0.11593086846506695, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:12,106]\u001b[0m Trial 421 finished with value: 0.08241984428417812 and parameters: {'n_estimators': 665, 'max_depth': 3, 'learning_rate': 0.1540899964735868, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:13,140]\u001b[0m Trial 422 finished with value: 0.08213717563466298 and parameters: {'n_estimators': 694, 'max_depth': 3, 'learning_rate': 0.1253322891461921, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:18,198]\u001b[0m Trial 423 finished with value: 0.10076673879446381 and parameters: {'n_estimators': 678, 'max_depth': 9, 'learning_rate': 0.1387684104549456, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:19,174]\u001b[0m Trial 424 finished with value: 0.0832436787503004 and parameters: {'n_estimators': 697, 'max_depth': 3, 'learning_rate': 0.1285720018732122, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:20,131]\u001b[0m Trial 425 finished with value: 0.08252665324010452 and parameters: {'n_estimators': 647, 'max_depth': 3, 'learning_rate': 0.16223142631104392, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:21,177]\u001b[0m Trial 426 finished with value: 0.08320587493050441 and parameters: {'n_estimators': 729, 'max_depth': 3, 'learning_rate': 0.11042272963307947, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:22,439]\u001b[0m Trial 427 finished with value: 0.08776937049652797 and parameters: {'n_estimators': 660, 'max_depth': 4, 'learning_rate': 0.1860127191520425, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:23,397]\u001b[0m Trial 428 finished with value: 0.08227879636452218 and parameters: {'n_estimators': 681, 'max_depth': 3, 'learning_rate': 0.1430242256487536, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:24,413]\u001b[0m Trial 429 finished with value: 0.08310102230104396 and parameters: {'n_estimators': 709, 'max_depth': 3, 'learning_rate': 0.14332917554812236, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:25,721]\u001b[0m Trial 430 finished with value: 0.08312565513737012 and parameters: {'n_estimators': 690, 'max_depth': 3, 'learning_rate': 0.15488661173576204, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:27,226]\u001b[0m Trial 431 finished with value: 0.08321513334889388 and parameters: {'n_estimators': 723, 'max_depth': 3, 'learning_rate': 0.1689256765108054, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:28,350]\u001b[0m Trial 432 finished with value: 0.08299056674606672 and parameters: {'n_estimators': 749, 'max_depth': 3, 'learning_rate': 0.14925867967009335, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:29,410]\u001b[0m Trial 433 finished with value: 0.08303414146151361 and parameters: {'n_estimators': 681, 'max_depth': 3, 'learning_rate': 0.134537371098811, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:30,322]\u001b[0m Trial 434 finished with value: 0.083011182298674 and parameters: {'n_estimators': 628, 'max_depth': 3, 'learning_rate': 0.14492870722400497, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:31,921]\u001b[0m Trial 435 finished with value: 0.08365498360084109 and parameters: {'n_estimators': 883, 'max_depth': 4, 'learning_rate': 0.04917653175939728, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:33,365]\u001b[0m Trial 436 finished with value: 0.08588668701466871 and parameters: {'n_estimators': 703, 'max_depth': 4, 'learning_rate': 0.1576758174977675, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:34,505]\u001b[0m Trial 437 finished with value: 0.08384000289415564 and parameters: {'n_estimators': 667, 'max_depth': 3, 'learning_rate': 0.17250264071297358, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:35,508]\u001b[0m Trial 438 finished with value: 0.0833124264339247 and parameters: {'n_estimators': 685, 'max_depth': 3, 'learning_rate': 0.13781731358248706, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:36,495]\u001b[0m Trial 439 finished with value: 0.08238305561835071 and parameters: {'n_estimators': 654, 'max_depth': 3, 'learning_rate': 0.14836564222125972, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:38,986]\u001b[0m Trial 440 finished with value: 0.09203839346356635 and parameters: {'n_estimators': 716, 'max_depth': 6, 'learning_rate': 0.13044260143878098, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:40,026]\u001b[0m Trial 441 finished with value: 0.08349258535191846 and parameters: {'n_estimators': 696, 'max_depth': 3, 'learning_rate': 0.1994472236759336, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:41,335]\u001b[0m Trial 442 finished with value: 0.08215443768884524 and parameters: {'n_estimators': 763, 'max_depth': 3, 'learning_rate': 0.16107090953015485, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:42,502]\u001b[0m Trial 443 finished with value: 0.08349214873884861 and parameters: {'n_estimators': 805, 'max_depth': 3, 'learning_rate': 0.16970213811628693, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:43,648]\u001b[0m Trial 444 finished with value: 0.08403318361265764 and parameters: {'n_estimators': 790, 'max_depth': 3, 'learning_rate': 0.1832325446059674, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:44,708]\u001b[0m Trial 445 finished with value: 0.0824595433892342 and parameters: {'n_estimators': 770, 'max_depth': 3, 'learning_rate': 0.16320825854971938, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:45,540]\u001b[0m Trial 446 finished with value: 0.08413298882527163 and parameters: {'n_estimators': 507, 'max_depth': 3, 'learning_rate': 0.15513545209100055, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:46,640]\u001b[0m Trial 447 finished with value: 0.08435908750491991 and parameters: {'n_estimators': 746, 'max_depth': 3, 'learning_rate': 0.17649704029436367, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:48,239]\u001b[0m Trial 448 finished with value: 0.08767755459365653 and parameters: {'n_estimators': 774, 'max_depth': 4, 'learning_rate': 0.1631226152238206, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:49,276]\u001b[0m Trial 449 finished with value: 0.0827298801062617 and parameters: {'n_estimators': 759, 'max_depth': 3, 'learning_rate': 0.14914658470642245, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:50,713]\u001b[0m Trial 450 finished with value: 0.09582911899764356 and parameters: {'n_estimators': 737, 'max_depth': 4, 'learning_rate': 0.01587337019268007, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:51,752]\u001b[0m Trial 451 finished with value: 0.08268836609716766 and parameters: {'n_estimators': 731, 'max_depth': 3, 'learning_rate': 0.14171775424242444, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:52,777]\u001b[0m Trial 452 finished with value: 0.08217887190324438 and parameters: {'n_estimators': 714, 'max_depth': 3, 'learning_rate': 0.1525744778637211, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:53,867]\u001b[0m Trial 453 finished with value: 0.08456203431506822 and parameters: {'n_estimators': 759, 'max_depth': 3, 'learning_rate': 0.2336589214848097, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:55,046]\u001b[0m Trial 454 finished with value: 0.08395906740770928 and parameters: {'n_estimators': 721, 'max_depth': 3, 'learning_rate': 0.15787298833549748, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:56,418]\u001b[0m Trial 455 finished with value: 0.08721133673682199 and parameters: {'n_estimators': 740, 'max_depth': 4, 'learning_rate': 0.17751095265142924, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:57,535]\u001b[0m Trial 456 finished with value: 0.08326946965557938 and parameters: {'n_estimators': 709, 'max_depth': 3, 'learning_rate': 0.15386769996771957, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:17:58,599]\u001b[0m Trial 457 finished with value: 0.08277857338975389 and parameters: {'n_estimators': 725, 'max_depth': 3, 'learning_rate': 0.16468430777013007, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:18:00,024]\u001b[0m Trial 458 finished with value: 0.08516307424345448 and parameters: {'n_estimators': 822, 'max_depth': 3, 'learning_rate': 0.19190295617452022, 'min_child_weight': 8}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:18:01,779]\u001b[0m Trial 459 finished with value: 0.08874462697002515 and parameters: {'n_estimators': 707, 'max_depth': 5, 'learning_rate': 0.14084087470311535, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:18:03,185]\u001b[0m Trial 460 finished with value: 0.08388595508820527 and parameters: {'n_estimators': 763, 'max_depth': 3, 'learning_rate': 0.15067430377322058, 'min_child_weight': 9}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:18:09,172]\u001b[0m Trial 461 finished with value: 0.10162837463997096 and parameters: {'n_estimators': 689, 'max_depth': 10, 'learning_rate': 0.13641409772184496, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:18:10,283]\u001b[0m Trial 462 finished with value: 0.08349279022803455 and parameters: {'n_estimators': 728, 'max_depth': 3, 'learning_rate': 0.16998641553189445, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:18:11,388]\u001b[0m Trial 463 finished with value: 0.08273657163673434 and parameters: {'n_estimators': 711, 'max_depth': 3, 'learning_rate': 0.1452340280134206, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:18:12,533]\u001b[0m Trial 464 finished with value: 0.08376690305749647 and parameters: {'n_estimators': 743, 'max_depth': 3, 'learning_rate': 0.15406959595541272, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:18:14,432]\u001b[0m Trial 465 finished with value: 0.08253757560419685 and parameters: {'n_estimators': 785, 'max_depth': 3, 'learning_rate': 0.13461750161396244, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:18:21,865]\u001b[0m Trial 466 finished with value: 0.10402306282405352 and parameters: {'n_estimators': 688, 'max_depth': 11, 'learning_rate': 0.16202811440850795, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:18:22,995]\u001b[0m Trial 467 finished with value: 0.08277068997162221 and parameters: {'n_estimators': 699, 'max_depth': 3, 'learning_rate': 0.14432830559187199, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:18:24,363]\u001b[0m Trial 468 finished with value: 0.08764543348113721 and parameters: {'n_estimators': 676, 'max_depth': 4, 'learning_rate': 0.1843077873500812, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:18:25,519]\u001b[0m Trial 469 finished with value: 0.08260209306314711 and parameters: {'n_estimators': 717, 'max_depth': 3, 'learning_rate': 0.1297981904447046, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:18:26,799]\u001b[0m Trial 470 finished with value: 0.08246848587072822 and parameters: {'n_estimators': 699, 'max_depth': 3, 'learning_rate': 0.16160960233647767, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:18:27,955]\u001b[0m Trial 471 finished with value: 0.08308283039199962 and parameters: {'n_estimators': 680, 'max_depth': 3, 'learning_rate': 0.14669893473493567, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:18:29,103]\u001b[0m Trial 472 finished with value: 0.08247267673486194 and parameters: {'n_estimators': 730, 'max_depth': 3, 'learning_rate': 0.13552391130513808, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:18:37,031]\u001b[0m Trial 473 finished with value: 0.09189560105049469 and parameters: {'n_estimators': 752, 'max_depth': 9, 'learning_rate': 0.011695486704802017, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:18:38,552]\u001b[0m Trial 474 finished with value: 0.08462355983006513 and parameters: {'n_estimators': 706, 'max_depth': 4, 'learning_rate': 0.1251899490256019, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:18:39,685]\u001b[0m Trial 475 finished with value: 0.08329194609971924 and parameters: {'n_estimators': 670, 'max_depth': 3, 'learning_rate': 0.17405071845430634, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:18:40,814]\u001b[0m Trial 476 finished with value: 0.08316037595639489 and parameters: {'n_estimators': 692, 'max_depth': 3, 'learning_rate': 0.1512903399379467, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:18:41,906]\u001b[0m Trial 477 finished with value: 0.08317869616338323 and parameters: {'n_estimators': 643, 'max_depth': 3, 'learning_rate': 0.14052281899132224, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:18:43,268]\u001b[0m Trial 478 finished with value: 0.08316144874053474 and parameters: {'n_estimators': 726, 'max_depth': 3, 'learning_rate': 0.20997270438957022, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:18:43,664]\u001b[0m Trial 479 finished with value: 0.08959409565848214 and parameters: {'n_estimators': 179, 'max_depth': 3, 'learning_rate': 0.1586883559664302, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:18:44,853]\u001b[0m Trial 480 finished with value: 0.08402716720523631 and parameters: {'n_estimators': 678, 'max_depth': 3, 'learning_rate': 0.1202684553412959, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:18:46,424]\u001b[0m Trial 481 finished with value: 0.08400077230926636 and parameters: {'n_estimators': 711, 'max_depth': 4, 'learning_rate': 0.13239120491426323, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:18:47,538]\u001b[0m Trial 482 finished with value: 0.0829619142270787 and parameters: {'n_estimators': 657, 'max_depth': 3, 'learning_rate': 0.16887767963475198, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:18:48,699]\u001b[0m Trial 483 finished with value: 0.08283674928845328 and parameters: {'n_estimators': 694, 'max_depth': 3, 'learning_rate': 0.1444621329942297, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:18:50,436]\u001b[0m Trial 484 finished with value: 0.0847778461277469 and parameters: {'n_estimators': 742, 'max_depth': 4, 'learning_rate': 0.1250804739862036, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:18:51,632]\u001b[0m Trial 485 finished with value: 0.08344920849250995 and parameters: {'n_estimators': 675, 'max_depth': 3, 'learning_rate': 0.15458902228454457, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:18:52,787]\u001b[0m Trial 486 finished with value: 0.08523696176791473 and parameters: {'n_estimators': 708, 'max_depth': 3, 'learning_rate': 0.2991405518739113, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:18:53,979]\u001b[0m Trial 487 finished with value: 0.08725620307003272 and parameters: {'n_estimators': 658, 'max_depth': 3, 'learning_rate': 0.06341909075662817, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:01,245]\u001b[0m Trial 488 finished with value: 0.10397492307026904 and parameters: {'n_estimators': 695, 'max_depth': 12, 'learning_rate': 0.13585722981870904, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:02,765]\u001b[0m Trial 489 finished with value: 0.08595226907323969 and parameters: {'n_estimators': 725, 'max_depth': 4, 'learning_rate': 0.03559299288035241, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:07,311]\u001b[0m Trial 490 finished with value: 0.09921985101954633 and parameters: {'n_estimators': 679, 'max_depth': 8, 'learning_rate': 0.17793439781725956, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:08,487]\u001b[0m Trial 491 finished with value: 0.09533855013187104 and parameters: {'n_estimators': 628, 'max_depth': 3, 'learning_rate': 0.02922880309814184, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:09,984]\u001b[0m Trial 492 finished with value: 0.08309507191276091 and parameters: {'n_estimators': 712, 'max_depth': 3, 'learning_rate': 0.14805547376078393, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:11,152]\u001b[0m Trial 493 finished with value: 0.0826759786006139 and parameters: {'n_estimators': 655, 'max_depth': 3, 'learning_rate': 0.12810803752829156, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:12,474]\u001b[0m Trial 494 finished with value: 0.08225593586524416 and parameters: {'n_estimators': 758, 'max_depth': 3, 'learning_rate': 0.16305586850227, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:14,139]\u001b[0m Trial 495 finished with value: 0.08740274630438884 and parameters: {'n_estimators': 766, 'max_depth': 4, 'learning_rate': 0.18314857689295505, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:15,499]\u001b[0m Trial 496 finished with value: 0.08315660205916471 and parameters: {'n_estimators': 759, 'max_depth': 3, 'learning_rate': 0.16582335826027694, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:17,226]\u001b[0m Trial 497 finished with value: 0.08608837841090153 and parameters: {'n_estimators': 798, 'max_depth': 3, 'learning_rate': 0.1889788080938073, 'min_child_weight': 6}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:18,648]\u001b[0m Trial 498 finished with value: 0.08467202290989553 and parameters: {'n_estimators': 754, 'max_depth': 3, 'learning_rate': 0.19867192150944626, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:20,132]\u001b[0m Trial 499 finished with value: 0.097137117730765 and parameters: {'n_estimators': 737, 'max_depth': 3, 'learning_rate': 0.024671263408131753, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:21,637]\u001b[0m Trial 500 finished with value: 0.08275036333514421 and parameters: {'n_estimators': 769, 'max_depth': 3, 'learning_rate': 0.16011689792716266, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:23,144]\u001b[0m Trial 501 finished with value: 0.08386434363894248 and parameters: {'n_estimators': 776, 'max_depth': 3, 'learning_rate': 0.1717421002487217, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:24,735]\u001b[0m Trial 502 finished with value: 0.08188325999628746 and parameters: {'n_estimators': 737, 'max_depth': 3, 'learning_rate': 0.15733862092607928, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:25,840]\u001b[0m Trial 503 finished with value: 0.08557863261436732 and parameters: {'n_estimators': 460, 'max_depth': 4, 'learning_rate': 0.16101351839315967, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:27,371]\u001b[0m Trial 504 finished with value: 0.08446231710143276 and parameters: {'n_estimators': 792, 'max_depth': 3, 'learning_rate': 0.17340558680629345, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:28,769]\u001b[0m Trial 505 finished with value: 0.08368820469053169 and parameters: {'n_estimators': 745, 'max_depth': 3, 'learning_rate': 0.15518712282686534, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:30,292]\u001b[0m Trial 506 finished with value: 0.08285472367859309 and parameters: {'n_estimators': 747, 'max_depth': 3, 'learning_rate': 0.16911125650362807, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:31,714]\u001b[0m Trial 507 finished with value: 0.08303516672857214 and parameters: {'n_estimators': 731, 'max_depth': 3, 'learning_rate': 0.15387586919944926, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:33,480]\u001b[0m Trial 508 finished with value: 0.0833906914083489 and parameters: {'n_estimators': 785, 'max_depth': 3, 'learning_rate': 0.18505976207857144, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:34,997]\u001b[0m Trial 509 finished with value: 0.08366981604698515 and parameters: {'n_estimators': 759, 'max_depth': 3, 'learning_rate': 0.150620817177425, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:36,795]\u001b[0m Trial 510 finished with value: 0.08793276147551224 and parameters: {'n_estimators': 724, 'max_depth': 4, 'learning_rate': 0.1639096402034529, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:39,330]\u001b[0m Trial 511 finished with value: 0.08952814150800537 and parameters: {'n_estimators': 740, 'max_depth': 5, 'learning_rate': 0.14411853109779815, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:41,013]\u001b[0m Trial 512 finished with value: 0.08308096878529299 and parameters: {'n_estimators': 846, 'max_depth': 3, 'learning_rate': 0.15948355857136037, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:42,741]\u001b[0m Trial 513 finished with value: 0.08256381892015496 and parameters: {'n_estimators': 719, 'max_depth': 3, 'learning_rate': 0.1431969508858246, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:44,174]\u001b[0m Trial 514 finished with value: 0.08317662056064302 and parameters: {'n_estimators': 714, 'max_depth': 3, 'learning_rate': 0.17505496791476777, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:46,380]\u001b[0m Trial 515 finished with value: 0.08710183821325071 and parameters: {'n_estimators': 768, 'max_depth': 4, 'learning_rate': 0.1521824313864104, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:47,647]\u001b[0m Trial 516 finished with value: 0.0834428219383049 and parameters: {'n_estimators': 600, 'max_depth': 3, 'learning_rate': 0.14079333632945634, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:49,112]\u001b[0m Trial 517 finished with value: 0.0830258565559016 and parameters: {'n_estimators': 701, 'max_depth': 3, 'learning_rate': 0.16748254715754632, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:50,279]\u001b[0m Trial 518 finished with value: 0.08355652692921657 and parameters: {'n_estimators': 551, 'max_depth': 3, 'learning_rate': 0.1508259434454712, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:51,878]\u001b[0m Trial 519 finished with value: 0.08424998322924931 and parameters: {'n_estimators': 744, 'max_depth': 3, 'learning_rate': 0.13920228717514224, 'min_child_weight': 5}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:53,366]\u001b[0m Trial 520 finished with value: 0.08334819052906109 and parameters: {'n_estimators': 730, 'max_depth': 3, 'learning_rate': 0.20215020943658382, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:55,503]\u001b[0m Trial 521 finished with value: 0.08573110756005994 and parameters: {'n_estimators': 698, 'max_depth': 4, 'learning_rate': 0.16010869186884255, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:56,873]\u001b[0m Trial 522 finished with value: 0.08488722805270349 and parameters: {'n_estimators': 575, 'max_depth': 3, 'learning_rate': 0.17235734073239584, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:19:58,323]\u001b[0m Trial 523 finished with value: 0.08380901110932767 and parameters: {'n_estimators': 670, 'max_depth': 3, 'learning_rate': 0.18564339247771802, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:20:03,788]\u001b[0m Trial 524 finished with value: 0.09618439324678967 and parameters: {'n_estimators': 812, 'max_depth': 7, 'learning_rate': 0.13532132625393206, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:20:05,257]\u001b[0m Trial 525 finished with value: 0.0826032479888251 and parameters: {'n_estimators': 715, 'max_depth': 3, 'learning_rate': 0.14936499105247958, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:20:06,663]\u001b[0m Trial 526 finished with value: 0.0820830552828102 and parameters: {'n_estimators': 647, 'max_depth': 3, 'learning_rate': 0.1636173543482512, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:20:08,615]\u001b[0m Trial 527 finished with value: 0.0862818168369496 and parameters: {'n_estimators': 631, 'max_depth': 4, 'learning_rate': 0.16779973207844429, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:20:09,961]\u001b[0m Trial 528 finished with value: 0.08401134731429585 and parameters: {'n_estimators': 649, 'max_depth': 3, 'learning_rate': 0.18932139406284207, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:20:11,309]\u001b[0m Trial 529 finished with value: 0.08295871971481268 and parameters: {'n_estimators': 608, 'max_depth': 3, 'learning_rate': 0.1759281951779109, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:20:12,609]\u001b[0m Trial 530 finished with value: 0.08240986881096399 and parameters: {'n_estimators': 615, 'max_depth': 3, 'learning_rate': 0.16083495087714486, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:20:14,022]\u001b[0m Trial 531 finished with value: 0.0824353229340784 and parameters: {'n_estimators': 639, 'max_depth': 3, 'learning_rate': 0.1586389039230595, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:20:15,472]\u001b[0m Trial 532 finished with value: 0.08462405213082154 and parameters: {'n_estimators': 660, 'max_depth': 3, 'learning_rate': 0.18160363577172994, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:20:17,270]\u001b[0m Trial 533 finished with value: 0.08262857330685547 and parameters: {'n_estimators': 639, 'max_depth': 3, 'learning_rate': 0.11795921766904939, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:20:21,245]\u001b[0m Trial 534 finished with value: 0.09748615573746822 and parameters: {'n_estimators': 669, 'max_depth': 6, 'learning_rate': 0.22003759287571517, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:20:23,374]\u001b[0m Trial 535 finished with value: 0.08613884514448175 and parameters: {'n_estimators': 684, 'max_depth': 4, 'learning_rate': 0.14168182341121105, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:20:25,309]\u001b[0m Trial 536 finished with value: 0.08814720651422511 and parameters: {'n_estimators': 774, 'max_depth': 4, 'learning_rate': 0.15602796503694719, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:20:26,972]\u001b[0m Trial 537 finished with value: 0.0824457607071543 and parameters: {'n_estimators': 646, 'max_depth': 3, 'learning_rate': 0.135826115700917, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:20:28,482]\u001b[0m Trial 538 finished with value: 0.08331839738519489 and parameters: {'n_estimators': 667, 'max_depth': 3, 'learning_rate': 0.19901820311392066, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:20:30,011]\u001b[0m Trial 539 finished with value: 0.0821804083798195 and parameters: {'n_estimators': 755, 'max_depth': 3, 'learning_rate': 0.16828028166544565, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:20:31,675]\u001b[0m Trial 540 finished with value: 0.08350042182975628 and parameters: {'n_estimators': 800, 'max_depth': 3, 'learning_rate': 0.17375743716683584, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:20:33,302]\u001b[0m Trial 541 finished with value: 0.08311370306065799 and parameters: {'n_estimators': 746, 'max_depth': 3, 'learning_rate': 0.16887776040111105, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:20:35,216]\u001b[0m Trial 542 finished with value: 0.08886475197480367 and parameters: {'n_estimators': 785, 'max_depth': 3, 'learning_rate': 0.04239532779336404, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:20:36,719]\u001b[0m Trial 543 finished with value: 0.08374134130419003 and parameters: {'n_estimators': 758, 'max_depth': 3, 'learning_rate': 0.19309868150230067, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:20:38,416]\u001b[0m Trial 544 finished with value: 0.08466248680311633 and parameters: {'n_estimators': 758, 'max_depth': 3, 'learning_rate': 0.1825961726920937, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:20:40,116]\u001b[0m Trial 545 finished with value: 0.083197319026209 and parameters: {'n_estimators': 780, 'max_depth': 3, 'learning_rate': 0.16743890241568476, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:20:41,952]\u001b[0m Trial 546 finished with value: 0.0870576233729369 and parameters: {'n_estimators': 739, 'max_depth': 4, 'learning_rate': 0.16047225107521063, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:20:48,741]\u001b[0m Trial 547 finished with value: 0.10433184556727466 and parameters: {'n_estimators': 728, 'max_depth': 13, 'learning_rate': 0.17748280787919002, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:20:50,818]\u001b[0m Trial 548 finished with value: 0.08395723755226109 and parameters: {'n_estimators': 755, 'max_depth': 3, 'learning_rate': 0.1546329370993487, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:20:54,728]\u001b[0m Trial 549 finished with value: 0.08220024890163888 and parameters: {'n_estimators': 720, 'max_depth': 3, 'learning_rate': 0.11445061316291538, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:20:57,925]\u001b[0m Trial 550 finished with value: 0.08265994797689089 and parameters: {'n_estimators': 712, 'max_depth': 3, 'learning_rate': 0.14933984675748252, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:21:08,827]\u001b[0m Trial 551 finished with value: 0.10548729044772294 and parameters: {'n_estimators': 697, 'max_depth': 15, 'learning_rate': 0.11869519426304462, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:21:10,258]\u001b[0m Trial 552 finished with value: 0.08270016460923824 and parameters: {'n_estimators': 688, 'max_depth': 3, 'learning_rate': 0.1314266942777292, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:21:11,678]\u001b[0m Trial 553 finished with value: 0.08263693091573175 and parameters: {'n_estimators': 718, 'max_depth': 3, 'learning_rate': 0.1646660770959431, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:21:13,102]\u001b[0m Trial 554 finished with value: 0.08408285162449515 and parameters: {'n_estimators': 702, 'max_depth': 3, 'learning_rate': 0.18346210546331748, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:21:14,818]\u001b[0m Trial 555 finished with value: 0.08247965927491127 and parameters: {'n_estimators': 684, 'max_depth': 3, 'learning_rate': 0.14665408250232365, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:21:17,115]\u001b[0m Trial 556 finished with value: 0.08510520859519778 and parameters: {'n_estimators': 726, 'max_depth': 4, 'learning_rate': 0.15849957083137878, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:21:18,981]\u001b[0m Trial 557 finished with value: 0.08393599427281197 and parameters: {'n_estimators': 773, 'max_depth': 3, 'learning_rate': 0.07207864442815914, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:21:20,466]\u001b[0m Trial 558 finished with value: 0.08298244561042319 and parameters: {'n_estimators': 713, 'max_depth': 3, 'learning_rate': 0.23990274177223708, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:21:21,838]\u001b[0m Trial 559 finished with value: 0.08562942857561037 and parameters: {'n_estimators': 682, 'max_depth': 3, 'learning_rate': 0.19993462739990095, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:21:23,646]\u001b[0m Trial 560 finished with value: 0.08615259273943957 and parameters: {'n_estimators': 739, 'max_depth': 4, 'learning_rate': 0.1291054978076435, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:21:25,077]\u001b[0m Trial 561 finished with value: 0.08253164483660641 and parameters: {'n_estimators': 702, 'max_depth': 3, 'learning_rate': 0.13974655975435227, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:21:26,452]\u001b[0m Trial 562 finished with value: 0.0834048056413013 and parameters: {'n_estimators': 673, 'max_depth': 3, 'learning_rate': 0.1221763712058863, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:21:28,163]\u001b[0m Trial 563 finished with value: 0.0836444464588196 and parameters: {'n_estimators': 747, 'max_depth': 3, 'learning_rate': 0.170550105181068, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:21:30,149]\u001b[0m Trial 564 finished with value: 0.08644759051217846 and parameters: {'n_estimators': 696, 'max_depth': 4, 'learning_rate': 0.14891361566879485, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:21:39,261]\u001b[0m Trial 565 finished with value: 0.10161932762356388 and parameters: {'n_estimators': 728, 'max_depth': 10, 'learning_rate': 0.11356180837381391, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:21:39,970]\u001b[0m Trial 566 finished with value: 0.0860720895428612 and parameters: {'n_estimators': 314, 'max_depth': 3, 'learning_rate': 0.13583541771198668, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:21:41,268]\u001b[0m Trial 567 finished with value: 0.08284569478174883 and parameters: {'n_estimators': 668, 'max_depth': 3, 'learning_rate': 0.15734934105985496, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:21:42,628]\u001b[0m Trial 568 finished with value: 0.08371538532284166 and parameters: {'n_estimators': 710, 'max_depth': 3, 'learning_rate': 0.17387314862216935, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:21:44,227]\u001b[0m Trial 569 finished with value: 0.08264559635056794 and parameters: {'n_estimators': 687, 'max_depth': 3, 'learning_rate': 0.1467013206415009, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:21:45,546]\u001b[0m Trial 570 finished with value: 0.08284416313727293 and parameters: {'n_estimators': 656, 'max_depth': 3, 'learning_rate': 0.12659470809342516, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:21:46,826]\u001b[0m Trial 571 finished with value: 0.08391010104815012 and parameters: {'n_estimators': 587, 'max_depth': 3, 'learning_rate': 0.18780877261032047, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:21:48,457]\u001b[0m Trial 572 finished with value: 0.08260172216704732 and parameters: {'n_estimators': 764, 'max_depth': 3, 'learning_rate': 0.16724782217695516, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:21:50,179]\u001b[0m Trial 573 finished with value: 0.08581525248878688 and parameters: {'n_estimators': 721, 'max_depth': 4, 'learning_rate': 0.139296221738182, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:21:51,580]\u001b[0m Trial 574 finished with value: 0.08305984930644876 and parameters: {'n_estimators': 684, 'max_depth': 3, 'learning_rate': 0.15808084582199747, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:21:53,004]\u001b[0m Trial 575 finished with value: 0.08274444052530283 and parameters: {'n_estimators': 705, 'max_depth': 3, 'learning_rate': 0.12162667707954823, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:21:54,778]\u001b[0m Trial 576 finished with value: 0.08488695733354572 and parameters: {'n_estimators': 621, 'max_depth': 4, 'learning_rate': 0.1335461517126799, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:21:56,312]\u001b[0m Trial 577 finished with value: 0.08243008456223479 and parameters: {'n_estimators': 742, 'max_depth': 3, 'learning_rate': 0.15222553815925846, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:21:57,692]\u001b[0m Trial 578 finished with value: 0.0829948282257483 and parameters: {'n_estimators': 644, 'max_depth': 3, 'learning_rate': 0.11533890114285782, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:21:58,972]\u001b[0m Trial 579 finished with value: 0.08495372244600471 and parameters: {'n_estimators': 672, 'max_depth': 3, 'learning_rate': 0.17524720343810785, 'min_child_weight': 6}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:22:00,600]\u001b[0m Trial 580 finished with value: 0.08321977121587064 and parameters: {'n_estimators': 721, 'max_depth': 3, 'learning_rate': 0.10611981878291742, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:22:02,319]\u001b[0m Trial 581 finished with value: 0.08635692506489748 and parameters: {'n_estimators': 688, 'max_depth': 4, 'learning_rate': 0.14607377700060628, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:22:03,702]\u001b[0m Trial 582 finished with value: 0.08264673692125761 and parameters: {'n_estimators': 658, 'max_depth': 3, 'learning_rate': 0.1621275125004416, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:22:05,053]\u001b[0m Trial 583 finished with value: 0.08186846040844238 and parameters: {'n_estimators': 704, 'max_depth': 3, 'learning_rate': 0.13006012578130702, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:22:06,524]\u001b[0m Trial 584 finished with value: 0.08263059881485445 and parameters: {'n_estimators': 733, 'max_depth': 3, 'learning_rate': 0.1162286580357564, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:22:14,569]\u001b[0m Trial 585 finished with value: 0.10441472501698376 and parameters: {'n_estimators': 705, 'max_depth': 14, 'learning_rate': 0.12564307662319638, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:22:23,908]\u001b[0m Trial 586 finished with value: 0.10528485609977607 and parameters: {'n_estimators': 746, 'max_depth': 13, 'learning_rate': 0.1310937377290127, 'min_child_weight': 5}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:22:25,299]\u001b[0m Trial 587 finished with value: 0.08499971771788298 and parameters: {'n_estimators': 760, 'max_depth': 3, 'learning_rate': 0.21202204518730663, 'min_child_weight': 7}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:22:27,994]\u001b[0m Trial 588 finished with value: 0.09259959919221114 and parameters: {'n_estimators': 719, 'max_depth': 6, 'learning_rate': 0.12345328546036881, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:22:29,304]\u001b[0m Trial 589 finished with value: 0.08261318952776894 and parameters: {'n_estimators': 702, 'max_depth': 3, 'learning_rate': 0.1375073568238676, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:22:31,160]\u001b[0m Trial 590 finished with value: 0.08385959899234947 and parameters: {'n_estimators': 789, 'max_depth': 3, 'learning_rate': 0.18138623007183724, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:22:32,765]\u001b[0m Trial 591 finished with value: 0.0826226315654416 and parameters: {'n_estimators': 725, 'max_depth': 3, 'learning_rate': 0.10721237733687917, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:22:33,993]\u001b[0m Trial 592 finished with value: 0.08258107343944868 and parameters: {'n_estimators': 693, 'max_depth': 3, 'learning_rate': 0.1301156789008013, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:22:35,593]\u001b[0m Trial 593 finished with value: 0.08331235626742295 and parameters: {'n_estimators': 712, 'max_depth': 4, 'learning_rate': 0.09985057175362787, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:22:36,906]\u001b[0m Trial 594 finished with value: 0.0834970814550906 and parameters: {'n_estimators': 737, 'max_depth': 3, 'learning_rate': 0.11641363489817311, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:22:38,399]\u001b[0m Trial 595 finished with value: 0.08322184354859442 and parameters: {'n_estimators': 766, 'max_depth': 3, 'learning_rate': 0.15415071727416532, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:22:39,706]\u001b[0m Trial 596 finished with value: 0.08366762990690224 and parameters: {'n_estimators': 677, 'max_depth': 3, 'learning_rate': 0.1659593999911231, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:22:40,887]\u001b[0m Trial 597 finished with value: 0.08232107545407749 and parameters: {'n_estimators': 694, 'max_depth': 3, 'learning_rate': 0.14068887185172663, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:22:42,569]\u001b[0m Trial 598 finished with value: 0.08502953198831671 and parameters: {'n_estimators': 753, 'max_depth': 4, 'learning_rate': 0.12515483968268246, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:22:43,822]\u001b[0m Trial 599 finished with value: 0.08469003944748557 and parameters: {'n_estimators': 714, 'max_depth': 3, 'learning_rate': 0.19219842381316604, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:22:45,015]\u001b[0m Trial 600 finished with value: 0.08222412784889119 and parameters: {'n_estimators': 671, 'max_depth': 3, 'learning_rate': 0.14906157628556277, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:22:46,296]\u001b[0m Trial 601 finished with value: 0.08312009935117741 and parameters: {'n_estimators': 629, 'max_depth': 3, 'learning_rate': 0.16936286465080716, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:22:47,720]\u001b[0m Trial 602 finished with value: 0.08571363180020228 and parameters: {'n_estimators': 652, 'max_depth': 4, 'learning_rate': 0.153887437635172, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:22:48,877]\u001b[0m Trial 603 finished with value: 0.08237007825013624 and parameters: {'n_estimators': 665, 'max_depth': 3, 'learning_rate': 0.16274267235643122, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:22:50,283]\u001b[0m Trial 604 finished with value: 0.0835774816385849 and parameters: {'n_estimators': 821, 'max_depth': 3, 'learning_rate': 0.176928520773795, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:22:51,468]\u001b[0m Trial 605 finished with value: 0.08347237759742218 and parameters: {'n_estimators': 666, 'max_depth': 3, 'learning_rate': 0.15065712291382669, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:22:52,545]\u001b[0m Trial 606 finished with value: 0.08308469365588271 and parameters: {'n_estimators': 647, 'max_depth': 3, 'learning_rate': 0.1454813109432611, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:22:53,964]\u001b[0m Trial 607 finished with value: 0.08238220838082617 and parameters: {'n_estimators': 676, 'max_depth': 3, 'learning_rate': 0.1618673397027641, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:22:55,050]\u001b[0m Trial 608 finished with value: 0.08422243198098708 and parameters: {'n_estimators': 632, 'max_depth': 3, 'learning_rate': 0.17909784336259274, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:22:57,279]\u001b[0m Trial 609 finished with value: 0.08466688799410906 and parameters: {'n_estimators': 778, 'max_depth': 4, 'learning_rate': 0.1354133755789244, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:22:58,632]\u001b[0m Trial 610 finished with value: 0.08329245959958595 and parameters: {'n_estimators': 694, 'max_depth': 3, 'learning_rate': 0.15562329873403538, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:22:59,792]\u001b[0m Trial 611 finished with value: 0.0831207423118143 and parameters: {'n_estimators': 678, 'max_depth': 3, 'learning_rate': 0.11082842859136707, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:01,094]\u001b[0m Trial 612 finished with value: 0.08315877480983573 and parameters: {'n_estimators': 729, 'max_depth': 3, 'learning_rate': 0.1443576064961453, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:02,533]\u001b[0m Trial 613 finished with value: 0.0852580342864113 and parameters: {'n_estimators': 660, 'max_depth': 4, 'learning_rate': 0.16643887788763198, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:03,662]\u001b[0m Trial 614 finished with value: 0.08307430873764746 and parameters: {'n_estimators': 614, 'max_depth': 3, 'learning_rate': 0.12999802253950452, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:05,082]\u001b[0m Trial 615 finished with value: 0.08793018395955414 and parameters: {'n_estimators': 707, 'max_depth': 3, 'learning_rate': 0.05307825821680772, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:06,379]\u001b[0m Trial 616 finished with value: 0.0830730472577487 and parameters: {'n_estimators': 741, 'max_depth': 3, 'learning_rate': 0.15087916556331513, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:07,588]\u001b[0m Trial 617 finished with value: 0.08393387458472819 and parameters: {'n_estimators': 683, 'max_depth': 3, 'learning_rate': 0.20139288286607787, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:08,935]\u001b[0m Trial 618 finished with value: 0.08540937207454335 and parameters: {'n_estimators': 798, 'max_depth': 3, 'learning_rate': 0.1891221547028895, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:10,085]\u001b[0m Trial 619 finished with value: 0.08348777746597265 and parameters: {'n_estimators': 644, 'max_depth': 3, 'learning_rate': 0.11685827255936913, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:11,494]\u001b[0m Trial 620 finished with value: 0.08265909677459007 and parameters: {'n_estimators': 714, 'max_depth': 3, 'learning_rate': 0.13979189757050017, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:12,826]\u001b[0m Trial 621 finished with value: 0.08387453848047574 and parameters: {'n_estimators': 695, 'max_depth': 3, 'learning_rate': 0.1743654188293742, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:14,118]\u001b[0m Trial 622 finished with value: 0.08635036092591931 and parameters: {'n_estimators': 660, 'max_depth': 4, 'learning_rate': 0.1625627713276094, 'min_child_weight': 10}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:15,613]\u001b[0m Trial 623 finished with value: 0.0829349762863024 and parameters: {'n_estimators': 727, 'max_depth': 3, 'learning_rate': 0.12161010463579387, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:16,773]\u001b[0m Trial 624 finished with value: 0.08241289957264089 and parameters: {'n_estimators': 679, 'max_depth': 3, 'learning_rate': 0.14859722881593906, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:23,600]\u001b[0m Trial 625 finished with value: 0.10413855843091069 and parameters: {'n_estimators': 745, 'max_depth': 11, 'learning_rate': 0.1345603131370177, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:24,829]\u001b[0m Trial 626 finished with value: 0.08253266605259721 and parameters: {'n_estimators': 708, 'max_depth': 3, 'learning_rate': 0.15705356794431038, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:26,354]\u001b[0m Trial 627 finished with value: 0.08648364062869808 and parameters: {'n_estimators': 693, 'max_depth': 4, 'learning_rate': 0.14182146726019546, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:27,634]\u001b[0m Trial 628 finished with value: 0.08191460734977042 and parameters: {'n_estimators': 757, 'max_depth': 3, 'learning_rate': 0.12728882667495164, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:28,916]\u001b[0m Trial 629 finished with value: 0.08310340315683169 and parameters: {'n_estimators': 671, 'max_depth': 3, 'learning_rate': 0.12348797862714754, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:30,392]\u001b[0m Trial 630 finished with value: 0.0829548528835559 and parameters: {'n_estimators': 641, 'max_depth': 4, 'learning_rate': 0.10213511870390063, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:31,648]\u001b[0m Trial 631 finished with value: 0.0864318137649662 and parameters: {'n_estimators': 721, 'max_depth': 3, 'learning_rate': 0.05990862607331417, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:32,902]\u001b[0m Trial 632 finished with value: 0.0829293169593892 and parameters: {'n_estimators': 701, 'max_depth': 3, 'learning_rate': 0.11224112088197917, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:34,350]\u001b[0m Trial 633 finished with value: 0.10305693039776236 and parameters: {'n_estimators': 663, 'max_depth': 3, 'learning_rate': 0.018462722617312657, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:35,476]\u001b[0m Trial 634 finished with value: 0.08356895669343908 and parameters: {'n_estimators': 683, 'max_depth': 3, 'learning_rate': 0.12860933352892376, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:36,876]\u001b[0m Trial 635 finished with value: 0.08244647842569405 and parameters: {'n_estimators': 738, 'max_depth': 3, 'learning_rate': 0.1187443507234804, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:38,286]\u001b[0m Trial 636 finished with value: 0.08415434846452788 and parameters: {'n_estimators': 708, 'max_depth': 3, 'learning_rate': 0.09466380305505799, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:39,394]\u001b[0m Trial 637 finished with value: 0.0830010827587181 and parameters: {'n_estimators': 650, 'max_depth': 3, 'learning_rate': 0.1272006463263806, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:41,240]\u001b[0m Trial 638 finished with value: 0.08438485467873562 and parameters: {'n_estimators': 773, 'max_depth': 4, 'learning_rate': 0.10957877901997236, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:42,713]\u001b[0m Trial 639 finished with value: 0.08223489665962366 and parameters: {'n_estimators': 728, 'max_depth': 3, 'learning_rate': 0.13208713022608695, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:43,907]\u001b[0m Trial 640 finished with value: 0.0827530514167065 and parameters: {'n_estimators': 689, 'max_depth': 3, 'learning_rate': 0.12141121216988328, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:45,379]\u001b[0m Trial 641 finished with value: 0.08357849234207144 and parameters: {'n_estimators': 670, 'max_depth': 4, 'learning_rate': 0.13575999819655604, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:46,926]\u001b[0m Trial 642 finished with value: 0.08273626391953012 and parameters: {'n_estimators': 755, 'max_depth': 3, 'learning_rate': 0.11411617134434474, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:48,496]\u001b[0m Trial 643 finished with value: 0.08404602275072952 and parameters: {'n_estimators': 919, 'max_depth': 3, 'learning_rate': 0.13851187530982773, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:49,586]\u001b[0m Trial 644 finished with value: 0.08231460044689605 and parameters: {'n_estimators': 615, 'max_depth': 3, 'learning_rate': 0.12837077796045013, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:51,134]\u001b[0m Trial 645 finished with value: 0.08210942459292067 and parameters: {'n_estimators': 713, 'max_depth': 3, 'learning_rate': 0.14511033084209582, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:52,434]\u001b[0m Trial 646 finished with value: 0.08321989252062911 and parameters: {'n_estimators': 734, 'max_depth': 3, 'learning_rate': 0.14401067022835423, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:53,663]\u001b[0m Trial 647 finished with value: 0.08319843094976215 and parameters: {'n_estimators': 715, 'max_depth': 3, 'learning_rate': 0.12436072079592768, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:55,451]\u001b[0m Trial 648 finished with value: 0.08535502837758135 and parameters: {'n_estimators': 752, 'max_depth': 4, 'learning_rate': 0.12989283455077824, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:56,264]\u001b[0m Trial 649 finished with value: 0.083684048231146 and parameters: {'n_estimators': 435, 'max_depth': 3, 'learning_rate': 0.13781332932286536, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:57,574]\u001b[0m Trial 650 finished with value: 0.0829440056016863 and parameters: {'n_estimators': 718, 'max_depth': 3, 'learning_rate': 0.14833361627149233, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:23:58,759]\u001b[0m Trial 651 finished with value: 0.08306201265224991 and parameters: {'n_estimators': 702, 'max_depth': 3, 'learning_rate': 0.1374834606729474, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:00,315]\u001b[0m Trial 652 finished with value: 0.08321963632160116 and parameters: {'n_estimators': 769, 'max_depth': 3, 'learning_rate': 0.11893274146526779, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:01,949]\u001b[0m Trial 653 finished with value: 0.08523513451119122 and parameters: {'n_estimators': 732, 'max_depth': 4, 'learning_rate': 0.10678818313067216, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:02,472]\u001b[0m Trial 654 finished with value: 0.08957199257037188 and parameters: {'n_estimators': 214, 'max_depth': 3, 'learning_rate': 0.14703802506055938, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:08,490]\u001b[0m Trial 655 finished with value: 0.10218071104046678 and parameters: {'n_estimators': 700, 'max_depth': 10, 'learning_rate': 0.13239896913106922, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:09,768]\u001b[0m Trial 656 finished with value: 0.08365996670182504 and parameters: {'n_estimators': 744, 'max_depth': 3, 'learning_rate': 0.12153543318508482, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:10,937]\u001b[0m Trial 657 finished with value: 0.08288414315622594 and parameters: {'n_estimators': 708, 'max_depth': 3, 'learning_rate': 0.15315080516969123, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:12,506]\u001b[0m Trial 658 finished with value: 0.0831609409532332 and parameters: {'n_estimators': 786, 'max_depth': 3, 'learning_rate': 0.14177711843725133, 'min_child_weight': 5}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:13,811]\u001b[0m Trial 659 finished with value: 0.08563748131796066 and parameters: {'n_estimators': 723, 'max_depth': 3, 'learning_rate': 0.2548747241399837, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:14,986]\u001b[0m Trial 660 finished with value: 0.08267228190331319 and parameters: {'n_estimators': 690, 'max_depth': 3, 'learning_rate': 0.13252029174315946, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:16,477]\u001b[0m Trial 661 finished with value: 0.08351072308999888 and parameters: {'n_estimators': 728, 'max_depth': 3, 'learning_rate': 0.11341129320436681, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:18,089]\u001b[0m Trial 662 finished with value: 0.08595073621997369 and parameters: {'n_estimators': 754, 'max_depth': 4, 'learning_rate': 0.15019659428991877, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:19,070]\u001b[0m Trial 663 finished with value: 0.0830822013310413 and parameters: {'n_estimators': 517, 'max_depth': 3, 'learning_rate': 0.1253339839805133, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:20,932]\u001b[0m Trial 664 finished with value: 0.09276760202112158 and parameters: {'n_estimators': 373, 'max_depth': 7, 'learning_rate': 0.1439683151793777, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:22,123]\u001b[0m Trial 665 finished with value: 0.08320761930510705 and parameters: {'n_estimators': 687, 'max_depth': 3, 'learning_rate': 0.15674875798905688, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:23,513]\u001b[0m Trial 666 finished with value: 0.08320375706140715 and parameters: {'n_estimators': 714, 'max_depth': 3, 'learning_rate': 0.13211047169099865, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:25,499]\u001b[0m Trial 667 finished with value: 0.087980450153498 and parameters: {'n_estimators': 676, 'max_depth': 5, 'learning_rate': 0.11769398280515334, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:26,675]\u001b[0m Trial 668 finished with value: 0.08257120222868822 and parameters: {'n_estimators': 696, 'max_depth': 3, 'learning_rate': 0.14734442027895514, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:28,022]\u001b[0m Trial 669 finished with value: 0.08382900630949686 and parameters: {'n_estimators': 591, 'max_depth': 4, 'learning_rate': 0.12509698131434807, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:29,289]\u001b[0m Trial 670 finished with value: 0.08347497757570743 and parameters: {'n_estimators': 657, 'max_depth': 3, 'learning_rate': 0.1027230032157396, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:30,617]\u001b[0m Trial 671 finished with value: 0.09168496044283138 and parameters: {'n_estimators': 737, 'max_depth': 3, 'learning_rate': 0.03320702970818471, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:31,725]\u001b[0m Trial 672 finished with value: 0.08287356761286935 and parameters: {'n_estimators': 638, 'max_depth': 3, 'learning_rate': 0.13862866638249727, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:36,448]\u001b[0m Trial 673 finished with value: 0.09610522792691246 and parameters: {'n_estimators': 707, 'max_depth': 8, 'learning_rate': 0.08533593395197639, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:37,668]\u001b[0m Trial 674 finished with value: 0.0840937911692142 and parameters: {'n_estimators': 670, 'max_depth': 3, 'learning_rate': 0.16938689419197644, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:38,981]\u001b[0m Trial 675 finished with value: 0.08320281863107817 and parameters: {'n_estimators': 774, 'max_depth': 3, 'learning_rate': 0.15245795177883723, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:40,613]\u001b[0m Trial 676 finished with value: 0.0840821741688485 and parameters: {'n_estimators': 720, 'max_depth': 4, 'learning_rate': 0.11001526821218946, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:41,895]\u001b[0m Trial 677 finished with value: 0.08296261071055012 and parameters: {'n_estimators': 690, 'max_depth': 3, 'learning_rate': 0.13191424627600118, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:43,690]\u001b[0m Trial 678 finished with value: 0.08702358037168506 and parameters: {'n_estimators': 745, 'max_depth': 4, 'learning_rate': 0.16020674929321868, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:45,100]\u001b[0m Trial 679 finished with value: 0.08304670690396758 and parameters: {'n_estimators': 808, 'max_depth': 3, 'learning_rate': 0.14045922285529247, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:46,208]\u001b[0m Trial 680 finished with value: 0.08282960813412675 and parameters: {'n_estimators': 653, 'max_depth': 3, 'learning_rate': 0.11801468492815838, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:47,409]\u001b[0m Trial 681 finished with value: 0.08504832917070236 and parameters: {'n_estimators': 705, 'max_depth': 3, 'learning_rate': 0.17801388115925534, 'min_child_weight': 9}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:48,546]\u001b[0m Trial 682 finished with value: 0.08280821131692603 and parameters: {'n_estimators': 675, 'max_depth': 3, 'learning_rate': 0.15503598270921382, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:50,067]\u001b[0m Trial 683 finished with value: 0.08357559868071203 and parameters: {'n_estimators': 725, 'max_depth': 3, 'learning_rate': 0.13064695733567486, 'min_child_weight': 7}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:55,397]\u001b[0m Trial 684 finished with value: 0.10030268846476795 and parameters: {'n_estimators': 628, 'max_depth': 9, 'learning_rate': 0.14111761936405912, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:56,971]\u001b[0m Trial 685 finished with value: 0.0830115284099357 and parameters: {'n_estimators': 758, 'max_depth': 3, 'learning_rate': 0.12237132441088976, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:57,625]\u001b[0m Trial 686 finished with value: 0.08538684115689459 and parameters: {'n_estimators': 266, 'max_depth': 3, 'learning_rate': 0.16069380900622773, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:24:58,905]\u001b[0m Trial 687 finished with value: 0.08316314519172648 and parameters: {'n_estimators': 690, 'max_depth': 3, 'learning_rate': 0.16967395092410117, 'min_child_weight': 5}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:00,685]\u001b[0m Trial 688 finished with value: 0.08674664611182713 and parameters: {'n_estimators': 712, 'max_depth': 4, 'learning_rate': 0.1433118490970702, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:01,882]\u001b[0m Trial 689 finished with value: 0.0833060353078631 and parameters: {'n_estimators': 664, 'max_depth': 3, 'learning_rate': 0.18556228523366272, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:03,032]\u001b[0m Trial 690 finished with value: 0.08276759290831337 and parameters: {'n_estimators': 686, 'max_depth': 3, 'learning_rate': 0.15017171241304128, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:04,502]\u001b[0m Trial 691 finished with value: 0.082636621675815 and parameters: {'n_estimators': 734, 'max_depth': 3, 'learning_rate': 0.12509694690062123, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:05,732]\u001b[0m Trial 692 finished with value: 0.08519768869200897 and parameters: {'n_estimators': 705, 'max_depth': 3, 'learning_rate': 0.07757191136502622, 'min_child_weight': 6}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:06,896]\u001b[0m Trial 693 finished with value: 0.08343585614273649 and parameters: {'n_estimators': 677, 'max_depth': 3, 'learning_rate': 0.11311557171009838, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:08,058]\u001b[0m Trial 694 finished with value: 0.08348041210697296 and parameters: {'n_estimators': 650, 'max_depth': 3, 'learning_rate': 0.1292691009335565, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:09,496]\u001b[0m Trial 695 finished with value: 0.08479695608461686 and parameters: {'n_estimators': 564, 'max_depth': 4, 'learning_rate': 0.13649513107876404, 'min_child_weight': 8}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:15,084]\u001b[0m Trial 696 finished with value: 0.10471228685959055 and parameters: {'n_estimators': 747, 'max_depth': 12, 'learning_rate': 0.1676914202667482, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:16,341]\u001b[0m Trial 697 finished with value: 0.08411816094404848 and parameters: {'n_estimators': 720, 'max_depth': 3, 'learning_rate': 0.1567562080628874, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:17,511]\u001b[0m Trial 698 finished with value: 0.08253897607087386 and parameters: {'n_estimators': 695, 'max_depth': 3, 'learning_rate': 0.14511749037242144, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:18,722]\u001b[0m Trial 699 finished with value: 0.08282121749834048 and parameters: {'n_estimators': 668, 'max_depth': 3, 'learning_rate': 0.11873708855386163, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:20,186]\u001b[0m Trial 700 finished with value: 0.08301176176181067 and parameters: {'n_estimators': 734, 'max_depth': 3, 'learning_rate': 0.13539981108321883, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:21,518]\u001b[0m Trial 701 finished with value: 0.08422872819064456 and parameters: {'n_estimators': 764, 'max_depth': 3, 'learning_rate': 0.18124041964484894, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:22,755]\u001b[0m Trial 702 finished with value: 0.08248307926521628 and parameters: {'n_estimators': 689, 'max_depth': 3, 'learning_rate': 0.15166500535799038, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:24,580]\u001b[0m Trial 703 finished with value: 0.0873169601423071 and parameters: {'n_estimators': 783, 'max_depth': 4, 'learning_rate': 0.1637461998205455, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:26,221]\u001b[0m Trial 704 finished with value: 0.08403867263588274 and parameters: {'n_estimators': 703, 'max_depth': 4, 'learning_rate': 0.11011895534471687, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:27,330]\u001b[0m Trial 705 finished with value: 0.08360671456693249 and parameters: {'n_estimators': 628, 'max_depth': 3, 'learning_rate': 0.12647269102228934, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:28,300]\u001b[0m Trial 706 finished with value: 0.0873387381930901 and parameters: {'n_estimators': 541, 'max_depth': 3, 'learning_rate': 0.06671503268327093, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:29,497]\u001b[0m Trial 707 finished with value: 0.08303636122381383 and parameters: {'n_estimators': 720, 'max_depth': 3, 'learning_rate': 0.13622406301697593, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:30,709]\u001b[0m Trial 708 finished with value: 0.08429013910580663 and parameters: {'n_estimators': 655, 'max_depth': 3, 'learning_rate': 0.19430527365763028, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:31,874]\u001b[0m Trial 709 finished with value: 0.08245037943250491 and parameters: {'n_estimators': 676, 'max_depth': 3, 'learning_rate': 0.14672297255525574, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:33,642]\u001b[0m Trial 710 finished with value: 0.08731979299476997 and parameters: {'n_estimators': 705, 'max_depth': 4, 'learning_rate': 0.17251366606197688, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:34,911]\u001b[0m Trial 711 finished with value: 0.0831649244330491 and parameters: {'n_estimators': 744, 'max_depth': 3, 'learning_rate': 0.1227627131097038, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:36,026]\u001b[0m Trial 712 finished with value: 0.08299715585650622 and parameters: {'n_estimators': 640, 'max_depth': 3, 'learning_rate': 0.1043974306177286, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:37,472]\u001b[0m Trial 713 finished with value: 0.08383338746534745 and parameters: {'n_estimators': 688, 'max_depth': 3, 'learning_rate': 0.15186767676099394, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:38,787]\u001b[0m Trial 714 finished with value: 0.08300374574868034 and parameters: {'n_estimators': 727, 'max_depth': 3, 'learning_rate': 0.14112833497126306, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:40,272]\u001b[0m Trial 715 finished with value: 0.08221742281473433 and parameters: {'n_estimators': 763, 'max_depth': 3, 'learning_rate': 0.1607043090824309, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:41,852]\u001b[0m Trial 716 finished with value: 0.08328189460011386 and parameters: {'n_estimators': 809, 'max_depth': 3, 'learning_rate': 0.1654628802533645, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:43,205]\u001b[0m Trial 717 finished with value: 0.08380170444242878 and parameters: {'n_estimators': 783, 'max_depth': 3, 'learning_rate': 0.1818743958189221, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:45,200]\u001b[0m Trial 718 finished with value: 0.08724155313411035 and parameters: {'n_estimators': 830, 'max_depth': 4, 'learning_rate': 0.1671540803995799, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:46,568]\u001b[0m Trial 719 finished with value: 0.08266658783360512 and parameters: {'n_estimators': 765, 'max_depth': 3, 'learning_rate': 0.0974637405572828, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:47,927]\u001b[0m Trial 720 finished with value: 0.0827461062704319 and parameters: {'n_estimators': 797, 'max_depth': 3, 'learning_rate': 0.1577915077952911, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:49,487]\u001b[0m Trial 721 finished with value: 0.08366220360416872 and parameters: {'n_estimators': 772, 'max_depth': 3, 'learning_rate': 0.17591148912188492, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:51,149]\u001b[0m Trial 722 finished with value: 0.08926097460933158 and parameters: {'n_estimators': 755, 'max_depth': 4, 'learning_rate': 0.20942361729892106, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:52,436]\u001b[0m Trial 723 finished with value: 0.0820128908958929 and parameters: {'n_estimators': 758, 'max_depth': 3, 'learning_rate': 0.11250294670220395, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:54,020]\u001b[0m Trial 724 finished with value: 0.08239676192015702 and parameters: {'n_estimators': 748, 'max_depth': 3, 'learning_rate': 0.10386498814694381, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:55,295]\u001b[0m Trial 725 finished with value: 0.08335814330410822 and parameters: {'n_estimators': 737, 'max_depth': 3, 'learning_rate': 0.11141623451369799, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:25:56,645]\u001b[0m Trial 726 finished with value: 0.08271122899355499 and parameters: {'n_estimators': 779, 'max_depth': 3, 'learning_rate': 0.11133536494010217, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:00,364]\u001b[0m Trial 727 finished with value: 0.09320347156581618 and parameters: {'n_estimators': 729, 'max_depth': 7, 'learning_rate': 0.09553541288513004, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:01,703]\u001b[0m Trial 728 finished with value: 0.08244578693077234 and parameters: {'n_estimators': 755, 'max_depth': 3, 'learning_rate': 0.12122899050678361, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:03,507]\u001b[0m Trial 729 finished with value: 0.08455687464846962 and parameters: {'n_estimators': 719, 'max_depth': 4, 'learning_rate': 0.11812073798303309, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:04,782]\u001b[0m Trial 730 finished with value: 0.0827144043953543 and parameters: {'n_estimators': 740, 'max_depth': 3, 'learning_rate': 0.10623457439146874, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:05,172]\u001b[0m Trial 731 finished with value: 0.09735407733563475 and parameters: {'n_estimators': 125, 'max_depth': 3, 'learning_rate': 0.12960668394221866, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:06,392]\u001b[0m Trial 732 finished with value: 0.08311742873296009 and parameters: {'n_estimators': 713, 'max_depth': 3, 'learning_rate': 0.1159218586730344, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:07,973]\u001b[0m Trial 733 finished with value: 0.08253193283787222 and parameters: {'n_estimators': 791, 'max_depth': 3, 'learning_rate': 0.11586838567679873, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:09,528]\u001b[0m Trial 734 finished with value: 0.08503616705876 and parameters: {'n_estimators': 704, 'max_depth': 4, 'learning_rate': 0.12954164878827032, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:10,839]\u001b[0m Trial 735 finished with value: 0.08229462409623334 and parameters: {'n_estimators': 736, 'max_depth': 3, 'learning_rate': 0.1241353813681686, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:19,678]\u001b[0m Trial 736 finished with value: 0.10227328262969504 and parameters: {'n_estimators': 720, 'max_depth': 11, 'learning_rate': 0.10263740811550334, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:21,010]\u001b[0m Trial 737 finished with value: 0.08244098270982371 and parameters: {'n_estimators': 769, 'max_depth': 3, 'learning_rate': 0.1346284737200319, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:22,567]\u001b[0m Trial 738 finished with value: 0.08458546705476075 and parameters: {'n_estimators': 698, 'max_depth': 4, 'learning_rate': 0.11288754244855999, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:23,978]\u001b[0m Trial 739 finished with value: 0.08245023856627462 and parameters: {'n_estimators': 752, 'max_depth': 3, 'learning_rate': 0.12166117836382019, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:25,325]\u001b[0m Trial 740 finished with value: 0.08390096654345602 and parameters: {'n_estimators': 712, 'max_depth': 3, 'learning_rate': 0.13252920054366898, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:26,492]\u001b[0m Trial 741 finished with value: 0.08261727710396506 and parameters: {'n_estimators': 684, 'max_depth': 3, 'learning_rate': 0.1402095639822616, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:28,181]\u001b[0m Trial 742 finished with value: 0.08296341485333819 and parameters: {'n_estimators': 868, 'max_depth': 3, 'learning_rate': 0.12351673195883892, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:29,319]\u001b[0m Trial 743 finished with value: 0.0843856743061751 and parameters: {'n_estimators': 603, 'max_depth': 3, 'learning_rate': 0.08946796956625185, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:30,543]\u001b[0m Trial 744 finished with value: 0.0826403264007882 and parameters: {'n_estimators': 732, 'max_depth': 3, 'learning_rate': 0.11512193984860616, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:31,953]\u001b[0m Trial 745 finished with value: 0.08236202033659863 and parameters: {'n_estimators': 697, 'max_depth': 3, 'learning_rate': 0.13053703552390591, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:33,631]\u001b[0m Trial 746 finished with value: 0.08491342896501539 and parameters: {'n_estimators': 751, 'max_depth': 4, 'learning_rate': 0.10780363924906386, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:34,915]\u001b[0m Trial 747 finished with value: 0.08249553332142545 and parameters: {'n_estimators': 721, 'max_depth': 3, 'learning_rate': 0.1381036787311191, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:36,086]\u001b[0m Trial 748 finished with value: 0.08248501382813546 and parameters: {'n_estimators': 667, 'max_depth': 3, 'learning_rate': 0.14275099212714165, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:37,490]\u001b[0m Trial 749 finished with value: 0.08325999242968649 and parameters: {'n_estimators': 689, 'max_depth': 3, 'learning_rate': 0.1263790013532831, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:38,719]\u001b[0m Trial 750 finished with value: 0.08317749236739873 and parameters: {'n_estimators': 705, 'max_depth': 3, 'learning_rate': 0.15401231759832698, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:40,491]\u001b[0m Trial 751 finished with value: 0.08683336062238954 and parameters: {'n_estimators': 735, 'max_depth': 4, 'learning_rate': 0.14524438384179114, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:42,008]\u001b[0m Trial 752 finished with value: 0.0816765138371928 and parameters: {'n_estimators': 769, 'max_depth': 3, 'learning_rate': 0.11833117503486733, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:43,471]\u001b[0m Trial 753 finished with value: 0.08259875051057868 and parameters: {'n_estimators': 808, 'max_depth': 3, 'learning_rate': 0.0998525009301017, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:45,020]\u001b[0m Trial 754 finished with value: 0.08402034285954971 and parameters: {'n_estimators': 798, 'max_depth': 3, 'learning_rate': 0.19254522680710542, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:46,428]\u001b[0m Trial 755 finished with value: 0.08252324178034955 and parameters: {'n_estimators': 791, 'max_depth': 3, 'learning_rate': 0.10908886006442631, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:47,709]\u001b[0m Trial 756 finished with value: 0.09427206101801941 and parameters: {'n_estimators': 765, 'max_depth': 3, 'learning_rate': 0.026647898132537002, 'min_child_weight': 5}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:49,630]\u001b[0m Trial 757 finished with value: 0.08361224594033768 and parameters: {'n_estimators': 775, 'max_depth': 3, 'learning_rate': 0.11932005845407037, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:52,727]\u001b[0m Trial 758 finished with value: 0.08780454576834186 and parameters: {'n_estimators': 790, 'max_depth': 4, 'learning_rate': 0.17173309554493013, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:54,949]\u001b[0m Trial 759 finished with value: 0.08306918463313627 and parameters: {'n_estimators': 765, 'max_depth': 3, 'learning_rate': 0.15619211271281505, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:56,628]\u001b[0m Trial 760 finished with value: 0.08294936107813836 and parameters: {'n_estimators': 750, 'max_depth': 3, 'learning_rate': 0.13137246881706985, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:58,560]\u001b[0m Trial 761 finished with value: 0.08226373331746348 and parameters: {'n_estimators': 783, 'max_depth': 3, 'learning_rate': 0.14373196937490337, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:26:59,914]\u001b[0m Trial 762 finished with value: 0.08270158075217962 and parameters: {'n_estimators': 752, 'max_depth': 3, 'learning_rate': 0.1158624162357173, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:27:06,059]\u001b[0m Trial 763 finished with value: 0.09324708168090719 and parameters: {'n_estimators': 770, 'max_depth': 8, 'learning_rate': 0.042843726284875934, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:27:07,606]\u001b[0m Trial 764 finished with value: 0.08375970454230489 and parameters: {'n_estimators': 731, 'max_depth': 3, 'learning_rate': 0.17767229932390421, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:27:09,184]\u001b[0m Trial 765 finished with value: 0.08278135784160373 and parameters: {'n_estimators': 751, 'max_depth': 3, 'learning_rate': 0.16004421192955068, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:27:11,064]\u001b[0m Trial 766 finished with value: 0.08603363354510356 and parameters: {'n_estimators': 839, 'max_depth': 4, 'learning_rate': 0.13610787419439035, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:27:18,672]\u001b[0m Trial 767 finished with value: 0.1045542831789172 and parameters: {'n_estimators': 737, 'max_depth': 14, 'learning_rate': 0.1250428786350259, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:27:20,036]\u001b[0m Trial 768 finished with value: 0.0826534965564436 and parameters: {'n_estimators': 777, 'max_depth': 3, 'learning_rate': 0.1506374313965675, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:27:21,335]\u001b[0m Trial 769 finished with value: 0.08254708094217993 and parameters: {'n_estimators': 725, 'max_depth': 3, 'learning_rate': 0.13500536043816425, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:27:23,302]\u001b[0m Trial 770 finished with value: 0.08678866035047919 and parameters: {'n_estimators': 711, 'max_depth': 4, 'learning_rate': 0.17224638785536334, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:27:24,842]\u001b[0m Trial 771 finished with value: 0.08257299826086026 and parameters: {'n_estimators': 821, 'max_depth': 3, 'learning_rate': 0.14631564544756903, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:27:26,225]\u001b[0m Trial 772 finished with value: 0.10524439261200529 and parameters: {'n_estimators': 742, 'max_depth': 3, 'learning_rate': 0.013625850783620138, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:27:27,875]\u001b[0m Trial 773 finished with value: 0.08462142450028 and parameters: {'n_estimators': 766, 'max_depth': 3, 'learning_rate': 0.18826524744798082, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:27:29,214]\u001b[0m Trial 774 finished with value: 0.09693374076406427 and parameters: {'n_estimators': 716, 'max_depth': 3, 'learning_rate': 0.0229374984835686, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:27:30,654]\u001b[0m Trial 775 finished with value: 0.10746782345354618 and parameters: {'n_estimators': 697, 'max_depth': 3, 'learning_rate': 0.01230901078678553, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:27:33,217]\u001b[0m Trial 776 finished with value: 0.0865524613434759 and parameters: {'n_estimators': 751, 'max_depth': 5, 'learning_rate': 0.10735323983030207, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:27:34,230]\u001b[0m Trial 777 finished with value: 0.08328747282232361 and parameters: {'n_estimators': 497, 'max_depth': 3, 'learning_rate': 0.12502488713863982, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:27:35,800]\u001b[0m Trial 778 finished with value: 0.08874189978266406 and parameters: {'n_estimators': 651, 'max_depth': 4, 'learning_rate': 0.16423417775402224, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:27:37,155]\u001b[0m Trial 779 finished with value: 0.08327021589551106 and parameters: {'n_estimators': 683, 'max_depth': 3, 'learning_rate': 0.11713230484586579, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:27:40,129]\u001b[0m Trial 780 finished with value: 0.09380548783775086 and parameters: {'n_estimators': 718, 'max_depth': 6, 'learning_rate': 0.15151530887678683, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:27:41,698]\u001b[0m Trial 781 finished with value: 0.08239314017106542 and parameters: {'n_estimators': 666, 'max_depth': 3, 'learning_rate': 0.13847891140955237, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:27:43,490]\u001b[0m Trial 782 finished with value: 0.08329407297801958 and parameters: {'n_estimators': 732, 'max_depth': 3, 'learning_rate': 0.09271083581693935, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:27:45,110]\u001b[0m Trial 783 finished with value: 0.08299425059418443 and parameters: {'n_estimators': 806, 'max_depth': 3, 'learning_rate': 0.12780347817724083, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:27:47,158]\u001b[0m Trial 784 finished with value: 0.08576349535115597 and parameters: {'n_estimators': 700, 'max_depth': 4, 'learning_rate': 0.1597624857693718, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:27:48,594]\u001b[0m Trial 785 finished with value: 0.0833380400574528 and parameters: {'n_estimators': 762, 'max_depth': 3, 'learning_rate': 0.14549308557812402, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:27:50,168]\u001b[0m Trial 786 finished with value: 0.08329158468354203 and parameters: {'n_estimators': 783, 'max_depth': 3, 'learning_rate': 0.1342535060272968, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:27:51,130]\u001b[0m Trial 787 finished with value: 0.09177749989897299 and parameters: {'n_estimators': 332, 'max_depth': 4, 'learning_rate': 0.04641348870264835, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:27:52,733]\u001b[0m Trial 788 finished with value: 0.08283706581670945 and parameters: {'n_estimators': 674, 'max_depth': 3, 'learning_rate': 0.1125215474151408, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:27:54,168]\u001b[0m Trial 789 finished with value: 0.08460100921379025 and parameters: {'n_estimators': 713, 'max_depth': 3, 'learning_rate': 0.17799139025876753, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:27:55,888]\u001b[0m Trial 790 finished with value: 0.08298000691791643 and parameters: {'n_estimators': 733, 'max_depth': 3, 'learning_rate': 0.10059070712010132, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:27:57,336]\u001b[0m Trial 791 finished with value: 0.0826500350927559 and parameters: {'n_estimators': 688, 'max_depth': 3, 'learning_rate': 0.12056863880773389, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:27:58,750]\u001b[0m Trial 792 finished with value: 0.08338863250547998 and parameters: {'n_estimators': 618, 'max_depth': 3, 'learning_rate': 0.2009680472335005, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:28:00,127]\u001b[0m Trial 793 finished with value: 0.08284291699085977 and parameters: {'n_estimators': 640, 'max_depth': 3, 'learning_rate': 0.15250381620117398, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:28:01,698]\u001b[0m Trial 794 finished with value: 0.08353928226982023 and parameters: {'n_estimators': 749, 'max_depth': 3, 'learning_rate': 0.16599778434710202, 'min_child_weight': 5}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:28:03,377]\u001b[0m Trial 795 finished with value: 0.0828191079128297 and parameters: {'n_estimators': 702, 'max_depth': 3, 'learning_rate': 0.14124538812396864, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:28:05,155]\u001b[0m Trial 796 finished with value: 0.08425831915679907 and parameters: {'n_estimators': 665, 'max_depth': 4, 'learning_rate': 0.1274215546451594, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:28:06,696]\u001b[0m Trial 797 finished with value: 0.08394784695156927 and parameters: {'n_estimators': 720, 'max_depth': 3, 'learning_rate': 0.15462616137961538, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:28:15,588]\u001b[0m Trial 798 finished with value: 0.1048112802167831 and parameters: {'n_estimators': 692, 'max_depth': 15, 'learning_rate': 0.13357088099558048, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:28:17,034]\u001b[0m Trial 799 finished with value: 0.08349310994313272 and parameters: {'n_estimators': 741, 'max_depth': 3, 'learning_rate': 0.16835846429333284, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:28:18,808]\u001b[0m Trial 800 finished with value: 0.08527538963067192 and parameters: {'n_estimators': 658, 'max_depth': 4, 'learning_rate': 0.12015803022390714, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:28:20,149]\u001b[0m Trial 801 finished with value: 0.08234989201226825 and parameters: {'n_estimators': 681, 'max_depth': 3, 'learning_rate': 0.1419592174015891, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:28:22,074]\u001b[0m Trial 802 finished with value: 0.08602489920911367 and parameters: {'n_estimators': 768, 'max_depth': 4, 'learning_rate': 0.15558648700803293, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:28:23,837]\u001b[0m Trial 803 finished with value: 0.08372643663163837 and parameters: {'n_estimators': 715, 'max_depth': 3, 'learning_rate': 0.1842789060181483, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:28:25,214]\u001b[0m Trial 804 finished with value: 0.08333347635755216 and parameters: {'n_estimators': 704, 'max_depth': 3, 'learning_rate': 0.11092013575667425, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:28:26,709]\u001b[0m Trial 805 finished with value: 0.0831372896782915 and parameters: {'n_estimators': 731, 'max_depth': 3, 'learning_rate': 0.1295974954849248, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:28:28,639]\u001b[0m Trial 806 finished with value: 0.08252173787708882 and parameters: {'n_estimators': 792, 'max_depth': 3, 'learning_rate': 0.1455263993883801, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:28:29,997]\u001b[0m Trial 807 finished with value: 0.0830012992386513 and parameters: {'n_estimators': 682, 'max_depth': 3, 'learning_rate': 0.13663957645342298, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:28:31,366]\u001b[0m Trial 808 finished with value: 0.08352099809410667 and parameters: {'n_estimators': 647, 'max_depth': 3, 'learning_rate': 0.16227949319932292, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:28:33,560]\u001b[0m Trial 809 finished with value: 0.08484631339069673 and parameters: {'n_estimators': 754, 'max_depth': 4, 'learning_rate': 0.1178836447692207, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:28:34,424]\u001b[0m Trial 810 finished with value: 0.08376725637774293 and parameters: {'n_estimators': 403, 'max_depth': 3, 'learning_rate': 0.17092450787136657, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:28:35,873]\u001b[0m Trial 811 finished with value: 0.08248257995669252 and parameters: {'n_estimators': 699, 'max_depth': 3, 'learning_rate': 0.12474742115561811, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:28:37,326]\u001b[0m Trial 812 finished with value: 0.10291475756183441 and parameters: {'n_estimators': 716, 'max_depth': 3, 'learning_rate': 0.016200841840064612, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:28:38,712]\u001b[0m Trial 813 finished with value: 0.08266324199933343 and parameters: {'n_estimators': 673, 'max_depth': 3, 'learning_rate': 0.14917926840519583, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:28:40,166]\u001b[0m Trial 814 finished with value: 0.08417132121165083 and parameters: {'n_estimators': 738, 'max_depth': 3, 'learning_rate': 0.10698050792822686, 'min_child_weight': 6}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:28:41,806]\u001b[0m Trial 815 finished with value: 0.08288839769821313 and parameters: {'n_estimators': 630, 'max_depth': 3, 'learning_rate': 0.1335363605080869, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:28:43,412]\u001b[0m Trial 816 finished with value: 0.08226414886960809 and parameters: {'n_estimators': 659, 'max_depth': 3, 'learning_rate': 0.15901835104941656, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:28:45,476]\u001b[0m Trial 817 finished with value: 0.10099639770365246 and parameters: {'n_estimators': 763, 'max_depth': 4, 'learning_rate': 0.010458650869590346, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:28:47,102]\u001b[0m Trial 818 finished with value: 0.08272441754739207 and parameters: {'n_estimators': 693, 'max_depth': 3, 'learning_rate': 0.14391074424100592, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:28:48,622]\u001b[0m Trial 819 finished with value: 0.08387966155676363 and parameters: {'n_estimators': 726, 'max_depth': 3, 'learning_rate': 0.1767632150689091, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:28:49,839]\u001b[0m Trial 820 finished with value: 0.09299240361592506 and parameters: {'n_estimators': 576, 'max_depth': 3, 'learning_rate': 0.03840544431048918, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:28:51,665]\u001b[0m Trial 821 finished with value: 0.08444045941099101 and parameters: {'n_estimators': 779, 'max_depth': 3, 'learning_rate': 0.19323404094153257, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:28:53,141]\u001b[0m Trial 822 finished with value: 0.0834123863076146 and parameters: {'n_estimators': 705, 'max_depth': 3, 'learning_rate': 0.12644046497283262, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:28:54,484]\u001b[0m Trial 823 finished with value: 0.08412272761003801 and parameters: {'n_estimators': 674, 'max_depth': 3, 'learning_rate': 0.22236113013090728, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:28:56,694]\u001b[0m Trial 824 finished with value: 0.08446355118849863 and parameters: {'n_estimators': 744, 'max_depth': 4, 'learning_rate': 0.11505062033227188, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:28:58,473]\u001b[0m Trial 825 finished with value: 0.08334290321166601 and parameters: {'n_estimators': 716, 'max_depth': 3, 'learning_rate': 0.1551993572049178, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:29:00,125]\u001b[0m Trial 826 finished with value: 0.08245797628427717 and parameters: {'n_estimators': 689, 'max_depth': 3, 'learning_rate': 0.13918244623543205, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:29:01,477]\u001b[0m Trial 827 finished with value: 0.09368903580642896 and parameters: {'n_estimators': 654, 'max_depth': 3, 'learning_rate': 0.0326010487335132, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:29:03,665]\u001b[0m Trial 828 finished with value: 0.08579221761627058 and parameters: {'n_estimators': 722, 'max_depth': 4, 'learning_rate': 0.13033894328852597, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:29:05,006]\u001b[0m Trial 829 finished with value: 0.0833885768570585 and parameters: {'n_estimators': 609, 'max_depth': 3, 'learning_rate': 0.16514411903862386, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:29:06,487]\u001b[0m Trial 830 finished with value: 0.08239055214740282 and parameters: {'n_estimators': 757, 'max_depth': 3, 'learning_rate': 0.14901918426564081, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:29:08,403]\u001b[0m Trial 831 finished with value: 0.08249961918811777 and parameters: {'n_estimators': 965, 'max_depth': 3, 'learning_rate': 0.1005023345821524, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:29:09,840]\u001b[0m Trial 832 finished with value: 0.08306052813844458 and parameters: {'n_estimators': 679, 'max_depth': 3, 'learning_rate': 0.12254535422857084, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:29:11,583]\u001b[0m Trial 833 finished with value: 0.0831357568439862 and parameters: {'n_estimators': 707, 'max_depth': 3, 'learning_rate': 0.13769495328945866, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:29:13,066]\u001b[0m Trial 834 finished with value: 0.08409036395262161 and parameters: {'n_estimators': 733, 'max_depth': 3, 'learning_rate': 0.17827262250122794, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:29:15,139]\u001b[0m Trial 835 finished with value: 0.08337417185758891 and parameters: {'n_estimators': 687, 'max_depth': 4, 'learning_rate': 0.08229887672934665, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:29:16,774]\u001b[0m Trial 836 finished with value: 0.0829387506742927 and parameters: {'n_estimators': 797, 'max_depth': 3, 'learning_rate': 0.11308576987941066, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:29:18,075]\u001b[0m Trial 837 finished with value: 0.08260896282855443 and parameters: {'n_estimators': 641, 'max_depth': 3, 'learning_rate': 0.15460360690578923, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:29:19,868]\u001b[0m Trial 838 finished with value: 0.08498237910796849 and parameters: {'n_estimators': 668, 'max_depth': 4, 'learning_rate': 0.1419744950580771, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:29:21,278]\u001b[0m Trial 839 finished with value: 0.08300392716098079 and parameters: {'n_estimators': 700, 'max_depth': 3, 'learning_rate': 0.1312083274621835, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:29:23,100]\u001b[0m Trial 840 finished with value: 0.08195714197156158 and parameters: {'n_estimators': 745, 'max_depth': 3, 'learning_rate': 0.1658806232814831, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:29:24,829]\u001b[0m Trial 841 finished with value: 0.08418743298567412 and parameters: {'n_estimators': 758, 'max_depth': 3, 'learning_rate': 0.18095678542287028, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:29:30,587]\u001b[0m Trial 842 finished with value: 0.10038129945318305 and parameters: {'n_estimators': 779, 'max_depth': 9, 'learning_rate': 0.21052632048264738, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:29:32,297]\u001b[0m Trial 843 finished with value: 0.08327494977087521 and parameters: {'n_estimators': 776, 'max_depth': 3, 'learning_rate': 0.16850676877998252, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:29:34,055]\u001b[0m Trial 844 finished with value: 0.08373973990748947 and parameters: {'n_estimators': 817, 'max_depth': 3, 'learning_rate': 0.18629379573373017, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:29:36,055]\u001b[0m Trial 845 finished with value: 0.08308164190378595 and parameters: {'n_estimators': 744, 'max_depth': 3, 'learning_rate': 0.17069966183423305, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:29:36,579]\u001b[0m Trial 846 finished with value: 0.09100211356259427 and parameters: {'n_estimators': 158, 'max_depth': 3, 'learning_rate': 0.16229873682854556, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:29:38,174]\u001b[0m Trial 847 finished with value: 0.08563965910037585 and parameters: {'n_estimators': 767, 'max_depth': 3, 'learning_rate': 0.19405781113347115, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:29:40,243]\u001b[0m Trial 848 finished with value: 0.08583124169737358 and parameters: {'n_estimators': 750, 'max_depth': 4, 'learning_rate': 0.15828151622270306, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:29:42,003]\u001b[0m Trial 849 finished with value: 0.08392212825027276 and parameters: {'n_estimators': 747, 'max_depth': 3, 'learning_rate': 0.1701692230805162, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:29:44,217]\u001b[0m Trial 850 finished with value: 0.08279077028629671 and parameters: {'n_estimators': 732, 'max_depth': 3, 'learning_rate': 0.1502904022359585, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:29:45,719]\u001b[0m Trial 851 finished with value: 0.08236456924978328 and parameters: {'n_estimators': 658, 'max_depth': 3, 'learning_rate': 0.1620530953546719, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:29:54,686]\u001b[0m Trial 852 finished with value: 0.10629128661382152 and parameters: {'n_estimators': 790, 'max_depth': 13, 'learning_rate': 0.176215047252645, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:29:56,154]\u001b[0m Trial 853 finished with value: 0.08250091564787407 and parameters: {'n_estimators': 630, 'max_depth': 3, 'learning_rate': 0.14942333872494162, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:29:58,073]\u001b[0m Trial 854 finished with value: 0.08713988911407138 and parameters: {'n_estimators': 706, 'max_depth': 4, 'learning_rate': 0.15772705954587596, 'min_child_weight': 7}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:29:59,888]\u001b[0m Trial 855 finished with value: 0.08433474922626143 and parameters: {'n_estimators': 726, 'max_depth': 3, 'learning_rate': 0.18707547658584547, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:30:02,028]\u001b[0m Trial 856 finished with value: 0.0826654757872344 and parameters: {'n_estimators': 768, 'max_depth': 3, 'learning_rate': 0.14775920295544864, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:30:03,669]\u001b[0m Trial 857 finished with value: 0.08395989014513086 and parameters: {'n_estimators': 679, 'max_depth': 3, 'learning_rate': 0.16758662872659952, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:30:05,638]\u001b[0m Trial 858 finished with value: 0.08574106211056873 and parameters: {'n_estimators': 688, 'max_depth': 4, 'learning_rate': 0.14181107678124188, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:30:07,490]\u001b[0m Trial 859 finished with value: 0.08285930018085841 and parameters: {'n_estimators': 660, 'max_depth': 3, 'learning_rate': 0.15707789466203675, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:30:09,666]\u001b[0m Trial 860 finished with value: 0.08299692922839406 and parameters: {'n_estimators': 714, 'max_depth': 3, 'learning_rate': 0.1388840598690053, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:30:11,405]\u001b[0m Trial 861 finished with value: 0.08441498679660339 and parameters: {'n_estimators': 742, 'max_depth': 3, 'learning_rate': 0.17713434890212187, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:30:12,977]\u001b[0m Trial 862 finished with value: 0.08389846280538288 and parameters: {'n_estimators': 696, 'max_depth': 3, 'learning_rate': 0.15042202634867674, 'min_child_weight': 10}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:30:14,714]\u001b[0m Trial 863 finished with value: 0.0848405257616334 and parameters: {'n_estimators': 644, 'max_depth': 3, 'learning_rate': 0.1986527036988652, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:30:16,613]\u001b[0m Trial 864 finished with value: 0.0834616221668075 and parameters: {'n_estimators': 799, 'max_depth': 3, 'learning_rate': 0.16653443786052805, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:30:18,816]\u001b[0m Trial 865 finished with value: 0.08584030665022686 and parameters: {'n_estimators': 761, 'max_depth': 4, 'learning_rate': 0.1352715793790048, 'min_child_weight': 5}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:30:20,402]\u001b[0m Trial 866 finished with value: 0.08322891696303439 and parameters: {'n_estimators': 673, 'max_depth': 3, 'learning_rate': 0.14662575234668676, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:30:22,364]\u001b[0m Trial 867 finished with value: 0.08353011074735918 and parameters: {'n_estimators': 735, 'max_depth': 3, 'learning_rate': 0.15462461322563228, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:30:23,975]\u001b[0m Trial 868 finished with value: 0.08309285294850441 and parameters: {'n_estimators': 717, 'max_depth': 3, 'learning_rate': 0.1314376208418407, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:30:26,027]\u001b[0m Trial 869 finished with value: 0.08651283933196181 and parameters: {'n_estimators': 703, 'max_depth': 4, 'learning_rate': 0.16195406116755112, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:30:28,132]\u001b[0m Trial 870 finished with value: 0.08310693915881492 and parameters: {'n_estimators': 778, 'max_depth': 3, 'learning_rate': 0.18134382979687547, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:30:29,497]\u001b[0m Trial 871 finished with value: 0.08290063321512871 and parameters: {'n_estimators': 600, 'max_depth': 3, 'learning_rate': 0.1437579922339411, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:30:31,118]\u001b[0m Trial 872 finished with value: 0.08320239035774854 and parameters: {'n_estimators': 682, 'max_depth': 3, 'learning_rate': 0.12460274275878987, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:30:33,260]\u001b[0m Trial 873 finished with value: 0.08601050254872308 and parameters: {'n_estimators': 751, 'max_depth': 3, 'learning_rate': 0.05742055212111398, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:30:34,845]\u001b[0m Trial 874 finished with value: 0.08393616543478491 and parameters: {'n_estimators': 727, 'max_depth': 3, 'learning_rate': 0.17053475901004184, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:30:36,434]\u001b[0m Trial 875 finished with value: 0.08260553336789768 and parameters: {'n_estimators': 662, 'max_depth': 3, 'learning_rate': 0.1397948438886735, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:30:38,761]\u001b[0m Trial 876 finished with value: 0.085613102481566 and parameters: {'n_estimators': 700, 'max_depth': 4, 'learning_rate': 0.15043954836571963, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:30:40,166]\u001b[0m Trial 877 finished with value: 0.0825841065356284 and parameters: {'n_estimators': 631, 'max_depth': 3, 'learning_rate': 0.13265002817958377, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:30:41,892]\u001b[0m Trial 878 finished with value: 0.08324735921670097 and parameters: {'n_estimators': 719, 'max_depth': 3, 'learning_rate': 0.16022513122254364, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:30:43,969]\u001b[0m Trial 879 finished with value: 0.08332004573388377 and parameters: {'n_estimators': 683, 'max_depth': 3, 'learning_rate': 0.12195076456712403, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:30:46,862]\u001b[0m Trial 880 finished with value: 0.0931690848266701 and parameters: {'n_estimators': 740, 'max_depth': 5, 'learning_rate': 0.19066856312436922, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:30:49,106]\u001b[0m Trial 881 finished with value: 0.0859662930507487 and parameters: {'n_estimators': 760, 'max_depth': 4, 'learning_rate': 0.14108204708152572, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:30:50,631]\u001b[0m Trial 882 finished with value: 0.08373161134596022 and parameters: {'n_estimators': 666, 'max_depth': 3, 'learning_rate': 0.17193390251585408, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:30:52,266]\u001b[0m Trial 883 finished with value: 0.08301017264797066 and parameters: {'n_estimators': 702, 'max_depth': 3, 'learning_rate': 0.15529843045242492, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:30:54,737]\u001b[0m Trial 884 finished with value: 0.08336194886702872 and parameters: {'n_estimators': 654, 'max_depth': 3, 'learning_rate': 0.12988232430013022, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:30:57,173]\u001b[0m Trial 885 finished with value: 0.08255570842720951 and parameters: {'n_estimators': 726, 'max_depth': 3, 'learning_rate': 0.14190854994742402, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:31:00,557]\u001b[0m Trial 886 finished with value: 0.08764987998468635 and parameters: {'n_estimators': 694, 'max_depth': 4, 'learning_rate': 0.16262529729994804, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:31:03,220]\u001b[0m Trial 887 finished with value: 0.08336795546281142 and parameters: {'n_estimators': 784, 'max_depth': 3, 'learning_rate': 0.14784454450877502, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:31:05,228]\u001b[0m Trial 888 finished with value: 0.08398129766152383 and parameters: {'n_estimators': 472, 'max_depth': 3, 'learning_rate': 0.17967940820076192, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:31:07,762]\u001b[0m Trial 889 finished with value: 0.0848594980991578 and parameters: {'n_estimators': 680, 'max_depth': 3, 'learning_rate': 0.2047983195444124, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:31:09,400]\u001b[0m Trial 890 finished with value: 0.08266155455990679 and parameters: {'n_estimators': 712, 'max_depth': 3, 'learning_rate': 0.12708046080689242, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:31:11,135]\u001b[0m Trial 891 finished with value: 0.08247719720819359 and parameters: {'n_estimators': 751, 'max_depth': 3, 'learning_rate': 0.13481420190708907, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:31:13,323]\u001b[0m Trial 892 finished with value: 0.08374440729884888 and parameters: {'n_estimators': 807, 'max_depth': 3, 'learning_rate': 0.15588222914210662, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:31:14,857]\u001b[0m Trial 893 finished with value: 0.08310656364763773 and parameters: {'n_estimators': 645, 'max_depth': 3, 'learning_rate': 0.11959702048429091, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:31:22,554]\u001b[0m Trial 894 finished with value: 0.10226158936330351 and parameters: {'n_estimators': 729, 'max_depth': 10, 'learning_rate': 0.16884397859763592, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:31:26,737]\u001b[0m Trial 895 finished with value: 0.09595609196563701 and parameters: {'n_estimators': 615, 'max_depth': 7, 'learning_rate': 0.14822505888217746, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:31:28,312]\u001b[0m Trial 896 finished with value: 0.08276011433173194 and parameters: {'n_estimators': 671, 'max_depth': 3, 'learning_rate': 0.13742475307465601, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:31:30,875]\u001b[0m Trial 897 finished with value: 0.08717349665672955 and parameters: {'n_estimators': 766, 'max_depth': 4, 'learning_rate': 0.1850485695789017, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:31:32,533]\u001b[0m Trial 898 finished with value: 0.08246932495623632 and parameters: {'n_estimators': 697, 'max_depth': 3, 'learning_rate': 0.12761894209574826, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:31:34,505]\u001b[0m Trial 899 finished with value: 0.08319389051401872 and parameters: {'n_estimators': 712, 'max_depth': 3, 'learning_rate': 0.15586628793552948, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:31:36,414]\u001b[0m Trial 900 finished with value: 0.0824771668600773 and parameters: {'n_estimators': 738, 'max_depth': 3, 'learning_rate': 0.14330565775641035, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:31:38,770]\u001b[0m Trial 901 finished with value: 0.08573726649146093 and parameters: {'n_estimators': 687, 'max_depth': 3, 'learning_rate': 0.2684926221413563, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:31:41,799]\u001b[0m Trial 902 finished with value: 0.08725387877287781 and parameters: {'n_estimators': 711, 'max_depth': 4, 'learning_rate': 0.1633002912647809, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:31:44,060]\u001b[0m Trial 903 finished with value: 0.08304789224788486 and parameters: {'n_estimators': 667, 'max_depth': 3, 'learning_rate': 0.11933415878884515, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:31:46,526]\u001b[0m Trial 904 finished with value: 0.08434879957739697 and parameters: {'n_estimators': 740, 'max_depth': 3, 'learning_rate': 0.1336145409888202, 'min_child_weight': 8}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:31:49,360]\u001b[0m Trial 905 finished with value: 0.08363829132457859 and parameters: {'n_estimators': 774, 'max_depth': 3, 'learning_rate': 0.1735018621174706, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:32:01,187]\u001b[0m Trial 906 finished with value: 0.10460715119245341 and parameters: {'n_estimators': 647, 'max_depth': 12, 'learning_rate': 0.1513013433305088, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:32:02,958]\u001b[0m Trial 907 finished with value: 0.08220427027112753 and parameters: {'n_estimators': 691, 'max_depth': 3, 'learning_rate': 0.1269926455703126, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:32:04,691]\u001b[0m Trial 908 finished with value: 0.08271754200267924 and parameters: {'n_estimators': 723, 'max_depth': 3, 'learning_rate': 0.14152530550953082, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:32:07,266]\u001b[0m Trial 909 finished with value: 0.08689833135454574 and parameters: {'n_estimators': 751, 'max_depth': 4, 'learning_rate': 0.16256475347691218, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:32:08,993]\u001b[0m Trial 910 finished with value: 0.0834464881164965 and parameters: {'n_estimators': 677, 'max_depth': 3, 'learning_rate': 0.11747895208757846, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:32:11,808]\u001b[0m Trial 911 finished with value: 0.08562707073378686 and parameters: {'n_estimators': 705, 'max_depth': 4, 'learning_rate': 0.14848744691870336, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:32:13,507]\u001b[0m Trial 912 finished with value: 0.08321486883375731 and parameters: {'n_estimators': 732, 'max_depth': 3, 'learning_rate': 0.13472753495801948, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:32:15,017]\u001b[0m Trial 913 finished with value: 0.08404642113923066 and parameters: {'n_estimators': 529, 'max_depth': 3, 'learning_rate': 0.10859143440573993, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:32:16,643]\u001b[0m Trial 914 finished with value: 0.08519422596037746 and parameters: {'n_estimators': 660, 'max_depth': 3, 'learning_rate': 0.07229589188662026, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:32:18,554]\u001b[0m Trial 915 finished with value: 0.08574035101850674 and parameters: {'n_estimators': 627, 'max_depth': 4, 'learning_rate': 0.18389015426646027, 'min_child_weight': 3}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:32:20,166]\u001b[0m Trial 916 finished with value: 0.08260114456740034 and parameters: {'n_estimators': 699, 'max_depth': 3, 'learning_rate': 0.1687751057217057, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:32:22,006]\u001b[0m Trial 917 finished with value: 0.08226617978994574 and parameters: {'n_estimators': 788, 'max_depth': 3, 'learning_rate': 0.12401111523414146, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:32:25,200]\u001b[0m Trial 918 finished with value: 0.09218102338526904 and parameters: {'n_estimators': 718, 'max_depth': 6, 'learning_rate': 0.15626623974421575, 'min_child_weight': 9}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:32:27,041]\u001b[0m Trial 919 finished with value: 0.08182862624074039 and parameters: {'n_estimators': 766, 'max_depth': 3, 'learning_rate': 0.13728378569937613, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:32:29,216]\u001b[0m Trial 920 finished with value: 0.08255383901034316 and parameters: {'n_estimators': 767, 'max_depth': 3, 'learning_rate': 0.1448147438328301, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:32:31,142]\u001b[0m Trial 921 finished with value: 0.08343164795439631 and parameters: {'n_estimators': 821, 'max_depth': 3, 'learning_rate': 0.15377347277412867, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:32:36,540]\u001b[0m Trial 922 finished with value: 0.09953847346653258 and parameters: {'n_estimators': 806, 'max_depth': 8, 'learning_rate': 0.13880058976984602, 'min_child_weight': 5}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:32:38,400]\u001b[0m Trial 923 finished with value: 0.08232191407943026 and parameters: {'n_estimators': 787, 'max_depth': 3, 'learning_rate': 0.14646900394482942, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:32:40,632]\u001b[0m Trial 924 finished with value: 0.0817069837129533 and parameters: {'n_estimators': 793, 'max_depth': 3, 'learning_rate': 0.162444710451635, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:32:42,926]\u001b[0m Trial 925 finished with value: 0.0867796506853938 and parameters: {'n_estimators': 797, 'max_depth': 4, 'learning_rate': 0.13468610635171926, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:32:44,821]\u001b[0m Trial 926 finished with value: 0.08460698583192258 and parameters: {'n_estimators': 840, 'max_depth': 3, 'learning_rate': 0.1725081263967201, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:32:46,918]\u001b[0m Trial 927 finished with value: 0.0817647500281331 and parameters: {'n_estimators': 834, 'max_depth': 3, 'learning_rate': 0.16091301471451083, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:32:48,880]\u001b[0m Trial 928 finished with value: 0.08327299992035239 and parameters: {'n_estimators': 863, 'max_depth': 3, 'learning_rate': 0.1766818880615941, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:32:51,211]\u001b[0m Trial 929 finished with value: 0.08248162883909414 and parameters: {'n_estimators': 857, 'max_depth': 3, 'learning_rate': 0.16700581902181608, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:32:53,061]\u001b[0m Trial 930 finished with value: 0.0840224012952457 and parameters: {'n_estimators': 815, 'max_depth': 3, 'learning_rate': 0.19852319063934928, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:32:55,234]\u001b[0m Trial 931 finished with value: 0.08418530669104944 and parameters: {'n_estimators': 787, 'max_depth': 3, 'learning_rate': 0.1874887300591317, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:32:57,621]\u001b[0m Trial 932 finished with value: 0.087973293685031 and parameters: {'n_estimators': 836, 'max_depth': 4, 'learning_rate': 0.167121555591288, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:32:59,874]\u001b[0m Trial 933 finished with value: 0.08268698123591149 and parameters: {'n_estimators': 805, 'max_depth': 3, 'learning_rate': 0.1600760284986654, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:33:02,069]\u001b[0m Trial 934 finished with value: 0.08437574712624792 and parameters: {'n_estimators': 840, 'max_depth': 3, 'learning_rate': 0.18000156376169202, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:33:04,171]\u001b[0m Trial 935 finished with value: 0.08327755337916884 and parameters: {'n_estimators': 889, 'max_depth': 3, 'learning_rate': 0.15534348984427815, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:33:06,439]\u001b[0m Trial 936 finished with value: 0.08214275629720465 and parameters: {'n_estimators': 828, 'max_depth': 3, 'learning_rate': 0.1612332481565138, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:33:08,383]\u001b[0m Trial 937 finished with value: 0.08414232814750208 and parameters: {'n_estimators': 823, 'max_depth': 3, 'learning_rate': 0.19178629677350842, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:33:10,854]\u001b[0m Trial 938 finished with value: 0.08329582745550315 and parameters: {'n_estimators': 816, 'max_depth': 3, 'learning_rate': 0.17561431548320536, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:33:13,327]\u001b[0m Trial 939 finished with value: 0.08720531957157558 and parameters: {'n_estimators': 866, 'max_depth': 4, 'learning_rate': 0.16253682436973171, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:33:15,642]\u001b[0m Trial 940 finished with value: 0.08479025950591201 and parameters: {'n_estimators': 845, 'max_depth': 3, 'learning_rate': 0.20595263934713665, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:33:17,529]\u001b[0m Trial 941 finished with value: 0.08275803518911087 and parameters: {'n_estimators': 841, 'max_depth': 3, 'learning_rate': 0.17004548173066703, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:33:19,400]\u001b[0m Trial 942 finished with value: 0.08291628786089525 and parameters: {'n_estimators': 827, 'max_depth': 3, 'learning_rate': 0.16233267053294487, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:33:21,304]\u001b[0m Trial 943 finished with value: 0.08393236306890406 and parameters: {'n_estimators': 820, 'max_depth': 3, 'learning_rate': 0.17856120466363304, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:33:30,243]\u001b[0m Trial 944 finished with value: 0.10079997009926882 and parameters: {'n_estimators': 828, 'max_depth': 9, 'learning_rate': 0.1563729293381524, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:33:32,593]\u001b[0m Trial 945 finished with value: 0.08716367263376307 and parameters: {'n_estimators': 813, 'max_depth': 4, 'learning_rate': 0.1542249774849553, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:33:34,567]\u001b[0m Trial 946 finished with value: 0.08492904714119187 and parameters: {'n_estimators': 884, 'max_depth': 3, 'learning_rate': 0.18695402486080043, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:33:36,451]\u001b[0m Trial 947 finished with value: 0.08386324725465437 and parameters: {'n_estimators': 844, 'max_depth': 3, 'learning_rate': 0.1704474751073428, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:33:38,659]\u001b[0m Trial 948 finished with value: 0.08271281080959585 and parameters: {'n_estimators': 805, 'max_depth': 3, 'learning_rate': 0.15874508107341037, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:33:40,531]\u001b[0m Trial 949 finished with value: 0.08308604866601085 and parameters: {'n_estimators': 832, 'max_depth': 3, 'learning_rate': 0.15099065435290046, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:33:42,385]\u001b[0m Trial 950 finished with value: 0.0834212191163988 and parameters: {'n_estimators': 795, 'max_depth': 3, 'learning_rate': 0.17494674951920125, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:33:45,027]\u001b[0m Trial 951 finished with value: 0.08595553137476505 and parameters: {'n_estimators': 810, 'max_depth': 4, 'learning_rate': 0.16684464047662384, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:33:47,115]\u001b[0m Trial 952 finished with value: 0.08234046626972627 and parameters: {'n_estimators': 865, 'max_depth': 3, 'learning_rate': 0.1478858964727512, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:33:49,060]\u001b[0m Trial 953 finished with value: 0.08227295477044481 and parameters: {'n_estimators': 853, 'max_depth': 3, 'learning_rate': 0.16325091486859175, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:33:51,067]\u001b[0m Trial 954 finished with value: 0.08406219882785552 and parameters: {'n_estimators': 873, 'max_depth': 3, 'learning_rate': 0.18680967534761442, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:33:52,873]\u001b[0m Trial 955 finished with value: 0.08224580632195619 and parameters: {'n_estimators': 793, 'max_depth': 3, 'learning_rate': 0.14957986550906002, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:33:55,114]\u001b[0m Trial 956 finished with value: 0.08345248633800766 and parameters: {'n_estimators': 852, 'max_depth': 3, 'learning_rate': 0.16029542270571043, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:33:57,662]\u001b[0m Trial 957 finished with value: 0.08819583941980899 and parameters: {'n_estimators': 900, 'max_depth': 4, 'learning_rate': 0.17304020205819748, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:33:59,932]\u001b[0m Trial 958 finished with value: 0.08470703788496611 and parameters: {'n_estimators': 824, 'max_depth': 3, 'learning_rate': 0.19794176508635292, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:34:01,885]\u001b[0m Trial 959 finished with value: 0.08301319642691471 and parameters: {'n_estimators': 786, 'max_depth': 3, 'learning_rate': 0.14685162469685276, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:34:04,134]\u001b[0m Trial 960 finished with value: 0.08769757652837515 and parameters: {'n_estimators': 785, 'max_depth': 4, 'learning_rate': 0.15882297471452234, 'min_child_weight': 6}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:34:06,289]\u001b[0m Trial 961 finished with value: 0.0843761094860751 and parameters: {'n_estimators': 794, 'max_depth': 3, 'learning_rate': 0.1794339982401027, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:34:08,024]\u001b[0m Trial 962 finished with value: 0.08335234029658274 and parameters: {'n_estimators': 770, 'max_depth': 3, 'learning_rate': 0.22003123066241978, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:34:09,883]\u001b[0m Trial 963 finished with value: 0.08313066301315937 and parameters: {'n_estimators': 806, 'max_depth': 3, 'learning_rate': 0.15063807189219117, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:34:12,057]\u001b[0m Trial 964 finished with value: 0.0833117535252543 and parameters: {'n_estimators': 837, 'max_depth': 3, 'learning_rate': 0.1652297350938591, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:34:13,792]\u001b[0m Trial 965 finished with value: 0.08264534952309555 and parameters: {'n_estimators': 766, 'max_depth': 3, 'learning_rate': 0.14228607598365733, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:34:15,742]\u001b[0m Trial 966 finished with value: 0.08355628770407349 and parameters: {'n_estimators': 828, 'max_depth': 3, 'learning_rate': 0.15318222542263998, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:34:17,521]\u001b[0m Trial 967 finished with value: 0.08442672436586694 and parameters: {'n_estimators': 772, 'max_depth': 3, 'learning_rate': 0.18001783589449, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:34:19,369]\u001b[0m Trial 968 finished with value: 0.08236143286177203 and parameters: {'n_estimators': 802, 'max_depth': 3, 'learning_rate': 0.14349624431249564, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:34:21,439]\u001b[0m Trial 969 finished with value: 0.083264741051013 and parameters: {'n_estimators': 781, 'max_depth': 3, 'learning_rate': 0.16472905815976166, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:34:24,065]\u001b[0m Trial 970 finished with value: 0.08688689989921365 and parameters: {'n_estimators': 935, 'max_depth': 4, 'learning_rate': 0.1385148334053181, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:34:25,948]\u001b[0m Trial 971 finished with value: 0.08449092200401603 and parameters: {'n_estimators': 851, 'max_depth': 3, 'learning_rate': 0.15561464773291378, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:34:27,722]\u001b[0m Trial 972 finished with value: 0.08576533320175621 and parameters: {'n_estimators': 765, 'max_depth': 3, 'learning_rate': 0.19424870780335707, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:34:30,296]\u001b[0m Trial 973 finished with value: 0.08308582311144246 and parameters: {'n_estimators': 819, 'max_depth': 3, 'learning_rate': 0.17387509715279575, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:34:32,861]\u001b[0m Trial 974 finished with value: 0.09109043997091111 and parameters: {'n_estimators': 791, 'max_depth': 4, 'learning_rate': 0.24650663804406728, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:34:34,636]\u001b[0m Trial 975 finished with value: 0.08254199390917345 and parameters: {'n_estimators': 776, 'max_depth': 3, 'learning_rate': 0.14677304980427192, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:34:36,407]\u001b[0m Trial 976 finished with value: 0.0821983819056997 and parameters: {'n_estimators': 761, 'max_depth': 3, 'learning_rate': 0.16117896721776728, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:34:38,554]\u001b[0m Trial 977 finished with value: 0.08240965082024795 and parameters: {'n_estimators': 813, 'max_depth': 3, 'learning_rate': 0.14178659514206726, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:34:40,637]\u001b[0m Trial 978 finished with value: 0.08636383225110883 and parameters: {'n_estimators': 750, 'max_depth': 4, 'learning_rate': 0.15321524788108443, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:34:48,405]\u001b[0m Trial 979 finished with value: 0.10300281038002383 and parameters: {'n_estimators': 758, 'max_depth': 11, 'learning_rate': 0.17261100702059304, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:34:50,092]\u001b[0m Trial 980 finished with value: 0.08214853769173854 and parameters: {'n_estimators': 776, 'max_depth': 3, 'learning_rate': 0.13683383908086277, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:34:52,201]\u001b[0m Trial 981 finished with value: 0.08268008983563277 and parameters: {'n_estimators': 778, 'max_depth': 3, 'learning_rate': 0.13301959538715558, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:34:54,025]\u001b[0m Trial 982 finished with value: 0.08370669105344264 and parameters: {'n_estimators': 813, 'max_depth': 3, 'learning_rate': 0.13450173694839665, 'min_child_weight': 5}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:34:55,837]\u001b[0m Trial 983 finished with value: 0.08268520080035745 and parameters: {'n_estimators': 789, 'max_depth': 3, 'learning_rate': 0.12790078260726107, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:34:58,090]\u001b[0m Trial 984 finished with value: 0.08279714856339851 and parameters: {'n_estimators': 838, 'max_depth': 3, 'learning_rate': 0.1382304006719829, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:35:00,886]\u001b[0m Trial 985 finished with value: 0.08713031875136815 and parameters: {'n_estimators': 810, 'max_depth': 4, 'learning_rate': 0.13997454518710972, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:35:02,741]\u001b[0m Trial 986 finished with value: 0.08250872961798147 and parameters: {'n_estimators': 802, 'max_depth': 3, 'learning_rate': 0.1290745231295914, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:35:04,798]\u001b[0m Trial 987 finished with value: 0.08243085229276693 and parameters: {'n_estimators': 772, 'max_depth': 3, 'learning_rate': 0.14506892626253398, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:35:06,691]\u001b[0m Trial 988 finished with value: 0.08238836573806105 and parameters: {'n_estimators': 798, 'max_depth': 3, 'learning_rate': 0.12265717949849361, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:35:08,628]\u001b[0m Trial 989 finished with value: 0.0825027710727266 and parameters: {'n_estimators': 826, 'max_depth': 3, 'learning_rate': 0.13134249274701296, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:35:10,684]\u001b[0m Trial 990 finished with value: 0.08298343614744504 and parameters: {'n_estimators': 779, 'max_depth': 3, 'learning_rate': 0.14436835278004947, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:35:12,925]\u001b[0m Trial 991 finished with value: 0.08356331086152853 and parameters: {'n_estimators': 758, 'max_depth': 3, 'learning_rate': 0.13403418837706157, 'min_child_weight': 4}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:35:15,981]\u001b[0m Trial 992 finished with value: 0.09050036013864263 and parameters: {'n_estimators': 780, 'max_depth': 5, 'learning_rate': 0.148399430193986, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:35:18,649]\u001b[0m Trial 993 finished with value: 0.08511173574036432 and parameters: {'n_estimators': 874, 'max_depth': 4, 'learning_rate': 0.1212087498590984, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:35:21,531]\u001b[0m Trial 994 finished with value: 0.08341313343305887 and parameters: {'n_estimators': 854, 'max_depth': 3, 'learning_rate': 0.15338518920892844, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:35:23,929]\u001b[0m Trial 995 finished with value: 0.08258826362434266 and parameters: {'n_estimators': 798, 'max_depth': 3, 'learning_rate': 0.13587862301329842, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:35:26,126]\u001b[0m Trial 996 finished with value: 0.0829828554386322 and parameters: {'n_estimators': 750, 'max_depth': 3, 'learning_rate': 0.11391082872008267, 'min_child_weight': 1}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:35:28,364]\u001b[0m Trial 997 finished with value: 0.08276858925895582 and parameters: {'n_estimators': 768, 'max_depth': 3, 'learning_rate': 0.14392078129021887, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:35:31,032]\u001b[0m Trial 998 finished with value: 0.08433108819297482 and parameters: {'n_estimators': 746, 'max_depth': 4, 'learning_rate': 0.12670312281649848, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n",
      "\u001b[32m[I 2026-01-21 16:35:32,828]\u001b[0m Trial 999 finished with value: 0.08237576200573207 and parameters: {'n_estimators': 799, 'max_depth': 3, 'learning_rate': 0.15760272502169156, 'min_child_weight': 2}. Best is trial 96 with value: 0.08165961576392188.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='minimize',sampler=TPESampler(seed=42),pruner=MedianPruner())\n",
    "study.optimize(objective, n_trials=160,show_progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39df35a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "markers",
         "name": "Objective Value",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500,
          501,
          502,
          503,
          504,
          505,
          506,
          507,
          508,
          509,
          510,
          511,
          512,
          513,
          514,
          515,
          516,
          517,
          518,
          519,
          520,
          521,
          522,
          523,
          524,
          525,
          526,
          527,
          528,
          529,
          530,
          531,
          532,
          533,
          534,
          535,
          536,
          537,
          538,
          539,
          540,
          541,
          542,
          543,
          544,
          545,
          546,
          547,
          548,
          549,
          550,
          551,
          552,
          553,
          554,
          555,
          556,
          557,
          558,
          559,
          560,
          561,
          562,
          563,
          564,
          565,
          566,
          567,
          568,
          569,
          570,
          571,
          572,
          573,
          574,
          575,
          576,
          577,
          578,
          579,
          580,
          581,
          582,
          583,
          584,
          585,
          586,
          587,
          588,
          589,
          590,
          591,
          592,
          593,
          594,
          595,
          596,
          597,
          598,
          599,
          600,
          601,
          602,
          603,
          604,
          605,
          606,
          607,
          608,
          609,
          610,
          611,
          612,
          613,
          614,
          615,
          616,
          617,
          618,
          619,
          620,
          621,
          622,
          623,
          624,
          625,
          626,
          627,
          628,
          629,
          630,
          631,
          632,
          633,
          634,
          635,
          636,
          637,
          638,
          639,
          640,
          641,
          642,
          643,
          644,
          645,
          646,
          647,
          648,
          649,
          650,
          651,
          652,
          653,
          654,
          655,
          656,
          657,
          658,
          659,
          660,
          661,
          662,
          663,
          664,
          665,
          666,
          667,
          668,
          669,
          670,
          671,
          672,
          673,
          674,
          675,
          676,
          677,
          678,
          679,
          680,
          681,
          682,
          683,
          684,
          685,
          686,
          687,
          688,
          689,
          690,
          691,
          692,
          693,
          694,
          695,
          696,
          697,
          698,
          699,
          700,
          701,
          702,
          703,
          704,
          705,
          706,
          707,
          708,
          709,
          710,
          711,
          712,
          713,
          714,
          715,
          716,
          717,
          718,
          719,
          720,
          721,
          722,
          723,
          724,
          725,
          726,
          727,
          728,
          729,
          730,
          731,
          732,
          733,
          734,
          735,
          736,
          737,
          738,
          739,
          740,
          741,
          742,
          743,
          744,
          745,
          746,
          747,
          748,
          749,
          750,
          751,
          752,
          753,
          754,
          755,
          756,
          757,
          758,
          759,
          760,
          761,
          762,
          763,
          764,
          765,
          766,
          767,
          768,
          769,
          770,
          771,
          772,
          773,
          774,
          775,
          776,
          777,
          778,
          779,
          780,
          781,
          782,
          783,
          784,
          785,
          786,
          787,
          788,
          789,
          790,
          791,
          792,
          793,
          794,
          795,
          796,
          797,
          798,
          799,
          800,
          801,
          802,
          803,
          804,
          805,
          806,
          807,
          808,
          809,
          810,
          811,
          812,
          813,
          814,
          815,
          816,
          817,
          818,
          819,
          820,
          821,
          822,
          823,
          824,
          825,
          826,
          827,
          828,
          829,
          830,
          831,
          832,
          833,
          834,
          835,
          836,
          837,
          838,
          839,
          840,
          841,
          842,
          843,
          844,
          845,
          846,
          847,
          848,
          849,
          850,
          851,
          852,
          853,
          854,
          855,
          856,
          857,
          858,
          859,
          860,
          861,
          862,
          863,
          864,
          865,
          866,
          867,
          868,
          869,
          870,
          871,
          872,
          873,
          874,
          875,
          876,
          877,
          878,
          879,
          880,
          881,
          882,
          883,
          884,
          885,
          886,
          887,
          888,
          889,
          890,
          891,
          892,
          893,
          894,
          895,
          896,
          897,
          898,
          899,
          900,
          901,
          902,
          903,
          904,
          905,
          906,
          907,
          908,
          909,
          910,
          911,
          912,
          913,
          914,
          915,
          916,
          917,
          918,
          919,
          920,
          921,
          922,
          923,
          924,
          925,
          926,
          927,
          928,
          929,
          930,
          931,
          932,
          933,
          934,
          935,
          936,
          937,
          938,
          939,
          940,
          941,
          942,
          943,
          944,
          945,
          946,
          947,
          948,
          949,
          950,
          951,
          952,
          953,
          954,
          955,
          956,
          957,
          958,
          959,
          960,
          961,
          962,
          963,
          964,
          965,
          966,
          967,
          968,
          969,
          970,
          971,
          972,
          973,
          974,
          975,
          976,
          977,
          978,
          979,
          980,
          981,
          982,
          983,
          984,
          985,
          986,
          987,
          988,
          989,
          990,
          991,
          992,
          993,
          994,
          995,
          996,
          997,
          998,
          999
         ],
         "y": [
          0.08539019019252586,
          0.09495197683593737,
          0.08561203246811563,
          0.10226171316013058,
          0.09436835513192324,
          0.08442866030728638,
          0.0882806271297641,
          0.09259418891274053,
          0.0936506610503796,
          0.08905616282810194,
          0.10070957535224619,
          0.0849354185113468,
          0.09538829459268595,
          0.08532982608602123,
          0.09802117955542877,
          0.08861113027563677,
          0.08782249655960989,
          0.09671218874633744,
          0.10056915412131459,
          0.08463494265724372,
          0.08550715408794303,
          0.08624021489163447,
          0.08421050029548485,
          0.0894877666245822,
          0.08522084111774414,
          0.08664087579640814,
          0.09277247714187424,
          0.08890925915171902,
          0.08679846383946271,
          0.08434924485492348,
          0.08332985614930138,
          0.08381487607339143,
          0.08381121898840153,
          0.08492498104511295,
          0.08351886332682175,
          0.1065370630575128,
          0.08407738487249838,
          0.10418987847943002,
          0.08315042884967586,
          0.08360267337033281,
          0.08344470964379247,
          0.08381392901444998,
          0.0857432511381672,
          0.08371432932413264,
          0.08615883393220121,
          0.08428574598953223,
          0.0897804337144529,
          0.10368442923134472,
          0.08419051036454686,
          0.08666228717253156,
          0.08539823364234111,
          0.08398322451750445,
          0.08324196548934296,
          0.08498755749166624,
          0.08592689223502362,
          0.08643366899645041,
          0.0835047463204986,
          0.09288435334111321,
          0.08424549859482734,
          0.08502893317695907,
          0.0890072310285759,
          0.08354751438906698,
          0.08481137014593011,
          0.08285306771082904,
          0.08435363262967716,
          0.08440017469452545,
          0.10234792567968147,
          0.08439105115445075,
          0.08367204082049616,
          0.08674702124077736,
          0.08364722764310903,
          0.08339834509471708,
          0.08458083145212651,
          0.08730137187390588,
          0.08347106168854283,
          0.10497466632612777,
          0.09500932690462209,
          0.08627061663570071,
          0.08361465461377242,
          0.0862346237864177,
          0.08576719136060315,
          0.08392691309042451,
          0.08667835916016219,
          0.0832016218464397,
          0.08360618571881825,
          0.10570571526403402,
          0.08912974625484643,
          0.08794829675893581,
          0.10369587843510582,
          0.08450059575739881,
          0.08564046243395204,
          0.08425998493872724,
          0.08328057039506313,
          0.08414219287159663,
          0.08939263864603345,
          0.0846428294515632,
          0.08165961576392188,
          0.08488512104472777,
          0.0864821733201315,
          0.0844953596275288,
          0.08312414300199703,
          0.08302348043185244,
          0.08349016143148591,
          0.08273551529377218,
          0.08712664607608565,
          0.08335508002367598,
          0.0833304450341904,
          0.08274977982394807,
          0.08723091108735959,
          0.10129893784474617,
          0.08328120589468982,
          0.08518771642746378,
          0.08417473164618214,
          0.08544436252688223,
          0.084259756018591,
          0.08477727688870212,
          0.0843732621596494,
          0.08238179744888834,
          0.0835201737237383,
          0.08440188316439623,
          0.0826963136261387,
          0.08325320855427412,
          0.08232280947837918,
          0.08313899416292488,
          0.0827606041580992,
          0.08345781219298537,
          0.082856239988547,
          0.08315158199964165,
          0.09996173825297669,
          0.08287132257374849,
          0.08399392866757854,
          0.08329801970738117,
          0.08218916379001612,
          0.08272626181044221,
          0.08356949324059257,
          0.08344809825525434,
          0.08412450105439928,
          0.08436250442465335,
          0.08543737126852116,
          0.08315415930339068,
          0.08328874654472859,
          0.08377063932211924,
          0.08374662836316327,
          0.08432312077904969,
          0.08334078110772783,
          0.10451366567669003,
          0.08396547011835669,
          0.09866471516554136,
          0.0840355188769745,
          0.08313315216040826,
          0.0833273896064651,
          0.08380846367977608,
          0.08301209213271031,
          0.08302693484535764,
          0.08513153125053821,
          0.08294988420046369,
          0.0828023224844447,
          0.08576151350462309,
          0.08313593893209155,
          0.0823890790084934,
          0.10233081625126154,
          0.08391457740581108,
          0.08346778197338994,
          0.08323672801444174,
          0.11016564627256034,
          0.08625986833194307,
          0.08308838128817571,
          0.08256675863644616,
          0.08687355439209934,
          0.08320375301071112,
          0.0831569856636962,
          0.0830861335286607,
          0.08373209848835807,
          0.08325772728529475,
          0.0840249429842295,
          0.08340173491254706,
          0.0841523490888042,
          0.08278875829740572,
          0.08588354015186887,
          0.08460117799571931,
          0.08271264948133934,
          0.08311842459534399,
          0.09775820704966276,
          0.08363667644538213,
          0.08518966582885326,
          0.08234088467720187,
          0.08300663028468404,
          0.08294792493806535,
          0.09374333088296935,
          0.08186637706985608,
          0.08286524474377754,
          0.08407084466100902,
          0.08513404749620522,
          0.08512809066960167,
          0.08317299290426386,
          0.08329165608201898,
          0.09249173931498986,
          0.08581289239318927,
          0.08311832057969164,
          0.08244091747412202,
          0.0859276097402228,
          0.08294365575003566,
          0.08291915631246957,
          0.08431553209465299,
          0.08316980756401962,
          0.08275119431371236,
          0.08259810148865966,
          0.08269710428895796,
          0.0825306928188557,
          0.08327358623423534,
          0.08337420215674948,
          0.10445665012752847,
          0.08334076815055066,
          0.08274338024429977,
          0.08251096914423196,
          0.08199902606831211,
          0.08235772891596732,
          0.08293553119582622,
          0.08253509322945916,
          0.08376735221276418,
          0.08242039156277536,
          0.08310408342648781,
          0.08236920723797173,
          0.08291592089843107,
          0.08237535912990515,
          0.08328242745865577,
          0.08330519127524712,
          0.08333766438105057,
          0.08348678243602832,
          0.08346972442740377,
          0.08291621932822964,
          0.08231996370906212,
          0.08231519235453469,
          0.08261364924345689,
          0.0825212172002789,
          0.08282661754470844,
          0.08280000274572497,
          0.0822068912312609,
          0.08306979246734976,
          0.08280936601250934,
          0.08323729894382537,
          0.0827596216791415,
          0.08254312932582486,
          0.08260706905635237,
          0.08219159376324617,
          0.08332867404127005,
          0.08245010521755164,
          0.0830015532855319,
          0.08311543440998431,
          0.08354447071482189,
          0.08459523466839948,
          0.08279362737753312,
          0.0832079959920258,
          0.08271413443324369,
          0.08228669392391635,
          0.08746037507141037,
          0.08244453094845948,
          0.08267614433778225,
          0.08361087104719155,
          0.08254682461025294,
          0.08315199608854278,
          0.0831869149479296,
          0.0831621325240794,
          0.08241511203722858,
          0.08509378854913294,
          0.09446901061125146,
          0.0829485186897416,
          0.08256420848681219,
          0.08241588768524585,
          0.0832236338954678,
          0.08285732035700681,
          0.08516721649864492,
          0.08291222919437333,
          0.08335991669751068,
          0.08242883783270984,
          0.08266547354134668,
          0.08538467882794278,
          0.09503155262312986,
          0.08288635613635478,
          0.08264974876794147,
          0.08211371813595908,
          0.08657470992634171,
          0.08259439090255191,
          0.08312408247804894,
          0.08297372789394555,
          0.0824331510535781,
          0.08637316625377117,
          0.10319183305913875,
          0.08404118132533886,
          0.08263370202318032,
          0.0834927133525499,
          0.08558875960099446,
          0.0825962951162357,
          0.08525988219355811,
          0.08491634346648637,
          0.08277226913673,
          0.0821336454759914,
          0.08324338368912149,
          0.08389344305054196,
          0.0848766197803248,
          0.0828420257148326,
          0.08199056700519247,
          0.09088192756881397,
          0.08291160072327894,
          0.1049394814041883,
          0.0836179953257,
          0.08265678691618213,
          0.08329057337357557,
          0.08586042141522317,
          0.0831940183020074,
          0.08226786937139183,
          0.08258516295466148,
          0.08418431503900826,
          0.08378835210536532,
          0.08655320541884155,
          0.08267759915260924,
          0.10230479617375414,
          0.08253292069688178,
          0.08360373450105948,
          0.08306322233093005,
          0.08297395306054933,
          0.08502468501565282,
          0.08294150522200303,
          0.08291127575946386,
          0.0845033398062486,
          0.08229269394206133,
          0.08319980298285114,
          0.08355104320136941,
          0.08214051827823698,
          0.08368642417592195,
          0.10056914324523544,
          0.08742187178093777,
          0.10750592060617267,
          0.08342305906642027,
          0.08514749289202753,
          0.08306091591721386,
          0.08237992365506012,
          0.08504635372532161,
          0.08217331987854679,
          0.08579425456448286,
          0.08467105207276104,
          0.08242706667986897,
          0.08495171802357053,
          0.08436796376240073,
          0.08324510960531548,
          0.08348012821167246,
          0.08222864482506259,
          0.08360830242929047,
          0.08218972123363966,
          0.0846592336480432,
          0.08296534942665079,
          0.08385594691626566,
          0.08428881746489056,
          0.08514809023965,
          0.08517640762635152,
          0.08374766448669568,
          0.08346478111354153,
          0.08345120363017818,
          0.08341290855897499,
          0.08363018486181899,
          0.08310301346123908,
          0.09272379644340216,
          0.08378574735051028,
          0.0852733325696883,
          0.08216595188325272,
          0.10378265171249632,
          0.08294003021589308,
          0.08326020405580013,
          0.08228238847757531,
          0.08428079796872202,
          0.08311972393512462,
          0.08338968730295256,
          0.08293213450970785,
          0.09393946362590899,
          0.08327466350181753,
          0.08506553342131629,
          0.0826984353792733,
          0.08311464356185652,
          0.08507777430539948,
          0.10664077240786737,
          0.08308505363475249,
          0.0840521759461056,
          0.0984829737053229,
          0.08409671431676492,
          0.08343552651457424,
          0.08218376867519202,
          0.0826176725634861,
          0.087404024246948,
          0.0824579868517901,
          0.08343538851576185,
          0.08632526446250707,
          0.10113468346246568,
          0.08408297959141789,
          0.08455967169326965,
          0.08290135112828734,
          0.08622599585167161,
          0.08256680193136179,
          0.08279304994854082,
          0.08298208106969547,
          0.086487997927399,
          0.08451040655479528,
          0.08312387237224324,
          0.08279913946855727,
          0.08525800338496867,
          0.08306589903344415,
          0.08263009693292171,
          0.08444546632016169,
          0.08282584835510597,
          0.08275273275273365,
          0.08324783184800626,
          0.08293670250894221,
          0.08291566242729534,
          0.08471320065483626,
          0.08267044541148255,
          0.08447697754338505,
          0.08390700147066862,
          0.08333214843043357,
          0.0828179712937066,
          0.08299945475657355,
          0.08281322887122904,
          0.08395017481762637,
          0.08241984428417812,
          0.08213717563466298,
          0.10076673879446381,
          0.0832436787503004,
          0.08252665324010452,
          0.08320587493050441,
          0.08776937049652797,
          0.08227879636452218,
          0.08310102230104396,
          0.08312565513737012,
          0.08321513334889388,
          0.08299056674606672,
          0.08303414146151361,
          0.083011182298674,
          0.08365498360084109,
          0.08588668701466871,
          0.08384000289415564,
          0.0833124264339247,
          0.08238305561835071,
          0.09203839346356635,
          0.08349258535191846,
          0.08215443768884524,
          0.08349214873884861,
          0.08403318361265764,
          0.0824595433892342,
          0.08413298882527163,
          0.08435908750491991,
          0.08767755459365653,
          0.0827298801062617,
          0.09582911899764356,
          0.08268836609716766,
          0.08217887190324438,
          0.08456203431506822,
          0.08395906740770928,
          0.08721133673682199,
          0.08326946965557938,
          0.08277857338975389,
          0.08516307424345448,
          0.08874462697002515,
          0.08388595508820527,
          0.10162837463997096,
          0.08349279022803455,
          0.08273657163673434,
          0.08376690305749647,
          0.08253757560419685,
          0.10402306282405352,
          0.08277068997162221,
          0.08764543348113721,
          0.08260209306314711,
          0.08246848587072822,
          0.08308283039199962,
          0.08247267673486194,
          0.09189560105049469,
          0.08462355983006513,
          0.08329194609971924,
          0.08316037595639489,
          0.08317869616338323,
          0.08316144874053474,
          0.08959409565848214,
          0.08402716720523631,
          0.08400077230926636,
          0.0829619142270787,
          0.08283674928845328,
          0.0847778461277469,
          0.08344920849250995,
          0.08523696176791473,
          0.08725620307003272,
          0.10397492307026904,
          0.08595226907323969,
          0.09921985101954633,
          0.09533855013187104,
          0.08309507191276091,
          0.0826759786006139,
          0.08225593586524416,
          0.08740274630438884,
          0.08315660205916471,
          0.08608837841090153,
          0.08467202290989553,
          0.097137117730765,
          0.08275036333514421,
          0.08386434363894248,
          0.08188325999628746,
          0.08557863261436732,
          0.08446231710143276,
          0.08368820469053169,
          0.08285472367859309,
          0.08303516672857214,
          0.0833906914083489,
          0.08366981604698515,
          0.08793276147551224,
          0.08952814150800537,
          0.08308096878529299,
          0.08256381892015496,
          0.08317662056064302,
          0.08710183821325071,
          0.0834428219383049,
          0.0830258565559016,
          0.08355652692921657,
          0.08424998322924931,
          0.08334819052906109,
          0.08573110756005994,
          0.08488722805270349,
          0.08380901110932767,
          0.09618439324678967,
          0.0826032479888251,
          0.0820830552828102,
          0.0862818168369496,
          0.08401134731429585,
          0.08295871971481268,
          0.08240986881096399,
          0.0824353229340784,
          0.08462405213082154,
          0.08262857330685547,
          0.09748615573746822,
          0.08613884514448175,
          0.08814720651422511,
          0.0824457607071543,
          0.08331839738519489,
          0.0821804083798195,
          0.08350042182975628,
          0.08311370306065799,
          0.08886475197480367,
          0.08374134130419003,
          0.08466248680311633,
          0.083197319026209,
          0.0870576233729369,
          0.10433184556727466,
          0.08395723755226109,
          0.08220024890163888,
          0.08265994797689089,
          0.10548729044772294,
          0.08270016460923824,
          0.08263693091573175,
          0.08408285162449515,
          0.08247965927491127,
          0.08510520859519778,
          0.08393599427281197,
          0.08298244561042319,
          0.08562942857561037,
          0.08615259273943957,
          0.08253164483660641,
          0.0834048056413013,
          0.0836444464588196,
          0.08644759051217846,
          0.10161932762356388,
          0.0860720895428612,
          0.08284569478174883,
          0.08371538532284166,
          0.08264559635056794,
          0.08284416313727293,
          0.08391010104815012,
          0.08260172216704732,
          0.08581525248878688,
          0.08305984930644876,
          0.08274444052530283,
          0.08488695733354572,
          0.08243008456223479,
          0.0829948282257483,
          0.08495372244600471,
          0.08321977121587064,
          0.08635692506489748,
          0.08264673692125761,
          0.08186846040844238,
          0.08263059881485445,
          0.10441472501698376,
          0.10528485609977607,
          0.08499971771788298,
          0.09259959919221114,
          0.08261318952776894,
          0.08385959899234947,
          0.0826226315654416,
          0.08258107343944868,
          0.08331235626742295,
          0.0834970814550906,
          0.08322184354859442,
          0.08366762990690224,
          0.08232107545407749,
          0.08502953198831671,
          0.08469003944748557,
          0.08222412784889119,
          0.08312009935117741,
          0.08571363180020228,
          0.08237007825013624,
          0.0835774816385849,
          0.08347237759742218,
          0.08308469365588271,
          0.08238220838082617,
          0.08422243198098708,
          0.08466688799410906,
          0.08329245959958595,
          0.0831207423118143,
          0.08315877480983573,
          0.0852580342864113,
          0.08307430873764746,
          0.08793018395955414,
          0.0830730472577487,
          0.08393387458472819,
          0.08540937207454335,
          0.08348777746597265,
          0.08265909677459007,
          0.08387453848047574,
          0.08635036092591931,
          0.0829349762863024,
          0.08241289957264089,
          0.10413855843091069,
          0.08253266605259721,
          0.08648364062869808,
          0.08191460734977042,
          0.08310340315683169,
          0.0829548528835559,
          0.0864318137649662,
          0.0829293169593892,
          0.10305693039776236,
          0.08356895669343908,
          0.08244647842569405,
          0.08415434846452788,
          0.0830010827587181,
          0.08438485467873562,
          0.08223489665962366,
          0.0827530514167065,
          0.08357849234207144,
          0.08273626391953012,
          0.08404602275072952,
          0.08231460044689605,
          0.08210942459292067,
          0.08321989252062911,
          0.08319843094976215,
          0.08535502837758135,
          0.083684048231146,
          0.0829440056016863,
          0.08306201265224991,
          0.08321963632160116,
          0.08523513451119122,
          0.08957199257037188,
          0.10218071104046678,
          0.08365996670182504,
          0.08288414315622594,
          0.0831609409532332,
          0.08563748131796066,
          0.08267228190331319,
          0.08351072308999888,
          0.08595073621997369,
          0.0830822013310413,
          0.09276760202112158,
          0.08320761930510705,
          0.08320375706140715,
          0.087980450153498,
          0.08257120222868822,
          0.08382900630949686,
          0.08347497757570743,
          0.09168496044283138,
          0.08287356761286935,
          0.09610522792691246,
          0.0840937911692142,
          0.08320281863107817,
          0.0840821741688485,
          0.08296261071055012,
          0.08702358037168506,
          0.08304670690396758,
          0.08282960813412675,
          0.08504832917070236,
          0.08280821131692603,
          0.08357559868071203,
          0.10030268846476795,
          0.0830115284099357,
          0.08538684115689459,
          0.08316314519172648,
          0.08674664611182713,
          0.0833060353078631,
          0.08276759290831337,
          0.082636621675815,
          0.08519768869200897,
          0.08343585614273649,
          0.08348041210697296,
          0.08479695608461686,
          0.10471228685959055,
          0.08411816094404848,
          0.08253897607087386,
          0.08282121749834048,
          0.08301176176181067,
          0.08422872819064456,
          0.08248307926521628,
          0.0873169601423071,
          0.08403867263588274,
          0.08360671456693249,
          0.0873387381930901,
          0.08303636122381383,
          0.08429013910580663,
          0.08245037943250491,
          0.08731979299476997,
          0.0831649244330491,
          0.08299715585650622,
          0.08383338746534745,
          0.08300374574868034,
          0.08221742281473433,
          0.08328189460011386,
          0.08380170444242878,
          0.08724155313411035,
          0.08266658783360512,
          0.0827461062704319,
          0.08366220360416872,
          0.08926097460933158,
          0.0820128908958929,
          0.08239676192015702,
          0.08335814330410822,
          0.08271122899355499,
          0.09320347156581618,
          0.08244578693077234,
          0.08455687464846962,
          0.0827144043953543,
          0.09735407733563475,
          0.08311742873296009,
          0.08253193283787222,
          0.08503616705876,
          0.08229462409623334,
          0.10227328262969504,
          0.08244098270982371,
          0.08458546705476075,
          0.08245023856627462,
          0.08390096654345602,
          0.08261727710396506,
          0.08296341485333819,
          0.0843856743061751,
          0.0826403264007882,
          0.08236202033659863,
          0.08491342896501539,
          0.08249553332142545,
          0.08248501382813546,
          0.08325999242968649,
          0.08317749236739873,
          0.08683336062238954,
          0.0816765138371928,
          0.08259875051057868,
          0.08402034285954971,
          0.08252324178034955,
          0.09427206101801941,
          0.08361224594033768,
          0.08780454576834186,
          0.08306918463313627,
          0.08294936107813836,
          0.08226373331746348,
          0.08270158075217962,
          0.09324708168090719,
          0.08375970454230489,
          0.08278135784160373,
          0.08603363354510356,
          0.1045542831789172,
          0.0826534965564436,
          0.08254708094217993,
          0.08678866035047919,
          0.08257299826086026,
          0.10524439261200529,
          0.08462142450028,
          0.09693374076406427,
          0.10746782345354618,
          0.0865524613434759,
          0.08328747282232361,
          0.08874189978266406,
          0.08327021589551106,
          0.09380548783775086,
          0.08239314017106542,
          0.08329407297801958,
          0.08299425059418443,
          0.08576349535115597,
          0.0833380400574528,
          0.08329158468354203,
          0.09177749989897299,
          0.08283706581670945,
          0.08460100921379025,
          0.08298000691791643,
          0.0826500350927559,
          0.08338863250547998,
          0.08284291699085977,
          0.08353928226982023,
          0.0828191079128297,
          0.08425831915679907,
          0.08394784695156927,
          0.1048112802167831,
          0.08349310994313272,
          0.08527538963067192,
          0.08234989201226825,
          0.08602489920911367,
          0.08372643663163837,
          0.08333347635755216,
          0.0831372896782915,
          0.08252173787708882,
          0.0830012992386513,
          0.08352099809410667,
          0.08484631339069673,
          0.08376725637774293,
          0.08248257995669252,
          0.10291475756183441,
          0.08266324199933343,
          0.08417132121165083,
          0.08288839769821313,
          0.08226414886960809,
          0.10099639770365246,
          0.08272441754739207,
          0.08387966155676363,
          0.09299240361592506,
          0.08444045941099101,
          0.0834123863076146,
          0.08412272761003801,
          0.08446355118849863,
          0.08334290321166601,
          0.08245797628427717,
          0.09368903580642896,
          0.08579221761627058,
          0.0833885768570585,
          0.08239055214740282,
          0.08249961918811777,
          0.08306052813844458,
          0.0831357568439862,
          0.08409036395262161,
          0.08337417185758891,
          0.0829387506742927,
          0.08260896282855443,
          0.08498237910796849,
          0.08300392716098079,
          0.08195714197156158,
          0.08418743298567412,
          0.10038129945318305,
          0.08327494977087521,
          0.08373973990748947,
          0.08308164190378595,
          0.09100211356259427,
          0.08563965910037585,
          0.08583124169737358,
          0.08392212825027276,
          0.08279077028629671,
          0.08236456924978328,
          0.10629128661382152,
          0.08250091564787407,
          0.08713988911407138,
          0.08433474922626143,
          0.0826654757872344,
          0.08395989014513086,
          0.08574106211056873,
          0.08285930018085841,
          0.08299692922839406,
          0.08441498679660339,
          0.08389846280538288,
          0.0848405257616334,
          0.0834616221668075,
          0.08584030665022686,
          0.08322891696303439,
          0.08353011074735918,
          0.08309285294850441,
          0.08651283933196181,
          0.08310693915881492,
          0.08290063321512871,
          0.08320239035774854,
          0.08601050254872308,
          0.08393616543478491,
          0.08260553336789768,
          0.085613102481566,
          0.0825841065356284,
          0.08324735921670097,
          0.08332004573388377,
          0.0931690848266701,
          0.0859662930507487,
          0.08373161134596022,
          0.08301017264797066,
          0.08336194886702872,
          0.08255570842720951,
          0.08764987998468635,
          0.08336795546281142,
          0.08398129766152383,
          0.0848594980991578,
          0.08266155455990679,
          0.08247719720819359,
          0.08374440729884888,
          0.08310656364763773,
          0.10226158936330351,
          0.09595609196563701,
          0.08276011433173194,
          0.08717349665672955,
          0.08246932495623632,
          0.08319389051401872,
          0.0824771668600773,
          0.08573726649146093,
          0.08725387877287781,
          0.08304789224788486,
          0.08434879957739697,
          0.08363829132457859,
          0.10460715119245341,
          0.08220427027112753,
          0.08271754200267924,
          0.08689833135454574,
          0.0834464881164965,
          0.08562707073378686,
          0.08321486883375731,
          0.08404642113923066,
          0.08519422596037746,
          0.08574035101850674,
          0.08260114456740034,
          0.08226617978994574,
          0.09218102338526904,
          0.08182862624074039,
          0.08255383901034316,
          0.08343164795439631,
          0.09953847346653258,
          0.08232191407943026,
          0.0817069837129533,
          0.0867796506853938,
          0.08460698583192258,
          0.0817647500281331,
          0.08327299992035239,
          0.08248162883909414,
          0.0840224012952457,
          0.08418530669104944,
          0.087973293685031,
          0.08268698123591149,
          0.08437574712624792,
          0.08327755337916884,
          0.08214275629720465,
          0.08414232814750208,
          0.08329582745550315,
          0.08720531957157558,
          0.08479025950591201,
          0.08275803518911087,
          0.08291628786089525,
          0.08393236306890406,
          0.10079997009926882,
          0.08716367263376307,
          0.08492904714119187,
          0.08386324725465437,
          0.08271281080959585,
          0.08308604866601085,
          0.0834212191163988,
          0.08595553137476505,
          0.08234046626972627,
          0.08227295477044481,
          0.08406219882785552,
          0.08224580632195619,
          0.08345248633800766,
          0.08819583941980899,
          0.08470703788496611,
          0.08301319642691471,
          0.08769757652837515,
          0.0843761094860751,
          0.08335234029658274,
          0.08313066301315937,
          0.0833117535252543,
          0.08264534952309555,
          0.08355628770407349,
          0.08442672436586694,
          0.08236143286177203,
          0.083264741051013,
          0.08688689989921365,
          0.08449092200401603,
          0.08576533320175621,
          0.08308582311144246,
          0.09109043997091111,
          0.08254199390917345,
          0.0821983819056997,
          0.08240965082024795,
          0.08636383225110883,
          0.10300281038002383,
          0.08214853769173854,
          0.08268008983563277,
          0.08370669105344264,
          0.08268520080035745,
          0.08279714856339851,
          0.08713031875136815,
          0.08250872961798147,
          0.08243085229276693,
          0.08238836573806105,
          0.0825027710727266,
          0.08298343614744504,
          0.08356331086152853,
          0.09050036013864263,
          0.08511173574036432,
          0.08341313343305887,
          0.08258826362434266,
          0.0829828554386322,
          0.08276858925895582,
          0.08433108819297482,
          0.08237576200573207
         ]
        },
        {
         "mode": "lines",
         "name": "Best Value",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500,
          501,
          502,
          503,
          504,
          505,
          506,
          507,
          508,
          509,
          510,
          511,
          512,
          513,
          514,
          515,
          516,
          517,
          518,
          519,
          520,
          521,
          522,
          523,
          524,
          525,
          526,
          527,
          528,
          529,
          530,
          531,
          532,
          533,
          534,
          535,
          536,
          537,
          538,
          539,
          540,
          541,
          542,
          543,
          544,
          545,
          546,
          547,
          548,
          549,
          550,
          551,
          552,
          553,
          554,
          555,
          556,
          557,
          558,
          559,
          560,
          561,
          562,
          563,
          564,
          565,
          566,
          567,
          568,
          569,
          570,
          571,
          572,
          573,
          574,
          575,
          576,
          577,
          578,
          579,
          580,
          581,
          582,
          583,
          584,
          585,
          586,
          587,
          588,
          589,
          590,
          591,
          592,
          593,
          594,
          595,
          596,
          597,
          598,
          599,
          600,
          601,
          602,
          603,
          604,
          605,
          606,
          607,
          608,
          609,
          610,
          611,
          612,
          613,
          614,
          615,
          616,
          617,
          618,
          619,
          620,
          621,
          622,
          623,
          624,
          625,
          626,
          627,
          628,
          629,
          630,
          631,
          632,
          633,
          634,
          635,
          636,
          637,
          638,
          639,
          640,
          641,
          642,
          643,
          644,
          645,
          646,
          647,
          648,
          649,
          650,
          651,
          652,
          653,
          654,
          655,
          656,
          657,
          658,
          659,
          660,
          661,
          662,
          663,
          664,
          665,
          666,
          667,
          668,
          669,
          670,
          671,
          672,
          673,
          674,
          675,
          676,
          677,
          678,
          679,
          680,
          681,
          682,
          683,
          684,
          685,
          686,
          687,
          688,
          689,
          690,
          691,
          692,
          693,
          694,
          695,
          696,
          697,
          698,
          699,
          700,
          701,
          702,
          703,
          704,
          705,
          706,
          707,
          708,
          709,
          710,
          711,
          712,
          713,
          714,
          715,
          716,
          717,
          718,
          719,
          720,
          721,
          722,
          723,
          724,
          725,
          726,
          727,
          728,
          729,
          730,
          731,
          732,
          733,
          734,
          735,
          736,
          737,
          738,
          739,
          740,
          741,
          742,
          743,
          744,
          745,
          746,
          747,
          748,
          749,
          750,
          751,
          752,
          753,
          754,
          755,
          756,
          757,
          758,
          759,
          760,
          761,
          762,
          763,
          764,
          765,
          766,
          767,
          768,
          769,
          770,
          771,
          772,
          773,
          774,
          775,
          776,
          777,
          778,
          779,
          780,
          781,
          782,
          783,
          784,
          785,
          786,
          787,
          788,
          789,
          790,
          791,
          792,
          793,
          794,
          795,
          796,
          797,
          798,
          799,
          800,
          801,
          802,
          803,
          804,
          805,
          806,
          807,
          808,
          809,
          810,
          811,
          812,
          813,
          814,
          815,
          816,
          817,
          818,
          819,
          820,
          821,
          822,
          823,
          824,
          825,
          826,
          827,
          828,
          829,
          830,
          831,
          832,
          833,
          834,
          835,
          836,
          837,
          838,
          839,
          840,
          841,
          842,
          843,
          844,
          845,
          846,
          847,
          848,
          849,
          850,
          851,
          852,
          853,
          854,
          855,
          856,
          857,
          858,
          859,
          860,
          861,
          862,
          863,
          864,
          865,
          866,
          867,
          868,
          869,
          870,
          871,
          872,
          873,
          874,
          875,
          876,
          877,
          878,
          879,
          880,
          881,
          882,
          883,
          884,
          885,
          886,
          887,
          888,
          889,
          890,
          891,
          892,
          893,
          894,
          895,
          896,
          897,
          898,
          899,
          900,
          901,
          902,
          903,
          904,
          905,
          906,
          907,
          908,
          909,
          910,
          911,
          912,
          913,
          914,
          915,
          916,
          917,
          918,
          919,
          920,
          921,
          922,
          923,
          924,
          925,
          926,
          927,
          928,
          929,
          930,
          931,
          932,
          933,
          934,
          935,
          936,
          937,
          938,
          939,
          940,
          941,
          942,
          943,
          944,
          945,
          946,
          947,
          948,
          949,
          950,
          951,
          952,
          953,
          954,
          955,
          956,
          957,
          958,
          959,
          960,
          961,
          962,
          963,
          964,
          965,
          966,
          967,
          968,
          969,
          970,
          971,
          972,
          973,
          974,
          975,
          976,
          977,
          978,
          979,
          980,
          981,
          982,
          983,
          984,
          985,
          986,
          987,
          988,
          989,
          990,
          991,
          992,
          993,
          994,
          995,
          996,
          997,
          998,
          999
         ],
         "y": [
          0.08539019019252586,
          0.08539019019252586,
          0.08539019019252586,
          0.08539019019252586,
          0.08539019019252586,
          0.08442866030728638,
          0.08442866030728638,
          0.08442866030728638,
          0.08442866030728638,
          0.08442866030728638,
          0.08442866030728638,
          0.08442866030728638,
          0.08442866030728638,
          0.08442866030728638,
          0.08442866030728638,
          0.08442866030728638,
          0.08442866030728638,
          0.08442866030728638,
          0.08442866030728638,
          0.08442866030728638,
          0.08442866030728638,
          0.08442866030728638,
          0.08421050029548485,
          0.08421050029548485,
          0.08421050029548485,
          0.08421050029548485,
          0.08421050029548485,
          0.08421050029548485,
          0.08421050029548485,
          0.08421050029548485,
          0.08332985614930138,
          0.08332985614930138,
          0.08332985614930138,
          0.08332985614930138,
          0.08332985614930138,
          0.08332985614930138,
          0.08332985614930138,
          0.08332985614930138,
          0.08315042884967586,
          0.08315042884967586,
          0.08315042884967586,
          0.08315042884967586,
          0.08315042884967586,
          0.08315042884967586,
          0.08315042884967586,
          0.08315042884967586,
          0.08315042884967586,
          0.08315042884967586,
          0.08315042884967586,
          0.08315042884967586,
          0.08315042884967586,
          0.08315042884967586,
          0.08315042884967586,
          0.08315042884967586,
          0.08315042884967586,
          0.08315042884967586,
          0.08315042884967586,
          0.08315042884967586,
          0.08315042884967586,
          0.08315042884967586,
          0.08315042884967586,
          0.08315042884967586,
          0.08315042884967586,
          0.08285306771082904,
          0.08285306771082904,
          0.08285306771082904,
          0.08285306771082904,
          0.08285306771082904,
          0.08285306771082904,
          0.08285306771082904,
          0.08285306771082904,
          0.08285306771082904,
          0.08285306771082904,
          0.08285306771082904,
          0.08285306771082904,
          0.08285306771082904,
          0.08285306771082904,
          0.08285306771082904,
          0.08285306771082904,
          0.08285306771082904,
          0.08285306771082904,
          0.08285306771082904,
          0.08285306771082904,
          0.08285306771082904,
          0.08285306771082904,
          0.08285306771082904,
          0.08285306771082904,
          0.08285306771082904,
          0.08285306771082904,
          0.08285306771082904,
          0.08285306771082904,
          0.08285306771082904,
          0.08285306771082904,
          0.08285306771082904,
          0.08285306771082904,
          0.08285306771082904,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188,
          0.08165961576392188
         ]
        },
        {
         "marker": {
          "color": "#cccccc"
         },
         "mode": "markers",
         "name": "Infeasible Trial",
         "showlegend": false,
         "type": "scatter",
         "x": [],
         "y": []
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Optimization History Plot"
        },
        "xaxis": {
         "title": {
          "text": "Trial"
         }
        },
        "yaxis": {
         "title": {
          "text": "Objective Value"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d20cc49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "cliponaxis": false,
         "hovertemplate": [
          "min_child_weight (IntDistribution): 0.00991614819855299<extra></extra>",
          "n_estimators (IntDistribution): 0.01905803826815064<extra></extra>",
          "learning_rate (FloatDistribution): 0.15316001238489357<extra></extra>",
          "max_depth (IntDistribution): 0.8178658011484029<extra></extra>"
         ],
         "name": "Objective Value",
         "orientation": "h",
         "text": [
          "<0.01",
          "0.02",
          "0.15",
          "0.82"
         ],
         "textposition": "outside",
         "type": "bar",
         "x": [
          0.00991614819855299,
          0.01905803826815064,
          0.15316001238489357,
          0.8178658011484029
         ],
         "y": [
          "min_child_weight",
          "n_estimators",
          "learning_rate",
          "max_depth"
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Hyperparameter Importances"
        },
        "xaxis": {
         "title": {
          "text": "Hyperparameter Importance"
         }
        },
        "yaxis": {
         "title": {
          "text": "Hyperparameter"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea0bd165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": [
           0,
           1,
           2,
           3,
           4,
           5,
           6,
           7,
           8,
           9,
           10,
           11,
           12,
           13,
           14,
           15,
           16,
           17,
           18,
           19,
           20,
           21,
           22,
           23,
           24,
           25,
           26,
           27,
           28,
           29,
           30,
           31,
           32,
           33,
           34,
           35,
           36,
           37,
           38,
           39,
           40,
           41,
           42,
           43,
           44,
           45,
           46,
           47,
           48,
           49,
           50,
           51,
           52,
           53,
           54,
           55,
           56,
           57,
           58,
           59,
           60,
           61,
           62,
           63,
           64,
           65,
           66,
           67,
           68,
           69,
           70,
           71,
           72,
           73,
           74,
           75,
           76,
           77,
           78,
           79,
           80,
           81,
           82,
           83,
           84,
           85,
           86,
           87,
           88,
           89,
           90,
           91,
           92,
           93,
           94,
           95,
           96,
           97,
           98,
           99,
           100,
           101,
           102,
           103,
           104,
           105,
           106,
           107,
           108,
           109,
           110,
           111,
           112,
           113,
           114,
           115,
           116,
           117,
           118,
           119,
           120,
           121,
           122,
           123,
           124,
           125,
           126,
           127,
           128,
           129,
           130,
           131,
           132,
           133,
           134,
           135,
           136,
           137,
           138,
           139,
           140,
           141,
           142,
           143,
           144,
           145,
           146,
           147,
           148,
           149,
           150,
           151,
           152,
           153,
           154,
           155,
           156,
           157,
           158,
           159,
           160,
           161,
           162,
           163,
           164,
           165,
           166,
           167,
           168,
           169,
           170,
           171,
           172,
           173,
           174,
           175,
           176,
           177,
           178,
           179,
           180,
           181,
           182,
           183,
           184,
           185,
           186,
           187,
           188,
           189,
           190,
           191,
           192,
           193,
           194,
           195,
           196,
           197,
           198,
           199,
           200,
           201,
           202,
           203,
           204,
           205,
           206,
           207,
           208,
           209,
           210,
           211,
           212,
           213,
           214,
           215,
           216,
           217,
           218,
           219,
           220,
           221,
           222,
           223,
           224,
           225,
           226,
           227,
           228,
           229,
           230,
           231,
           232,
           233,
           234,
           235,
           236,
           237,
           238,
           239,
           240,
           241,
           242,
           243,
           244,
           245,
           246,
           247,
           248,
           249,
           250,
           251,
           252,
           253,
           254,
           255,
           256,
           257,
           258,
           259,
           260,
           261,
           262,
           263,
           264,
           265,
           266,
           267,
           268,
           269,
           270,
           271,
           272,
           273,
           274,
           275,
           276,
           277,
           278,
           279,
           280,
           281,
           282,
           283,
           284,
           285,
           286,
           287,
           288,
           289,
           290,
           291,
           292,
           293,
           294,
           295,
           296,
           297,
           298,
           299,
           300,
           301,
           302,
           303,
           304,
           305,
           306,
           307,
           308,
           309,
           310,
           311,
           312,
           313,
           314,
           315,
           316,
           317,
           318,
           319,
           320,
           321,
           322,
           323,
           324,
           325,
           326,
           327,
           328,
           329,
           330,
           331,
           332,
           333,
           334,
           335,
           336,
           337,
           338,
           339,
           340,
           341,
           342,
           343,
           344,
           345,
           346,
           347,
           348,
           349,
           350,
           351,
           352,
           353,
           354,
           355,
           356,
           357,
           358,
           359,
           360,
           361,
           362,
           363,
           364,
           365,
           366,
           367,
           368,
           369,
           370,
           371,
           372,
           373,
           374,
           375,
           376,
           377,
           378,
           379,
           380,
           381,
           382,
           383,
           384,
           385,
           386,
           387,
           388,
           389,
           390,
           391,
           392,
           393,
           394,
           395,
           396,
           397,
           398,
           399,
           400,
           401,
           402,
           403,
           404,
           405,
           406,
           407,
           408,
           409,
           410,
           411,
           412,
           413,
           414,
           415,
           416,
           417,
           418,
           419,
           420,
           421,
           422,
           423,
           424,
           425,
           426,
           427,
           428,
           429,
           430,
           431,
           432,
           433,
           434,
           435,
           436,
           437,
           438,
           439,
           440,
           441,
           442,
           443,
           444,
           445,
           446,
           447,
           448,
           449,
           450,
           451,
           452,
           453,
           454,
           455,
           456,
           457,
           458,
           459,
           460,
           461,
           462,
           463,
           464,
           465,
           466,
           467,
           468,
           469,
           470,
           471,
           472,
           473,
           474,
           475,
           476,
           477,
           478,
           479,
           480,
           481,
           482,
           483,
           484,
           485,
           486,
           487,
           488,
           489,
           490,
           491,
           492,
           493,
           494,
           495,
           496,
           497,
           498,
           499,
           500,
           501,
           502,
           503,
           504,
           505,
           506,
           507,
           508,
           509,
           510,
           511,
           512,
           513,
           514,
           515,
           516,
           517,
           518,
           519,
           520,
           521,
           522,
           523,
           524,
           525,
           526,
           527,
           528,
           529,
           530,
           531,
           532,
           533,
           534,
           535,
           536,
           537,
           538,
           539,
           540,
           541,
           542,
           543,
           544,
           545,
           546,
           547,
           548,
           549,
           550,
           551,
           552,
           553,
           554,
           555,
           556,
           557,
           558,
           559,
           560,
           561,
           562,
           563,
           564,
           565,
           566,
           567,
           568,
           569,
           570,
           571,
           572,
           573,
           574,
           575,
           576,
           577,
           578,
           579,
           580,
           581,
           582,
           583,
           584,
           585,
           586,
           587,
           588,
           589,
           590,
           591,
           592,
           593,
           594,
           595,
           596,
           597,
           598,
           599,
           600,
           601,
           602,
           603,
           604,
           605,
           606,
           607,
           608,
           609,
           610,
           611,
           612,
           613,
           614,
           615,
           616,
           617,
           618,
           619,
           620,
           621,
           622,
           623,
           624,
           625,
           626,
           627,
           628,
           629,
           630,
           631,
           632,
           633,
           634,
           635,
           636,
           637,
           638,
           639,
           640,
           641,
           642,
           643,
           644,
           645,
           646,
           647,
           648,
           649,
           650,
           651,
           652,
           653,
           654,
           655,
           656,
           657,
           658,
           659,
           660,
           661,
           662,
           663,
           664,
           665,
           666,
           667,
           668,
           669,
           670,
           671,
           672,
           673,
           674,
           675,
           676,
           677,
           678,
           679,
           680,
           681,
           682,
           683,
           684,
           685,
           686,
           687,
           688,
           689,
           690,
           691,
           692,
           693,
           694,
           695,
           696,
           697,
           698,
           699,
           700,
           701,
           702,
           703,
           704,
           705,
           706,
           707,
           708,
           709,
           710,
           711,
           712,
           713,
           714,
           715,
           716,
           717,
           718,
           719,
           720,
           721,
           722,
           723,
           724,
           725,
           726,
           727,
           728,
           729,
           730,
           731,
           732,
           733,
           734,
           735,
           736,
           737,
           738,
           739,
           740,
           741,
           742,
           743,
           744,
           745,
           746,
           747,
           748,
           749,
           750,
           751,
           752,
           753,
           754,
           755,
           756,
           757,
           758,
           759,
           760,
           761,
           762,
           763,
           764,
           765,
           766,
           767,
           768,
           769,
           770,
           771,
           772,
           773,
           774,
           775,
           776,
           777,
           778,
           779,
           780,
           781,
           782,
           783,
           784,
           785,
           786,
           787,
           788,
           789,
           790,
           791,
           792,
           793,
           794,
           795,
           796,
           797,
           798,
           799,
           800,
           801,
           802,
           803,
           804,
           805,
           806,
           807,
           808,
           809,
           810,
           811,
           812,
           813,
           814,
           815,
           816,
           817,
           818,
           819,
           820,
           821,
           822,
           823,
           824,
           825,
           826,
           827,
           828,
           829,
           830,
           831,
           832,
           833,
           834,
           835,
           836,
           837,
           838,
           839,
           840,
           841,
           842,
           843,
           844,
           845,
           846,
           847,
           848,
           849,
           850,
           851,
           852,
           853,
           854,
           855,
           856,
           857,
           858,
           859,
           860,
           861,
           862,
           863,
           864,
           865,
           866,
           867,
           868,
           869,
           870,
           871,
           872,
           873,
           874,
           875,
           876,
           877,
           878,
           879,
           880,
           881,
           882,
           883,
           884,
           885,
           886,
           887,
           888,
           889,
           890,
           891,
           892,
           893,
           894,
           895,
           896,
           897,
           898,
           899,
           900,
           901,
           902,
           903,
           904,
           905,
           906,
           907,
           908,
           909,
           910,
           911,
           912,
           913,
           914,
           915,
           916,
           917,
           918,
           919,
           920,
           921,
           922,
           923,
           924,
           925,
           926,
           927,
           928,
           929,
           930,
           931,
           932,
           933,
           934,
           935,
           936,
           937,
           938,
           939,
           940,
           941,
           942,
           943,
           944,
           945,
           946,
           947,
           948,
           949,
           950,
           951,
           952,
           953,
           954,
           955,
           956,
           957,
           958,
           959,
           960,
           961,
           962,
           963,
           964,
           965,
           966,
           967,
           968,
           969,
           970,
           971,
           972,
           973,
           974,
           975,
           976,
           977,
           978,
           979,
           980,
           981,
           982,
           983,
           984,
           985,
           986,
           987,
           988,
           989,
           990,
           991,
           992,
           993,
           994,
           995,
           996,
           997,
           998,
           999
          ],
          "colorbar": {
           "title": {
            "text": "Trial"
           },
           "x": 1,
           "xpad": 40
          },
          "colorscale": [
           [
            0,
            "rgb(247,251,255)"
           ],
           [
            0.125,
            "rgb(222,235,247)"
           ],
           [
            0.25,
            "rgb(198,219,239)"
           ],
           [
            0.375,
            "rgb(158,202,225)"
           ],
           [
            0.5,
            "rgb(107,174,214)"
           ],
           [
            0.625,
            "rgb(66,146,198)"
           ],
           [
            0.75,
            "rgb(33,113,181)"
           ],
           [
            0.875,
            "rgb(8,81,156)"
           ],
           [
            1,
            "rgb(8,48,107)"
           ]
          ],
          "line": {
           "color": "Grey",
           "width": 0.5
          },
          "showscale": true
         },
         "mode": "markers",
         "name": "Feasible Trial",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0.08978658960089328,
          0.04225670585219596,
          0.09152415967258339,
          0.08499608012413307,
          0.017248301793842766,
          0.14454227768925998,
          0.013405067306329557,
          0.02678845034771573,
          0.01097092238863852,
          0.07777355373762264,
          0.27267565225837204,
          0.19045697560007604,
          0.22790075635283982,
          0.1617740873548449,
          0.1505512620673958,
          0.154721973803218,
          0.05256557470451341,
          0.19186452535726453,
          0.11733138927856819,
          0.03726403894408621,
          0.02520961766152917,
          0.035327354565561804,
          0.0610564159249591,
          0.05832186435832847,
          0.054532236488827504,
          0.02967135026923937,
          0.0222935837699687,
          0.06690915411666694,
          0.04223229255158766,
          0.12148859170414321,
          0.11249292670209345,
          0.115050939635253,
          0.10282474929939976,
          0.1023454873561578,
          0.07595117182921536,
          0.07723971680912345,
          0.10283928424506285,
          0.12182892124864252,
          0.09093076376412754,
          0.09024050537160284,
          0.08504677843784278,
          0.08526438478797285,
          0.07171610758179825,
          0.09038952555685499,
          0.050090028121262106,
          0.0680388176581034,
          0.135195239675003,
          0.08695949298730145,
          0.18448885399020545,
          0.07610270793434759,
          0.0461730771330967,
          0.09265727991103724,
          0.09489057507106707,
          0.06180117209397851,
          0.13813118130903948,
          0.09583258825663805,
          0.07947726297966977,
          0.1722617953681521,
          0.07739471993573048,
          0.1075160121036574,
          0.13028686825818497,
          0.08460950328385586,
          0.06569259144917551,
          0.08077242352373684,
          0.05685210698664559,
          0.11135592890465919,
          0.22364142672437978,
          0.07219916763828681,
          0.08238202829404043,
          0.15527121101810207,
          0.09802483535771658,
          0.08405483104230632,
          0.0675263469060003,
          0.12299342371480639,
          0.07981864573742056,
          0.014092989635293521,
          0.1086039143559855,
          0.060595925185273916,
          0.05198491090905934,
          0.0817185794344923,
          0.09718350156459914,
          0.07307146616511602,
          0.07916804669993499,
          0.08640028784086477,
          0.09448692513835313,
          0.11139774869374261,
          0.06425382491006983,
          0.12268207362193763,
          0.14473459322293603,
          0.08869978895266002,
          0.04681430509551698,
          0.07118296299886646,
          0.10047759061246937,
          0.09958471917536217,
          0.08865499678568776,
          0.10271048638980614,
          0.1305406029660337,
          0.13579984952374738,
          0.128351553549392,
          0.16336367009525457,
          0.11684755278935727,
          0.11487771270054614,
          0.1160674269691832,
          0.10363236931060575,
          0.10486902407350014,
          0.11565471390482743,
          0.1467339355171277,
          0.14032232137911704,
          0.1716096310857956,
          0.13197299451598388,
          0.19686211973582013,
          0.21284438682818743,
          0.2573362514157516,
          0.18470150049441122,
          0.12342352819145197,
          0.19835927604465456,
          0.15550711222825636,
          0.1417590430315001,
          0.1411043833680087,
          0.25715681280501174,
          0.1077450816752893,
          0.10724059566697194,
          0.10542742723185726,
          0.10997590253386248,
          0.11668457280589149,
          0.1292886889574429,
          0.11809413583643358,
          0.1180279161431575,
          0.1481997044308352,
          0.16403809237444936,
          0.16573585552214937,
          0.11104707563596589,
          0.13801112479702166,
          0.14191624976621722,
          0.1537375807154451,
          0.1369706909144448,
          0.12395820780479369,
          0.17785803488799526,
          0.14353716350915044,
          0.13405820894560508,
          0.15869826973665974,
          0.11658539958968088,
          0.1277263961665088,
          0.10853179618895935,
          0.12110494404716916,
          0.14507074038584247,
          0.13639171825799357,
          0.11238564435102358,
          0.10322045668079496,
          0.1660799124374714,
          0.1651366333583015,
          0.15286807372382835,
          0.12802760864145787,
          0.12784958699998758,
          0.1261944095331,
          0.13532433933688193,
          0.14085302275617623,
          0.14069482415142037,
          0.15154086975109562,
          0.1330362377141421,
          0.1370092278862532,
          0.13185349868056861,
          0.17805642639231592,
          0.12025742635806869,
          0.010378973569057811,
          0.14716036895634124,
          0.09963558574504541,
          0.13973814936294954,
          0.1609132460578733,
          0.14178668903280967,
          0.1268643147989451,
          0.13290225573477593,
          0.12019996757476797,
          0.15214886960476046,
          0.11491677371037282,
          0.09436776330584665,
          0.10647568348454815,
          0.14108357812745545,
          0.1392830063377355,
          0.17338984425136839,
          0.15705465604688915,
          0.158421171065339,
          0.14615087650234898,
          0.1338226018140962,
          0.19077551462452097,
          0.12495509112417334,
          0.15032173910311109,
          0.1419365210881878,
          0.02080342899711038,
          0.15890258227803425,
          0.16926927538063177,
          0.16904208683351568,
          0.20773195866623698,
          0.18043615339507732,
          0.16509441720922918,
          0.1538172785739412,
          0.037374917201836444,
          0.12330769828374147,
          0.16042916038914823,
          0.14385568265471832,
          0.14224587898891486,
          0.15046884654817935,
          0.13023642816500441,
          0.17093907596417868,
          0.11869732635990873,
          0.1414067208791691,
          0.1381752365021477,
          0.13736718028084632,
          0.1382901086616111,
          0.13812304221937574,
          0.1286032868551115,
          0.14618571428673297,
          0.13808973747749967,
          0.1304660168185342,
          0.1293197990960976,
          0.1282709846134543,
          0.1249657112280401,
          0.12899706846722223,
          0.12265854350792257,
          0.12142548645862575,
          0.12695046859378734,
          0.12782331377936376,
          0.1233028505972129,
          0.11133452136965735,
          0.12369671382064866,
          0.12113605530093002,
          0.10990256239727371,
          0.1240892314689277,
          0.13384598550855942,
          0.11573371362562394,
          0.1512250693646204,
          0.13051819305340795,
          0.12553229969743437,
          0.1321980613581436,
          0.13022799664454113,
          0.12582295355401366,
          0.1306461235844909,
          0.12061138869972476,
          0.12030460811937331,
          0.12383216793083743,
          0.11260067480876691,
          0.13377565509996925,
          0.13305597335317693,
          0.11893581462617447,
          0.12330753726364783,
          0.11994707099506251,
          0.12414401068657961,
          0.12742768482582006,
          0.13219496439329864,
          0.11474885589868798,
          0.1241502238684362,
          0.1494991087348143,
          0.13432866134592425,
          0.124926024678362,
          0.14408896612176275,
          0.14490393078199054,
          0.11442389434732107,
          0.10657408998167088,
          0.11332185614130329,
          0.11743350889026086,
          0.12640080137565118,
          0.10426825177558603,
          0.11374029478435731,
          0.12331647684896496,
          0.12155435884246352,
          0.1139963114047943,
          0.12342887071002819,
          0.10764713410167656,
          0.1477851086419211,
          0.15139933317782445,
          0.1454667316843672,
          0.13473906654691553,
          0.15570116711532464,
          0.1334278205895036,
          0.14524655842574996,
          0.14493793627601226,
          0.12822881844104156,
          0.029424623795244804,
          0.14680658631822177,
          0.11180706672950355,
          0.1596647076635128,
          0.1601561323232085,
          0.1570134056819719,
          0.1432695740113138,
          0.11855465621049265,
          0.13757482304307347,
          0.013399778495374916,
          0.15706450307640876,
          0.18327569199882493,
          0.14152475896185862,
          0.09859684146603792,
          0.14957823486025926,
          0.16598630304534764,
          0.13708862791665433,
          0.12350921715179004,
          0.13438231578427978,
          0.11800022154866892,
          0.10442942862659536,
          0.11768636498561107,
          0.1513202508267093,
          0.11033988519544023,
          0.13790133434153026,
          0.14060581484453336,
          0.1476278484118076,
          0.16112454022311065,
          0.13108567402429414,
          0.13897800248180486,
          0.12822132343920442,
          0.15310343815449812,
          0.1761957503333402,
          0.13674488871801474,
          0.12434917433904207,
          0.1356332994882256,
          0.12038022904710605,
          0.13011994056216725,
          0.1458280112679509,
          0.13310804067309584,
          0.16058101456692556,
          0.11833799274094756,
          0.1390084446817895,
          0.12550358934501196,
          0.10787972119581064,
          0.15361831708872656,
          0.1183023176608315,
          0.13111286746066964,
          0.14706100859622576,
          0.1483784597945296,
          0.16922405208778718,
          0.16272617069275944,
          0.16690747633113653,
          0.17513492159295752,
          0.201744326145274,
          0.15893696609343366,
          0.11502050021881001,
          0.19199542829851968,
          0.1261119983256676,
          0.13839062525395407,
          0.14777406504294713,
          0.15958875150819574,
          0.28183571682530023,
          0.18339304412257268,
          0.15864922195203376,
          0.22873435311700674,
          0.13726266719431376,
          0.1743939296900131,
          0.0986764267333128,
          0.16323272359212893,
          0.16737418668167678,
          0.16333928423694558,
          0.18682580269285856,
          0.1612367696695066,
          0.1722502401260967,
          0.1791799355169692,
          0.15790232018038525,
          0.04139217389343924,
          0.1638557734298231,
          0.1536835958272594,
          0.14247459583800262,
          0.19321798887291278,
          0.15326469897021103,
          0.1654123313909216,
          0.17875733030102187,
          0.14071784533639825,
          0.15365795633244098,
          0.14441597406543358,
          0.1700284729570938,
          0.14850120633406186,
          0.1381824558269524,
          0.11035431570305058,
          0.10673704404460277,
          0.10085546983090601,
          0.1096439095644406,
          0.11362738374211649,
          0.11837369088267372,
          0.12846530317218494,
          0.12159918709159756,
          0.10473485939363336,
          0.1588106470835251,
          0.2131369313519442,
          0.09429658952122823,
          0.1131357865715089,
          0.13215392322762637,
          0.14912066158077822,
          0.12205004952071309,
          0.1774655527365081,
          0.13052937330805023,
          0.13271849711232264,
          0.16433111571613443,
          0.14836611835380692,
          0.14125458811777683,
          0.056022746594372785,
          0.020281215100443548,
          0.12900230178141608,
          0.19147390737421194,
          0.15759159883390034,
          0.14358542177433672,
          0.11300629895304438,
          0.13016555058226223,
          0.11711199315704739,
          0.17305987730210545,
          0.13612431063346395,
          0.15369809132501225,
          0.1046292265631998,
          0.12429077421281269,
          0.16553152521787645,
          0.14717100904205002,
          0.13525496203868753,
          0.11939313059434521,
          0.14232542742183865,
          0.10955386302155608,
          0.12846222422596343,
          0.15460480448480116,
          0.1804718823624195,
          0.1371759175449779,
          0.12073543052466343,
          0.09063822398633373,
          0.16525694331500834,
          0.14563648746893948,
          0.12993002928050323,
          0.10013008707833468,
          0.11593086846506695,
          0.1540899964735868,
          0.1253322891461921,
          0.1387684104549456,
          0.1285720018732122,
          0.16223142631104392,
          0.11042272963307947,
          0.1860127191520425,
          0.1430242256487536,
          0.14332917554812236,
          0.15488661173576204,
          0.1689256765108054,
          0.14925867967009335,
          0.134537371098811,
          0.14492870722400497,
          0.04917653175939728,
          0.1576758174977675,
          0.17250264071297358,
          0.13781731358248706,
          0.14836564222125972,
          0.13044260143878098,
          0.1994472236759336,
          0.16107090953015485,
          0.16970213811628693,
          0.1832325446059674,
          0.16320825854971938,
          0.15513545209100055,
          0.17649704029436367,
          0.1631226152238206,
          0.14914658470642245,
          0.01587337019268007,
          0.14171775424242444,
          0.1525744778637211,
          0.2336589214848097,
          0.15787298833549748,
          0.17751095265142924,
          0.15386769996771957,
          0.16468430777013007,
          0.19190295617452022,
          0.14084087470311535,
          0.15067430377322058,
          0.13641409772184496,
          0.16998641553189445,
          0.1452340280134206,
          0.15406959595541272,
          0.13461750161396244,
          0.16202811440850795,
          0.14432830559187199,
          0.1843077873500812,
          0.1297981904447046,
          0.16160960233647767,
          0.14669893473493567,
          0.13552391130513808,
          0.011695486704802017,
          0.1251899490256019,
          0.17405071845430634,
          0.1512903399379467,
          0.14052281899132224,
          0.20997270438957022,
          0.1586883559664302,
          0.1202684553412959,
          0.13239120491426323,
          0.16887767963475198,
          0.1444621329942297,
          0.1250804739862036,
          0.15458902228454457,
          0.2991405518739113,
          0.06341909075662817,
          0.13585722981870904,
          0.03559299288035241,
          0.17793439781725956,
          0.02922880309814184,
          0.14805547376078393,
          0.12810803752829156,
          0.16305586850227,
          0.18314857689295505,
          0.16582335826027694,
          0.1889788080938073,
          0.19867192150944626,
          0.024671263408131753,
          0.16011689792716266,
          0.1717421002487217,
          0.15733862092607928,
          0.16101351839315967,
          0.17340558680629345,
          0.15518712282686534,
          0.16911125650362807,
          0.15387586919944926,
          0.18505976207857144,
          0.150620817177425,
          0.1639096402034529,
          0.14411853109779815,
          0.15948355857136037,
          0.1431969508858246,
          0.17505496791476777,
          0.1521824313864104,
          0.14079333632945634,
          0.16748254715754632,
          0.1508259434454712,
          0.13920228717514224,
          0.20215020943658382,
          0.16010869186884255,
          0.17235734073239584,
          0.18564339247771802,
          0.13532132625393206,
          0.14936499105247958,
          0.1636173543482512,
          0.16779973207844429,
          0.18932139406284207,
          0.1759281951779109,
          0.16083495087714486,
          0.1586389039230595,
          0.18160363577172994,
          0.11795921766904939,
          0.22003759287571517,
          0.14168182341121105,
          0.15602796503694719,
          0.135826115700917,
          0.19901820311392066,
          0.16828028166544565,
          0.17375743716683584,
          0.16887776040111105,
          0.04239532779336404,
          0.19309868150230067,
          0.1825961726920937,
          0.16743890241568476,
          0.16047225107521063,
          0.17748280787919002,
          0.1546329370993487,
          0.11445061316291538,
          0.14933984675748252,
          0.11869519426304462,
          0.1314266942777292,
          0.1646660770959431,
          0.18346210546331748,
          0.14665408250232365,
          0.15849957083137878,
          0.07207864442815914,
          0.23990274177223708,
          0.19993462739990095,
          0.1291054978076435,
          0.13974655975435227,
          0.1221763712058863,
          0.170550105181068,
          0.14891361566879485,
          0.11356180837381391,
          0.13583541771198668,
          0.15734934105985496,
          0.17387314862216935,
          0.1467013206415009,
          0.12659470809342516,
          0.18780877261032047,
          0.16724782217695516,
          0.139296221738182,
          0.15808084582199747,
          0.12162667707954823,
          0.1335461517126799,
          0.15222553815925846,
          0.11533890114285782,
          0.17524720343810785,
          0.10611981878291742,
          0.14607377700060628,
          0.1621275125004416,
          0.13006012578130702,
          0.1162286580357564,
          0.12564307662319638,
          0.1310937377290127,
          0.21202204518730663,
          0.12345328546036881,
          0.1375073568238676,
          0.18138623007183724,
          0.10721237733687917,
          0.1301156789008013,
          0.09985057175362787,
          0.11641363489817311,
          0.15415071727416532,
          0.1659593999911231,
          0.14068887185172663,
          0.12515483968268246,
          0.19219842381316604,
          0.14906157628556277,
          0.16936286465080716,
          0.153887437635172,
          0.16274267235643122,
          0.176928520773795,
          0.15065712291382669,
          0.1454813109432611,
          0.1618673397027641,
          0.17909784336259274,
          0.1354133755789244,
          0.15562329873403538,
          0.11082842859136707,
          0.1443576064961453,
          0.16643887788763198,
          0.12999802253950452,
          0.05307825821680772,
          0.15087916556331513,
          0.20139288286607787,
          0.1891221547028895,
          0.11685827255936913,
          0.13979189757050017,
          0.1743654188293742,
          0.1625627713276094,
          0.12161010463579387,
          0.14859722881593906,
          0.1345603131370177,
          0.15705356794431038,
          0.14182146726019546,
          0.12728882667495164,
          0.12348797862714754,
          0.10213511870390063,
          0.05990862607331417,
          0.11224112088197917,
          0.018462722617312657,
          0.12860933352892376,
          0.1187443507234804,
          0.09466380305505799,
          0.1272006463263806,
          0.10957877901997236,
          0.13208713022608695,
          0.12141121216988328,
          0.13575999819655604,
          0.11411617134434474,
          0.13851187530982773,
          0.12837077796045013,
          0.14511033084209582,
          0.14401067022835423,
          0.12436072079592768,
          0.12989283455077824,
          0.13781332932286536,
          0.14833361627149233,
          0.1374834606729474,
          0.11893274146526779,
          0.10678818313067216,
          0.14703802506055938,
          0.13239896913106922,
          0.12153543318508482,
          0.15315080516969123,
          0.14177711843725133,
          0.2548747241399837,
          0.13252029174315946,
          0.11341129320436681,
          0.15019659428991877,
          0.1253339839805133,
          0.1439683151793777,
          0.15674875798905688,
          0.13211047169099865,
          0.11769398280515334,
          0.14734442027895514,
          0.12509698131434807,
          0.1027230032157396,
          0.03320702970818471,
          0.13862866638249727,
          0.08533593395197639,
          0.16938689419197644,
          0.15245795177883723,
          0.11001526821218946,
          0.13191424627600118,
          0.16020674929321868,
          0.14045922285529247,
          0.11801468492815838,
          0.17801388115925534,
          0.15503598270921382,
          0.13064695733567486,
          0.14111761936405912,
          0.12237132441088976,
          0.16069380900622773,
          0.16967395092410117,
          0.1433118490970702,
          0.18556228523366272,
          0.15017171241304128,
          0.12509694690062123,
          0.07757191136502622,
          0.11311557171009838,
          0.1292691009335565,
          0.13649513107876404,
          0.1676914202667482,
          0.1567562080628874,
          0.14511749037242144,
          0.11873708855386163,
          0.13539981108321883,
          0.18124041964484894,
          0.15166500535799038,
          0.1637461998205455,
          0.11011895534471687,
          0.12647269102228934,
          0.06671503268327093,
          0.13622406301697593,
          0.19430527365763028,
          0.14672297255525574,
          0.17251366606197688,
          0.1227627131097038,
          0.1043974306177286,
          0.15186767676099394,
          0.14112833497126306,
          0.1607043090824309,
          0.1654628802533645,
          0.1818743958189221,
          0.1671540803995799,
          0.0974637405572828,
          0.1577915077952911,
          0.17591148912188492,
          0.20942361729892106,
          0.11250294670220395,
          0.10386498814694381,
          0.11141623451369799,
          0.11133536494010217,
          0.09553541288513004,
          0.12122899050678361,
          0.11812073798303309,
          0.10623457439146874,
          0.12960668394221866,
          0.1159218586730344,
          0.11586838567679873,
          0.12954164878827032,
          0.1241353813681686,
          0.10263740811550334,
          0.1346284737200319,
          0.11288754244855999,
          0.12166117836382019,
          0.13252920054366898,
          0.1402095639822616,
          0.12351673195883892,
          0.08946796956625185,
          0.11512193984860616,
          0.13053703552390591,
          0.10780363924906386,
          0.1381036787311191,
          0.14275099212714165,
          0.1263790013532831,
          0.15401231759832698,
          0.14524438384179114,
          0.11833117503486733,
          0.0998525009301017,
          0.19254522680710542,
          0.10908886006442631,
          0.026647898132537002,
          0.11932005845407037,
          0.17173309554493013,
          0.15619211271281505,
          0.13137246881706985,
          0.14373196937490337,
          0.1158624162357173,
          0.042843726284875934,
          0.17767229932390421,
          0.16004421192955068,
          0.13610787419439035,
          0.1250428786350259,
          0.1506374313965675,
          0.13500536043816425,
          0.17224638785536334,
          0.14631564544756903,
          0.013625850783620138,
          0.18826524744798082,
          0.0229374984835686,
          0.01230901078678553,
          0.10735323983030207,
          0.12502488713863982,
          0.16423417775402224,
          0.11713230484586579,
          0.15151530887678683,
          0.13847891140955237,
          0.09271083581693935,
          0.12780347817724083,
          0.1597624857693718,
          0.14549308557812402,
          0.1342535060272968,
          0.04641348870264835,
          0.1125215474151408,
          0.17799139025876753,
          0.10059070712010132,
          0.12056863880773389,
          0.2009680472335005,
          0.15250381620117398,
          0.16599778434710202,
          0.14124538812396864,
          0.1274215546451594,
          0.15462616137961538,
          0.13357088099558048,
          0.16835846429333284,
          0.12015803022390714,
          0.1419592174015891,
          0.15558648700803293,
          0.1842789060181483,
          0.11092013575667425,
          0.1295974954849248,
          0.1455263993883801,
          0.13663957645342298,
          0.16227949319932292,
          0.1178836447692207,
          0.17092450787136657,
          0.12474742115561811,
          0.016200841840064612,
          0.14917926840519583,
          0.10698050792822686,
          0.1335363605080869,
          0.15901835104941656,
          0.010458650869590346,
          0.14391074424100592,
          0.1767632150689091,
          0.03840544431048918,
          0.19323404094153257,
          0.12644046497283262,
          0.22236113013090728,
          0.11505062033227188,
          0.1551993572049178,
          0.13918244623543205,
          0.0326010487335132,
          0.13033894328852597,
          0.16514411903862386,
          0.14901918426564081,
          0.1005023345821524,
          0.12254535422857084,
          0.13769495328945866,
          0.17827262250122794,
          0.08229887672934665,
          0.11308576987941066,
          0.15460360690578923,
          0.1419744950580771,
          0.1312083274621835,
          0.1658806232814831,
          0.18095678542287028,
          0.21052632048264738,
          0.16850676877998252,
          0.18629379573373017,
          0.17069966183423305,
          0.16229873682854556,
          0.19405781113347115,
          0.15828151622270306,
          0.1701692230805162,
          0.1502904022359585,
          0.1620530953546719,
          0.176215047252645,
          0.14942333872494162,
          0.15772705954587596,
          0.18707547658584547,
          0.14775920295544864,
          0.16758662872659952,
          0.14181107678124188,
          0.15707789466203675,
          0.1388840598690053,
          0.17713434890212187,
          0.15042202634867674,
          0.1986527036988652,
          0.16653443786052805,
          0.1352715793790048,
          0.14662575234668676,
          0.15462461322563228,
          0.1314376208418407,
          0.16195406116755112,
          0.18134382979687547,
          0.1437579922339411,
          0.12460274275878987,
          0.05742055212111398,
          0.17053475901004184,
          0.1397948438886735,
          0.15043954836571963,
          0.13265002817958377,
          0.16022513122254364,
          0.12195076456712403,
          0.19066856312436922,
          0.14108204708152572,
          0.17193390251585408,
          0.15529843045242492,
          0.12988232430013022,
          0.14190854994742402,
          0.16262529729994804,
          0.14784454450877502,
          0.17967940820076192,
          0.2047983195444124,
          0.12708046080689242,
          0.13481420190708907,
          0.15588222914210662,
          0.11959702048429091,
          0.16884397859763592,
          0.14822505888217746,
          0.13742475307465601,
          0.1850485695789017,
          0.12761894209574826,
          0.15586628793552948,
          0.14330565775641035,
          0.2684926221413563,
          0.1633002912647809,
          0.11933415878884515,
          0.1336145409888202,
          0.1735018621174706,
          0.1513013433305088,
          0.1269926455703126,
          0.14152530550953082,
          0.16256475347691218,
          0.11747895208757846,
          0.14848744691870336,
          0.13472753495801948,
          0.10859143440573993,
          0.07229589188662026,
          0.18389015426646027,
          0.1687751057217057,
          0.12401111523414146,
          0.15626623974421575,
          0.13728378569937613,
          0.1448147438328301,
          0.15377347277412867,
          0.13880058976984602,
          0.14646900394482942,
          0.162444710451635,
          0.13468610635171926,
          0.1725081263967201,
          0.16091301471451083,
          0.1766818880615941,
          0.16700581902181608,
          0.19852319063934928,
          0.1874887300591317,
          0.167121555591288,
          0.1600760284986654,
          0.18000156376169202,
          0.15534348984427815,
          0.1612332481565138,
          0.19178629677350842,
          0.17561431548320536,
          0.16253682436973171,
          0.20595263934713665,
          0.17004548173066703,
          0.16233267053294487,
          0.17856120466363304,
          0.1563729293381524,
          0.1542249774849553,
          0.18695402486080043,
          0.1704474751073428,
          0.15874508107341037,
          0.15099065435290046,
          0.17494674951920125,
          0.16684464047662384,
          0.1478858964727512,
          0.16325091486859175,
          0.18680967534761442,
          0.14957986550906002,
          0.16029542270571043,
          0.17304020205819748,
          0.19794176508635292,
          0.14685162469685276,
          0.15882297471452234,
          0.1794339982401027,
          0.22003123066241978,
          0.15063807189219117,
          0.1652297350938591,
          0.14228607598365733,
          0.15318222542263998,
          0.18001783589449,
          0.14349624431249564,
          0.16472905815976166,
          0.1385148334053181,
          0.15561464773291378,
          0.19424870780335707,
          0.17387509715279575,
          0.24650663804406728,
          0.14677304980427192,
          0.16117896721776728,
          0.14178659514206726,
          0.15321524788108443,
          0.17261100702059304,
          0.13683383908086277,
          0.13301959538715558,
          0.13450173694839665,
          0.12790078260726107,
          0.1382304006719829,
          0.13997454518710972,
          0.1290745231295914,
          0.14506892626253398,
          0.12265717949849361,
          0.13134249274701296,
          0.14436835278004947,
          0.13403418837706157,
          0.148399430193986,
          0.1212087498590984,
          0.15338518920892844,
          0.13587862301329842,
          0.11391082872008267,
          0.14392078129021887,
          0.12670312281649848,
          0.15760272502169156
         ],
         "xaxis": "x",
         "y": [
          0.08539019019252586,
          0.09495197683593737,
          0.08561203246811563,
          0.10226171316013058,
          0.09436835513192324,
          0.08442866030728638,
          0.0882806271297641,
          0.09259418891274053,
          0.0936506610503796,
          0.08905616282810194,
          0.10070957535224619,
          0.0849354185113468,
          0.09538829459268595,
          0.08532982608602123,
          0.09802117955542877,
          0.08861113027563677,
          0.08782249655960989,
          0.09671218874633744,
          0.10056915412131459,
          0.08463494265724372,
          0.08550715408794303,
          0.08624021489163447,
          0.08421050029548485,
          0.0894877666245822,
          0.08522084111774414,
          0.08664087579640814,
          0.09277247714187424,
          0.08890925915171902,
          0.08679846383946271,
          0.08434924485492348,
          0.08332985614930138,
          0.08381487607339143,
          0.08381121898840153,
          0.08492498104511295,
          0.08351886332682175,
          0.1065370630575128,
          0.08407738487249838,
          0.10418987847943002,
          0.08315042884967586,
          0.08360267337033281,
          0.08344470964379247,
          0.08381392901444998,
          0.0857432511381672,
          0.08371432932413264,
          0.08615883393220121,
          0.08428574598953223,
          0.0897804337144529,
          0.10368442923134472,
          0.08419051036454686,
          0.08666228717253156,
          0.08539823364234111,
          0.08398322451750445,
          0.08324196548934296,
          0.08498755749166624,
          0.08592689223502362,
          0.08643366899645041,
          0.0835047463204986,
          0.09288435334111321,
          0.08424549859482734,
          0.08502893317695907,
          0.0890072310285759,
          0.08354751438906698,
          0.08481137014593011,
          0.08285306771082904,
          0.08435363262967716,
          0.08440017469452545,
          0.10234792567968147,
          0.08439105115445075,
          0.08367204082049616,
          0.08674702124077736,
          0.08364722764310903,
          0.08339834509471708,
          0.08458083145212651,
          0.08730137187390588,
          0.08347106168854283,
          0.10497466632612777,
          0.09500932690462209,
          0.08627061663570071,
          0.08361465461377242,
          0.0862346237864177,
          0.08576719136060315,
          0.08392691309042451,
          0.08667835916016219,
          0.0832016218464397,
          0.08360618571881825,
          0.10570571526403402,
          0.08912974625484643,
          0.08794829675893581,
          0.10369587843510582,
          0.08450059575739881,
          0.08564046243395204,
          0.08425998493872724,
          0.08328057039506313,
          0.08414219287159663,
          0.08939263864603345,
          0.0846428294515632,
          0.08165961576392188,
          0.08488512104472777,
          0.0864821733201315,
          0.0844953596275288,
          0.08312414300199703,
          0.08302348043185244,
          0.08349016143148591,
          0.08273551529377218,
          0.08712664607608565,
          0.08335508002367598,
          0.0833304450341904,
          0.08274977982394807,
          0.08723091108735959,
          0.10129893784474617,
          0.08328120589468982,
          0.08518771642746378,
          0.08417473164618214,
          0.08544436252688223,
          0.084259756018591,
          0.08477727688870212,
          0.0843732621596494,
          0.08238179744888834,
          0.0835201737237383,
          0.08440188316439623,
          0.0826963136261387,
          0.08325320855427412,
          0.08232280947837918,
          0.08313899416292488,
          0.0827606041580992,
          0.08345781219298537,
          0.082856239988547,
          0.08315158199964165,
          0.09996173825297669,
          0.08287132257374849,
          0.08399392866757854,
          0.08329801970738117,
          0.08218916379001612,
          0.08272626181044221,
          0.08356949324059257,
          0.08344809825525434,
          0.08412450105439928,
          0.08436250442465335,
          0.08543737126852116,
          0.08315415930339068,
          0.08328874654472859,
          0.08377063932211924,
          0.08374662836316327,
          0.08432312077904969,
          0.08334078110772783,
          0.10451366567669003,
          0.08396547011835669,
          0.09866471516554136,
          0.0840355188769745,
          0.08313315216040826,
          0.0833273896064651,
          0.08380846367977608,
          0.08301209213271031,
          0.08302693484535764,
          0.08513153125053821,
          0.08294988420046369,
          0.0828023224844447,
          0.08576151350462309,
          0.08313593893209155,
          0.0823890790084934,
          0.10233081625126154,
          0.08391457740581108,
          0.08346778197338994,
          0.08323672801444174,
          0.11016564627256034,
          0.08625986833194307,
          0.08308838128817571,
          0.08256675863644616,
          0.08687355439209934,
          0.08320375301071112,
          0.0831569856636962,
          0.0830861335286607,
          0.08373209848835807,
          0.08325772728529475,
          0.0840249429842295,
          0.08340173491254706,
          0.0841523490888042,
          0.08278875829740572,
          0.08588354015186887,
          0.08460117799571931,
          0.08271264948133934,
          0.08311842459534399,
          0.09775820704966276,
          0.08363667644538213,
          0.08518966582885326,
          0.08234088467720187,
          0.08300663028468404,
          0.08294792493806535,
          0.09374333088296935,
          0.08186637706985608,
          0.08286524474377754,
          0.08407084466100902,
          0.08513404749620522,
          0.08512809066960167,
          0.08317299290426386,
          0.08329165608201898,
          0.09249173931498986,
          0.08581289239318927,
          0.08311832057969164,
          0.08244091747412202,
          0.0859276097402228,
          0.08294365575003566,
          0.08291915631246957,
          0.08431553209465299,
          0.08316980756401962,
          0.08275119431371236,
          0.08259810148865966,
          0.08269710428895796,
          0.0825306928188557,
          0.08327358623423534,
          0.08337420215674948,
          0.10445665012752847,
          0.08334076815055066,
          0.08274338024429977,
          0.08251096914423196,
          0.08199902606831211,
          0.08235772891596732,
          0.08293553119582622,
          0.08253509322945916,
          0.08376735221276418,
          0.08242039156277536,
          0.08310408342648781,
          0.08236920723797173,
          0.08291592089843107,
          0.08237535912990515,
          0.08328242745865577,
          0.08330519127524712,
          0.08333766438105057,
          0.08348678243602832,
          0.08346972442740377,
          0.08291621932822964,
          0.08231996370906212,
          0.08231519235453469,
          0.08261364924345689,
          0.0825212172002789,
          0.08282661754470844,
          0.08280000274572497,
          0.0822068912312609,
          0.08306979246734976,
          0.08280936601250934,
          0.08323729894382537,
          0.0827596216791415,
          0.08254312932582486,
          0.08260706905635237,
          0.08219159376324617,
          0.08332867404127005,
          0.08245010521755164,
          0.0830015532855319,
          0.08311543440998431,
          0.08354447071482189,
          0.08459523466839948,
          0.08279362737753312,
          0.0832079959920258,
          0.08271413443324369,
          0.08228669392391635,
          0.08746037507141037,
          0.08244453094845948,
          0.08267614433778225,
          0.08361087104719155,
          0.08254682461025294,
          0.08315199608854278,
          0.0831869149479296,
          0.0831621325240794,
          0.08241511203722858,
          0.08509378854913294,
          0.09446901061125146,
          0.0829485186897416,
          0.08256420848681219,
          0.08241588768524585,
          0.0832236338954678,
          0.08285732035700681,
          0.08516721649864492,
          0.08291222919437333,
          0.08335991669751068,
          0.08242883783270984,
          0.08266547354134668,
          0.08538467882794278,
          0.09503155262312986,
          0.08288635613635478,
          0.08264974876794147,
          0.08211371813595908,
          0.08657470992634171,
          0.08259439090255191,
          0.08312408247804894,
          0.08297372789394555,
          0.0824331510535781,
          0.08637316625377117,
          0.10319183305913875,
          0.08404118132533886,
          0.08263370202318032,
          0.0834927133525499,
          0.08558875960099446,
          0.0825962951162357,
          0.08525988219355811,
          0.08491634346648637,
          0.08277226913673,
          0.0821336454759914,
          0.08324338368912149,
          0.08389344305054196,
          0.0848766197803248,
          0.0828420257148326,
          0.08199056700519247,
          0.09088192756881397,
          0.08291160072327894,
          0.1049394814041883,
          0.0836179953257,
          0.08265678691618213,
          0.08329057337357557,
          0.08586042141522317,
          0.0831940183020074,
          0.08226786937139183,
          0.08258516295466148,
          0.08418431503900826,
          0.08378835210536532,
          0.08655320541884155,
          0.08267759915260924,
          0.10230479617375414,
          0.08253292069688178,
          0.08360373450105948,
          0.08306322233093005,
          0.08297395306054933,
          0.08502468501565282,
          0.08294150522200303,
          0.08291127575946386,
          0.0845033398062486,
          0.08229269394206133,
          0.08319980298285114,
          0.08355104320136941,
          0.08214051827823698,
          0.08368642417592195,
          0.10056914324523544,
          0.08742187178093777,
          0.10750592060617267,
          0.08342305906642027,
          0.08514749289202753,
          0.08306091591721386,
          0.08237992365506012,
          0.08504635372532161,
          0.08217331987854679,
          0.08579425456448286,
          0.08467105207276104,
          0.08242706667986897,
          0.08495171802357053,
          0.08436796376240073,
          0.08324510960531548,
          0.08348012821167246,
          0.08222864482506259,
          0.08360830242929047,
          0.08218972123363966,
          0.0846592336480432,
          0.08296534942665079,
          0.08385594691626566,
          0.08428881746489056,
          0.08514809023965,
          0.08517640762635152,
          0.08374766448669568,
          0.08346478111354153,
          0.08345120363017818,
          0.08341290855897499,
          0.08363018486181899,
          0.08310301346123908,
          0.09272379644340216,
          0.08378574735051028,
          0.0852733325696883,
          0.08216595188325272,
          0.10378265171249632,
          0.08294003021589308,
          0.08326020405580013,
          0.08228238847757531,
          0.08428079796872202,
          0.08311972393512462,
          0.08338968730295256,
          0.08293213450970785,
          0.09393946362590899,
          0.08327466350181753,
          0.08506553342131629,
          0.0826984353792733,
          0.08311464356185652,
          0.08507777430539948,
          0.10664077240786737,
          0.08308505363475249,
          0.0840521759461056,
          0.0984829737053229,
          0.08409671431676492,
          0.08343552651457424,
          0.08218376867519202,
          0.0826176725634861,
          0.087404024246948,
          0.0824579868517901,
          0.08343538851576185,
          0.08632526446250707,
          0.10113468346246568,
          0.08408297959141789,
          0.08455967169326965,
          0.08290135112828734,
          0.08622599585167161,
          0.08256680193136179,
          0.08279304994854082,
          0.08298208106969547,
          0.086487997927399,
          0.08451040655479528,
          0.08312387237224324,
          0.08279913946855727,
          0.08525800338496867,
          0.08306589903344415,
          0.08263009693292171,
          0.08444546632016169,
          0.08282584835510597,
          0.08275273275273365,
          0.08324783184800626,
          0.08293670250894221,
          0.08291566242729534,
          0.08471320065483626,
          0.08267044541148255,
          0.08447697754338505,
          0.08390700147066862,
          0.08333214843043357,
          0.0828179712937066,
          0.08299945475657355,
          0.08281322887122904,
          0.08395017481762637,
          0.08241984428417812,
          0.08213717563466298,
          0.10076673879446381,
          0.0832436787503004,
          0.08252665324010452,
          0.08320587493050441,
          0.08776937049652797,
          0.08227879636452218,
          0.08310102230104396,
          0.08312565513737012,
          0.08321513334889388,
          0.08299056674606672,
          0.08303414146151361,
          0.083011182298674,
          0.08365498360084109,
          0.08588668701466871,
          0.08384000289415564,
          0.0833124264339247,
          0.08238305561835071,
          0.09203839346356635,
          0.08349258535191846,
          0.08215443768884524,
          0.08349214873884861,
          0.08403318361265764,
          0.0824595433892342,
          0.08413298882527163,
          0.08435908750491991,
          0.08767755459365653,
          0.0827298801062617,
          0.09582911899764356,
          0.08268836609716766,
          0.08217887190324438,
          0.08456203431506822,
          0.08395906740770928,
          0.08721133673682199,
          0.08326946965557938,
          0.08277857338975389,
          0.08516307424345448,
          0.08874462697002515,
          0.08388595508820527,
          0.10162837463997096,
          0.08349279022803455,
          0.08273657163673434,
          0.08376690305749647,
          0.08253757560419685,
          0.10402306282405352,
          0.08277068997162221,
          0.08764543348113721,
          0.08260209306314711,
          0.08246848587072822,
          0.08308283039199962,
          0.08247267673486194,
          0.09189560105049469,
          0.08462355983006513,
          0.08329194609971924,
          0.08316037595639489,
          0.08317869616338323,
          0.08316144874053474,
          0.08959409565848214,
          0.08402716720523631,
          0.08400077230926636,
          0.0829619142270787,
          0.08283674928845328,
          0.0847778461277469,
          0.08344920849250995,
          0.08523696176791473,
          0.08725620307003272,
          0.10397492307026904,
          0.08595226907323969,
          0.09921985101954633,
          0.09533855013187104,
          0.08309507191276091,
          0.0826759786006139,
          0.08225593586524416,
          0.08740274630438884,
          0.08315660205916471,
          0.08608837841090153,
          0.08467202290989553,
          0.097137117730765,
          0.08275036333514421,
          0.08386434363894248,
          0.08188325999628746,
          0.08557863261436732,
          0.08446231710143276,
          0.08368820469053169,
          0.08285472367859309,
          0.08303516672857214,
          0.0833906914083489,
          0.08366981604698515,
          0.08793276147551224,
          0.08952814150800537,
          0.08308096878529299,
          0.08256381892015496,
          0.08317662056064302,
          0.08710183821325071,
          0.0834428219383049,
          0.0830258565559016,
          0.08355652692921657,
          0.08424998322924931,
          0.08334819052906109,
          0.08573110756005994,
          0.08488722805270349,
          0.08380901110932767,
          0.09618439324678967,
          0.0826032479888251,
          0.0820830552828102,
          0.0862818168369496,
          0.08401134731429585,
          0.08295871971481268,
          0.08240986881096399,
          0.0824353229340784,
          0.08462405213082154,
          0.08262857330685547,
          0.09748615573746822,
          0.08613884514448175,
          0.08814720651422511,
          0.0824457607071543,
          0.08331839738519489,
          0.0821804083798195,
          0.08350042182975628,
          0.08311370306065799,
          0.08886475197480367,
          0.08374134130419003,
          0.08466248680311633,
          0.083197319026209,
          0.0870576233729369,
          0.10433184556727466,
          0.08395723755226109,
          0.08220024890163888,
          0.08265994797689089,
          0.10548729044772294,
          0.08270016460923824,
          0.08263693091573175,
          0.08408285162449515,
          0.08247965927491127,
          0.08510520859519778,
          0.08393599427281197,
          0.08298244561042319,
          0.08562942857561037,
          0.08615259273943957,
          0.08253164483660641,
          0.0834048056413013,
          0.0836444464588196,
          0.08644759051217846,
          0.10161932762356388,
          0.0860720895428612,
          0.08284569478174883,
          0.08371538532284166,
          0.08264559635056794,
          0.08284416313727293,
          0.08391010104815012,
          0.08260172216704732,
          0.08581525248878688,
          0.08305984930644876,
          0.08274444052530283,
          0.08488695733354572,
          0.08243008456223479,
          0.0829948282257483,
          0.08495372244600471,
          0.08321977121587064,
          0.08635692506489748,
          0.08264673692125761,
          0.08186846040844238,
          0.08263059881485445,
          0.10441472501698376,
          0.10528485609977607,
          0.08499971771788298,
          0.09259959919221114,
          0.08261318952776894,
          0.08385959899234947,
          0.0826226315654416,
          0.08258107343944868,
          0.08331235626742295,
          0.0834970814550906,
          0.08322184354859442,
          0.08366762990690224,
          0.08232107545407749,
          0.08502953198831671,
          0.08469003944748557,
          0.08222412784889119,
          0.08312009935117741,
          0.08571363180020228,
          0.08237007825013624,
          0.0835774816385849,
          0.08347237759742218,
          0.08308469365588271,
          0.08238220838082617,
          0.08422243198098708,
          0.08466688799410906,
          0.08329245959958595,
          0.0831207423118143,
          0.08315877480983573,
          0.0852580342864113,
          0.08307430873764746,
          0.08793018395955414,
          0.0830730472577487,
          0.08393387458472819,
          0.08540937207454335,
          0.08348777746597265,
          0.08265909677459007,
          0.08387453848047574,
          0.08635036092591931,
          0.0829349762863024,
          0.08241289957264089,
          0.10413855843091069,
          0.08253266605259721,
          0.08648364062869808,
          0.08191460734977042,
          0.08310340315683169,
          0.0829548528835559,
          0.0864318137649662,
          0.0829293169593892,
          0.10305693039776236,
          0.08356895669343908,
          0.08244647842569405,
          0.08415434846452788,
          0.0830010827587181,
          0.08438485467873562,
          0.08223489665962366,
          0.0827530514167065,
          0.08357849234207144,
          0.08273626391953012,
          0.08404602275072952,
          0.08231460044689605,
          0.08210942459292067,
          0.08321989252062911,
          0.08319843094976215,
          0.08535502837758135,
          0.083684048231146,
          0.0829440056016863,
          0.08306201265224991,
          0.08321963632160116,
          0.08523513451119122,
          0.08957199257037188,
          0.10218071104046678,
          0.08365996670182504,
          0.08288414315622594,
          0.0831609409532332,
          0.08563748131796066,
          0.08267228190331319,
          0.08351072308999888,
          0.08595073621997369,
          0.0830822013310413,
          0.09276760202112158,
          0.08320761930510705,
          0.08320375706140715,
          0.087980450153498,
          0.08257120222868822,
          0.08382900630949686,
          0.08347497757570743,
          0.09168496044283138,
          0.08287356761286935,
          0.09610522792691246,
          0.0840937911692142,
          0.08320281863107817,
          0.0840821741688485,
          0.08296261071055012,
          0.08702358037168506,
          0.08304670690396758,
          0.08282960813412675,
          0.08504832917070236,
          0.08280821131692603,
          0.08357559868071203,
          0.10030268846476795,
          0.0830115284099357,
          0.08538684115689459,
          0.08316314519172648,
          0.08674664611182713,
          0.0833060353078631,
          0.08276759290831337,
          0.082636621675815,
          0.08519768869200897,
          0.08343585614273649,
          0.08348041210697296,
          0.08479695608461686,
          0.10471228685959055,
          0.08411816094404848,
          0.08253897607087386,
          0.08282121749834048,
          0.08301176176181067,
          0.08422872819064456,
          0.08248307926521628,
          0.0873169601423071,
          0.08403867263588274,
          0.08360671456693249,
          0.0873387381930901,
          0.08303636122381383,
          0.08429013910580663,
          0.08245037943250491,
          0.08731979299476997,
          0.0831649244330491,
          0.08299715585650622,
          0.08383338746534745,
          0.08300374574868034,
          0.08221742281473433,
          0.08328189460011386,
          0.08380170444242878,
          0.08724155313411035,
          0.08266658783360512,
          0.0827461062704319,
          0.08366220360416872,
          0.08926097460933158,
          0.0820128908958929,
          0.08239676192015702,
          0.08335814330410822,
          0.08271122899355499,
          0.09320347156581618,
          0.08244578693077234,
          0.08455687464846962,
          0.0827144043953543,
          0.09735407733563475,
          0.08311742873296009,
          0.08253193283787222,
          0.08503616705876,
          0.08229462409623334,
          0.10227328262969504,
          0.08244098270982371,
          0.08458546705476075,
          0.08245023856627462,
          0.08390096654345602,
          0.08261727710396506,
          0.08296341485333819,
          0.0843856743061751,
          0.0826403264007882,
          0.08236202033659863,
          0.08491342896501539,
          0.08249553332142545,
          0.08248501382813546,
          0.08325999242968649,
          0.08317749236739873,
          0.08683336062238954,
          0.0816765138371928,
          0.08259875051057868,
          0.08402034285954971,
          0.08252324178034955,
          0.09427206101801941,
          0.08361224594033768,
          0.08780454576834186,
          0.08306918463313627,
          0.08294936107813836,
          0.08226373331746348,
          0.08270158075217962,
          0.09324708168090719,
          0.08375970454230489,
          0.08278135784160373,
          0.08603363354510356,
          0.1045542831789172,
          0.0826534965564436,
          0.08254708094217993,
          0.08678866035047919,
          0.08257299826086026,
          0.10524439261200529,
          0.08462142450028,
          0.09693374076406427,
          0.10746782345354618,
          0.0865524613434759,
          0.08328747282232361,
          0.08874189978266406,
          0.08327021589551106,
          0.09380548783775086,
          0.08239314017106542,
          0.08329407297801958,
          0.08299425059418443,
          0.08576349535115597,
          0.0833380400574528,
          0.08329158468354203,
          0.09177749989897299,
          0.08283706581670945,
          0.08460100921379025,
          0.08298000691791643,
          0.0826500350927559,
          0.08338863250547998,
          0.08284291699085977,
          0.08353928226982023,
          0.0828191079128297,
          0.08425831915679907,
          0.08394784695156927,
          0.1048112802167831,
          0.08349310994313272,
          0.08527538963067192,
          0.08234989201226825,
          0.08602489920911367,
          0.08372643663163837,
          0.08333347635755216,
          0.0831372896782915,
          0.08252173787708882,
          0.0830012992386513,
          0.08352099809410667,
          0.08484631339069673,
          0.08376725637774293,
          0.08248257995669252,
          0.10291475756183441,
          0.08266324199933343,
          0.08417132121165083,
          0.08288839769821313,
          0.08226414886960809,
          0.10099639770365246,
          0.08272441754739207,
          0.08387966155676363,
          0.09299240361592506,
          0.08444045941099101,
          0.0834123863076146,
          0.08412272761003801,
          0.08446355118849863,
          0.08334290321166601,
          0.08245797628427717,
          0.09368903580642896,
          0.08579221761627058,
          0.0833885768570585,
          0.08239055214740282,
          0.08249961918811777,
          0.08306052813844458,
          0.0831357568439862,
          0.08409036395262161,
          0.08337417185758891,
          0.0829387506742927,
          0.08260896282855443,
          0.08498237910796849,
          0.08300392716098079,
          0.08195714197156158,
          0.08418743298567412,
          0.10038129945318305,
          0.08327494977087521,
          0.08373973990748947,
          0.08308164190378595,
          0.09100211356259427,
          0.08563965910037585,
          0.08583124169737358,
          0.08392212825027276,
          0.08279077028629671,
          0.08236456924978328,
          0.10629128661382152,
          0.08250091564787407,
          0.08713988911407138,
          0.08433474922626143,
          0.0826654757872344,
          0.08395989014513086,
          0.08574106211056873,
          0.08285930018085841,
          0.08299692922839406,
          0.08441498679660339,
          0.08389846280538288,
          0.0848405257616334,
          0.0834616221668075,
          0.08584030665022686,
          0.08322891696303439,
          0.08353011074735918,
          0.08309285294850441,
          0.08651283933196181,
          0.08310693915881492,
          0.08290063321512871,
          0.08320239035774854,
          0.08601050254872308,
          0.08393616543478491,
          0.08260553336789768,
          0.085613102481566,
          0.0825841065356284,
          0.08324735921670097,
          0.08332004573388377,
          0.0931690848266701,
          0.0859662930507487,
          0.08373161134596022,
          0.08301017264797066,
          0.08336194886702872,
          0.08255570842720951,
          0.08764987998468635,
          0.08336795546281142,
          0.08398129766152383,
          0.0848594980991578,
          0.08266155455990679,
          0.08247719720819359,
          0.08374440729884888,
          0.08310656364763773,
          0.10226158936330351,
          0.09595609196563701,
          0.08276011433173194,
          0.08717349665672955,
          0.08246932495623632,
          0.08319389051401872,
          0.0824771668600773,
          0.08573726649146093,
          0.08725387877287781,
          0.08304789224788486,
          0.08434879957739697,
          0.08363829132457859,
          0.10460715119245341,
          0.08220427027112753,
          0.08271754200267924,
          0.08689833135454574,
          0.0834464881164965,
          0.08562707073378686,
          0.08321486883375731,
          0.08404642113923066,
          0.08519422596037746,
          0.08574035101850674,
          0.08260114456740034,
          0.08226617978994574,
          0.09218102338526904,
          0.08182862624074039,
          0.08255383901034316,
          0.08343164795439631,
          0.09953847346653258,
          0.08232191407943026,
          0.0817069837129533,
          0.0867796506853938,
          0.08460698583192258,
          0.0817647500281331,
          0.08327299992035239,
          0.08248162883909414,
          0.0840224012952457,
          0.08418530669104944,
          0.087973293685031,
          0.08268698123591149,
          0.08437574712624792,
          0.08327755337916884,
          0.08214275629720465,
          0.08414232814750208,
          0.08329582745550315,
          0.08720531957157558,
          0.08479025950591201,
          0.08275803518911087,
          0.08291628786089525,
          0.08393236306890406,
          0.10079997009926882,
          0.08716367263376307,
          0.08492904714119187,
          0.08386324725465437,
          0.08271281080959585,
          0.08308604866601085,
          0.0834212191163988,
          0.08595553137476505,
          0.08234046626972627,
          0.08227295477044481,
          0.08406219882785552,
          0.08224580632195619,
          0.08345248633800766,
          0.08819583941980899,
          0.08470703788496611,
          0.08301319642691471,
          0.08769757652837515,
          0.0843761094860751,
          0.08335234029658274,
          0.08313066301315937,
          0.0833117535252543,
          0.08264534952309555,
          0.08355628770407349,
          0.08442672436586694,
          0.08236143286177203,
          0.083264741051013,
          0.08688689989921365,
          0.08449092200401603,
          0.08576533320175621,
          0.08308582311144246,
          0.09109043997091111,
          0.08254199390917345,
          0.0821983819056997,
          0.08240965082024795,
          0.08636383225110883,
          0.10300281038002383,
          0.08214853769173854,
          0.08268008983563277,
          0.08370669105344264,
          0.08268520080035745,
          0.08279714856339851,
          0.08713031875136815,
          0.08250872961798147,
          0.08243085229276693,
          0.08238836573806105,
          0.0825027710727266,
          0.08298343614744504,
          0.08356331086152853,
          0.09050036013864263,
          0.08511173574036432,
          0.08341313343305887,
          0.08258826362434266,
          0.0829828554386322,
          0.08276858925895582,
          0.08433108819297482,
          0.08237576200573207
         ],
         "yaxis": "y"
        },
        {
         "marker": {
          "color": [
           0,
           1,
           2,
           3,
           4,
           5,
           6,
           7,
           8,
           9,
           10,
           11,
           12,
           13,
           14,
           15,
           16,
           17,
           18,
           19,
           20,
           21,
           22,
           23,
           24,
           25,
           26,
           27,
           28,
           29,
           30,
           31,
           32,
           33,
           34,
           35,
           36,
           37,
           38,
           39,
           40,
           41,
           42,
           43,
           44,
           45,
           46,
           47,
           48,
           49,
           50,
           51,
           52,
           53,
           54,
           55,
           56,
           57,
           58,
           59,
           60,
           61,
           62,
           63,
           64,
           65,
           66,
           67,
           68,
           69,
           70,
           71,
           72,
           73,
           74,
           75,
           76,
           77,
           78,
           79,
           80,
           81,
           82,
           83,
           84,
           85,
           86,
           87,
           88,
           89,
           90,
           91,
           92,
           93,
           94,
           95,
           96,
           97,
           98,
           99,
           100,
           101,
           102,
           103,
           104,
           105,
           106,
           107,
           108,
           109,
           110,
           111,
           112,
           113,
           114,
           115,
           116,
           117,
           118,
           119,
           120,
           121,
           122,
           123,
           124,
           125,
           126,
           127,
           128,
           129,
           130,
           131,
           132,
           133,
           134,
           135,
           136,
           137,
           138,
           139,
           140,
           141,
           142,
           143,
           144,
           145,
           146,
           147,
           148,
           149,
           150,
           151,
           152,
           153,
           154,
           155,
           156,
           157,
           158,
           159,
           160,
           161,
           162,
           163,
           164,
           165,
           166,
           167,
           168,
           169,
           170,
           171,
           172,
           173,
           174,
           175,
           176,
           177,
           178,
           179,
           180,
           181,
           182,
           183,
           184,
           185,
           186,
           187,
           188,
           189,
           190,
           191,
           192,
           193,
           194,
           195,
           196,
           197,
           198,
           199,
           200,
           201,
           202,
           203,
           204,
           205,
           206,
           207,
           208,
           209,
           210,
           211,
           212,
           213,
           214,
           215,
           216,
           217,
           218,
           219,
           220,
           221,
           222,
           223,
           224,
           225,
           226,
           227,
           228,
           229,
           230,
           231,
           232,
           233,
           234,
           235,
           236,
           237,
           238,
           239,
           240,
           241,
           242,
           243,
           244,
           245,
           246,
           247,
           248,
           249,
           250,
           251,
           252,
           253,
           254,
           255,
           256,
           257,
           258,
           259,
           260,
           261,
           262,
           263,
           264,
           265,
           266,
           267,
           268,
           269,
           270,
           271,
           272,
           273,
           274,
           275,
           276,
           277,
           278,
           279,
           280,
           281,
           282,
           283,
           284,
           285,
           286,
           287,
           288,
           289,
           290,
           291,
           292,
           293,
           294,
           295,
           296,
           297,
           298,
           299,
           300,
           301,
           302,
           303,
           304,
           305,
           306,
           307,
           308,
           309,
           310,
           311,
           312,
           313,
           314,
           315,
           316,
           317,
           318,
           319,
           320,
           321,
           322,
           323,
           324,
           325,
           326,
           327,
           328,
           329,
           330,
           331,
           332,
           333,
           334,
           335,
           336,
           337,
           338,
           339,
           340,
           341,
           342,
           343,
           344,
           345,
           346,
           347,
           348,
           349,
           350,
           351,
           352,
           353,
           354,
           355,
           356,
           357,
           358,
           359,
           360,
           361,
           362,
           363,
           364,
           365,
           366,
           367,
           368,
           369,
           370,
           371,
           372,
           373,
           374,
           375,
           376,
           377,
           378,
           379,
           380,
           381,
           382,
           383,
           384,
           385,
           386,
           387,
           388,
           389,
           390,
           391,
           392,
           393,
           394,
           395,
           396,
           397,
           398,
           399,
           400,
           401,
           402,
           403,
           404,
           405,
           406,
           407,
           408,
           409,
           410,
           411,
           412,
           413,
           414,
           415,
           416,
           417,
           418,
           419,
           420,
           421,
           422,
           423,
           424,
           425,
           426,
           427,
           428,
           429,
           430,
           431,
           432,
           433,
           434,
           435,
           436,
           437,
           438,
           439,
           440,
           441,
           442,
           443,
           444,
           445,
           446,
           447,
           448,
           449,
           450,
           451,
           452,
           453,
           454,
           455,
           456,
           457,
           458,
           459,
           460,
           461,
           462,
           463,
           464,
           465,
           466,
           467,
           468,
           469,
           470,
           471,
           472,
           473,
           474,
           475,
           476,
           477,
           478,
           479,
           480,
           481,
           482,
           483,
           484,
           485,
           486,
           487,
           488,
           489,
           490,
           491,
           492,
           493,
           494,
           495,
           496,
           497,
           498,
           499,
           500,
           501,
           502,
           503,
           504,
           505,
           506,
           507,
           508,
           509,
           510,
           511,
           512,
           513,
           514,
           515,
           516,
           517,
           518,
           519,
           520,
           521,
           522,
           523,
           524,
           525,
           526,
           527,
           528,
           529,
           530,
           531,
           532,
           533,
           534,
           535,
           536,
           537,
           538,
           539,
           540,
           541,
           542,
           543,
           544,
           545,
           546,
           547,
           548,
           549,
           550,
           551,
           552,
           553,
           554,
           555,
           556,
           557,
           558,
           559,
           560,
           561,
           562,
           563,
           564,
           565,
           566,
           567,
           568,
           569,
           570,
           571,
           572,
           573,
           574,
           575,
           576,
           577,
           578,
           579,
           580,
           581,
           582,
           583,
           584,
           585,
           586,
           587,
           588,
           589,
           590,
           591,
           592,
           593,
           594,
           595,
           596,
           597,
           598,
           599,
           600,
           601,
           602,
           603,
           604,
           605,
           606,
           607,
           608,
           609,
           610,
           611,
           612,
           613,
           614,
           615,
           616,
           617,
           618,
           619,
           620,
           621,
           622,
           623,
           624,
           625,
           626,
           627,
           628,
           629,
           630,
           631,
           632,
           633,
           634,
           635,
           636,
           637,
           638,
           639,
           640,
           641,
           642,
           643,
           644,
           645,
           646,
           647,
           648,
           649,
           650,
           651,
           652,
           653,
           654,
           655,
           656,
           657,
           658,
           659,
           660,
           661,
           662,
           663,
           664,
           665,
           666,
           667,
           668,
           669,
           670,
           671,
           672,
           673,
           674,
           675,
           676,
           677,
           678,
           679,
           680,
           681,
           682,
           683,
           684,
           685,
           686,
           687,
           688,
           689,
           690,
           691,
           692,
           693,
           694,
           695,
           696,
           697,
           698,
           699,
           700,
           701,
           702,
           703,
           704,
           705,
           706,
           707,
           708,
           709,
           710,
           711,
           712,
           713,
           714,
           715,
           716,
           717,
           718,
           719,
           720,
           721,
           722,
           723,
           724,
           725,
           726,
           727,
           728,
           729,
           730,
           731,
           732,
           733,
           734,
           735,
           736,
           737,
           738,
           739,
           740,
           741,
           742,
           743,
           744,
           745,
           746,
           747,
           748,
           749,
           750,
           751,
           752,
           753,
           754,
           755,
           756,
           757,
           758,
           759,
           760,
           761,
           762,
           763,
           764,
           765,
           766,
           767,
           768,
           769,
           770,
           771,
           772,
           773,
           774,
           775,
           776,
           777,
           778,
           779,
           780,
           781,
           782,
           783,
           784,
           785,
           786,
           787,
           788,
           789,
           790,
           791,
           792,
           793,
           794,
           795,
           796,
           797,
           798,
           799,
           800,
           801,
           802,
           803,
           804,
           805,
           806,
           807,
           808,
           809,
           810,
           811,
           812,
           813,
           814,
           815,
           816,
           817,
           818,
           819,
           820,
           821,
           822,
           823,
           824,
           825,
           826,
           827,
           828,
           829,
           830,
           831,
           832,
           833,
           834,
           835,
           836,
           837,
           838,
           839,
           840,
           841,
           842,
           843,
           844,
           845,
           846,
           847,
           848,
           849,
           850,
           851,
           852,
           853,
           854,
           855,
           856,
           857,
           858,
           859,
           860,
           861,
           862,
           863,
           864,
           865,
           866,
           867,
           868,
           869,
           870,
           871,
           872,
           873,
           874,
           875,
           876,
           877,
           878,
           879,
           880,
           881,
           882,
           883,
           884,
           885,
           886,
           887,
           888,
           889,
           890,
           891,
           892,
           893,
           894,
           895,
           896,
           897,
           898,
           899,
           900,
           901,
           902,
           903,
           904,
           905,
           906,
           907,
           908,
           909,
           910,
           911,
           912,
           913,
           914,
           915,
           916,
           917,
           918,
           919,
           920,
           921,
           922,
           923,
           924,
           925,
           926,
           927,
           928,
           929,
           930,
           931,
           932,
           933,
           934,
           935,
           936,
           937,
           938,
           939,
           940,
           941,
           942,
           943,
           944,
           945,
           946,
           947,
           948,
           949,
           950,
           951,
           952,
           953,
           954,
           955,
           956,
           957,
           958,
           959,
           960,
           961,
           962,
           963,
           964,
           965,
           966,
           967,
           968,
           969,
           970,
           971,
           972,
           973,
           974,
           975,
           976,
           977,
           978,
           979,
           980,
           981,
           982,
           983,
           984,
           985,
           986,
           987,
           988,
           989,
           990,
           991,
           992,
           993,
           994,
           995,
           996,
           997,
           998,
           999
          ],
          "colorbar": {
           "title": {
            "text": "Trial"
           },
           "x": 1,
           "xpad": 40
          },
          "colorscale": [
           [
            0,
            "rgb(247,251,255)"
           ],
           [
            0.125,
            "rgb(222,235,247)"
           ],
           [
            0.25,
            "rgb(198,219,239)"
           ],
           [
            0.375,
            "rgb(158,202,225)"
           ],
           [
            0.5,
            "rgb(107,174,214)"
           ],
           [
            0.625,
            "rgb(66,146,198)"
           ],
           [
            0.75,
            "rgb(33,113,181)"
           ],
           [
            0.875,
            "rgb(8,81,156)"
           ],
           [
            1,
            "rgb(8,48,107)"
           ]
          ],
          "line": {
           "color": "Grey",
           "width": 0.5
          },
          "showscale": false
         },
         "mode": "markers",
         "name": "Feasible Trial",
         "showlegend": false,
         "type": "scatter",
         "x": [
          4,
          12,
          6,
          12,
          4,
          3,
          6,
          3,
          15,
          8,
          10,
          3,
          6,
          3,
          8,
          5,
          3,
          8,
          10,
          5,
          5,
          4,
          5,
          7,
          5,
          7,
          5,
          4,
          7,
          4,
          4,
          4,
          4,
          6,
          4,
          14,
          4,
          11,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          6,
          13,
          3,
          6,
          4,
          3,
          3,
          3,
          4,
          5,
          3,
          5,
          3,
          4,
          5,
          3,
          3,
          3,
          4,
          4,
          9,
          4,
          3,
          4,
          3,
          3,
          3,
          5,
          4,
          3,
          4,
          3,
          4,
          5,
          3,
          4,
          3,
          4,
          4,
          15,
          3,
          5,
          11,
          3,
          6,
          4,
          4,
          4,
          3,
          5,
          3,
          4,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          9,
          3,
          3,
          3,
          4,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          8,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          14,
          4,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          4,
          3,
          3,
          10,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          7,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          12,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          4,
          8,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          7,
          10,
          3,
          3,
          3,
          4,
          3,
          3,
          4,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          6,
          3,
          13,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          11,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          9,
          4,
          15,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          12,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          7,
          3,
          4,
          3,
          3,
          3,
          14,
          3,
          4,
          8,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          9,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          4,
          3,
          3,
          3,
          6,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          4,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          5,
          3,
          10,
          3,
          3,
          3,
          3,
          11,
          3,
          4,
          3,
          3,
          3,
          3,
          9,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          4,
          3,
          3,
          3,
          12,
          4,
          8,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          5,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          7,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          6,
          4,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          13,
          3,
          3,
          3,
          15,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          4,
          10,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          14,
          13,
          3,
          6,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          11,
          3,
          4,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          4,
          3,
          10,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          7,
          3,
          3,
          5,
          3,
          4,
          3,
          3,
          3,
          8,
          3,
          3,
          4,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          9,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          12,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          4,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          7,
          3,
          4,
          3,
          3,
          3,
          3,
          4,
          3,
          11,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          8,
          3,
          3,
          4,
          14,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          5,
          3,
          4,
          3,
          6,
          3,
          3,
          3,
          4,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          15,
          3,
          4,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          4,
          3,
          3,
          3,
          9,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          13,
          3,
          4,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          5,
          4,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          10,
          7,
          3,
          4,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          12,
          3,
          3,
          4,
          3,
          4,
          3,
          3,
          3,
          4,
          3,
          3,
          6,
          3,
          3,
          3,
          8,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          9,
          4,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          4,
          11,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          5,
          4,
          3,
          3,
          3,
          3,
          4,
          3
         ],
         "xaxis": "x2",
         "y": [
          0.08539019019252586,
          0.09495197683593737,
          0.08561203246811563,
          0.10226171316013058,
          0.09436835513192324,
          0.08442866030728638,
          0.0882806271297641,
          0.09259418891274053,
          0.0936506610503796,
          0.08905616282810194,
          0.10070957535224619,
          0.0849354185113468,
          0.09538829459268595,
          0.08532982608602123,
          0.09802117955542877,
          0.08861113027563677,
          0.08782249655960989,
          0.09671218874633744,
          0.10056915412131459,
          0.08463494265724372,
          0.08550715408794303,
          0.08624021489163447,
          0.08421050029548485,
          0.0894877666245822,
          0.08522084111774414,
          0.08664087579640814,
          0.09277247714187424,
          0.08890925915171902,
          0.08679846383946271,
          0.08434924485492348,
          0.08332985614930138,
          0.08381487607339143,
          0.08381121898840153,
          0.08492498104511295,
          0.08351886332682175,
          0.1065370630575128,
          0.08407738487249838,
          0.10418987847943002,
          0.08315042884967586,
          0.08360267337033281,
          0.08344470964379247,
          0.08381392901444998,
          0.0857432511381672,
          0.08371432932413264,
          0.08615883393220121,
          0.08428574598953223,
          0.0897804337144529,
          0.10368442923134472,
          0.08419051036454686,
          0.08666228717253156,
          0.08539823364234111,
          0.08398322451750445,
          0.08324196548934296,
          0.08498755749166624,
          0.08592689223502362,
          0.08643366899645041,
          0.0835047463204986,
          0.09288435334111321,
          0.08424549859482734,
          0.08502893317695907,
          0.0890072310285759,
          0.08354751438906698,
          0.08481137014593011,
          0.08285306771082904,
          0.08435363262967716,
          0.08440017469452545,
          0.10234792567968147,
          0.08439105115445075,
          0.08367204082049616,
          0.08674702124077736,
          0.08364722764310903,
          0.08339834509471708,
          0.08458083145212651,
          0.08730137187390588,
          0.08347106168854283,
          0.10497466632612777,
          0.09500932690462209,
          0.08627061663570071,
          0.08361465461377242,
          0.0862346237864177,
          0.08576719136060315,
          0.08392691309042451,
          0.08667835916016219,
          0.0832016218464397,
          0.08360618571881825,
          0.10570571526403402,
          0.08912974625484643,
          0.08794829675893581,
          0.10369587843510582,
          0.08450059575739881,
          0.08564046243395204,
          0.08425998493872724,
          0.08328057039506313,
          0.08414219287159663,
          0.08939263864603345,
          0.0846428294515632,
          0.08165961576392188,
          0.08488512104472777,
          0.0864821733201315,
          0.0844953596275288,
          0.08312414300199703,
          0.08302348043185244,
          0.08349016143148591,
          0.08273551529377218,
          0.08712664607608565,
          0.08335508002367598,
          0.0833304450341904,
          0.08274977982394807,
          0.08723091108735959,
          0.10129893784474617,
          0.08328120589468982,
          0.08518771642746378,
          0.08417473164618214,
          0.08544436252688223,
          0.084259756018591,
          0.08477727688870212,
          0.0843732621596494,
          0.08238179744888834,
          0.0835201737237383,
          0.08440188316439623,
          0.0826963136261387,
          0.08325320855427412,
          0.08232280947837918,
          0.08313899416292488,
          0.0827606041580992,
          0.08345781219298537,
          0.082856239988547,
          0.08315158199964165,
          0.09996173825297669,
          0.08287132257374849,
          0.08399392866757854,
          0.08329801970738117,
          0.08218916379001612,
          0.08272626181044221,
          0.08356949324059257,
          0.08344809825525434,
          0.08412450105439928,
          0.08436250442465335,
          0.08543737126852116,
          0.08315415930339068,
          0.08328874654472859,
          0.08377063932211924,
          0.08374662836316327,
          0.08432312077904969,
          0.08334078110772783,
          0.10451366567669003,
          0.08396547011835669,
          0.09866471516554136,
          0.0840355188769745,
          0.08313315216040826,
          0.0833273896064651,
          0.08380846367977608,
          0.08301209213271031,
          0.08302693484535764,
          0.08513153125053821,
          0.08294988420046369,
          0.0828023224844447,
          0.08576151350462309,
          0.08313593893209155,
          0.0823890790084934,
          0.10233081625126154,
          0.08391457740581108,
          0.08346778197338994,
          0.08323672801444174,
          0.11016564627256034,
          0.08625986833194307,
          0.08308838128817571,
          0.08256675863644616,
          0.08687355439209934,
          0.08320375301071112,
          0.0831569856636962,
          0.0830861335286607,
          0.08373209848835807,
          0.08325772728529475,
          0.0840249429842295,
          0.08340173491254706,
          0.0841523490888042,
          0.08278875829740572,
          0.08588354015186887,
          0.08460117799571931,
          0.08271264948133934,
          0.08311842459534399,
          0.09775820704966276,
          0.08363667644538213,
          0.08518966582885326,
          0.08234088467720187,
          0.08300663028468404,
          0.08294792493806535,
          0.09374333088296935,
          0.08186637706985608,
          0.08286524474377754,
          0.08407084466100902,
          0.08513404749620522,
          0.08512809066960167,
          0.08317299290426386,
          0.08329165608201898,
          0.09249173931498986,
          0.08581289239318927,
          0.08311832057969164,
          0.08244091747412202,
          0.0859276097402228,
          0.08294365575003566,
          0.08291915631246957,
          0.08431553209465299,
          0.08316980756401962,
          0.08275119431371236,
          0.08259810148865966,
          0.08269710428895796,
          0.0825306928188557,
          0.08327358623423534,
          0.08337420215674948,
          0.10445665012752847,
          0.08334076815055066,
          0.08274338024429977,
          0.08251096914423196,
          0.08199902606831211,
          0.08235772891596732,
          0.08293553119582622,
          0.08253509322945916,
          0.08376735221276418,
          0.08242039156277536,
          0.08310408342648781,
          0.08236920723797173,
          0.08291592089843107,
          0.08237535912990515,
          0.08328242745865577,
          0.08330519127524712,
          0.08333766438105057,
          0.08348678243602832,
          0.08346972442740377,
          0.08291621932822964,
          0.08231996370906212,
          0.08231519235453469,
          0.08261364924345689,
          0.0825212172002789,
          0.08282661754470844,
          0.08280000274572497,
          0.0822068912312609,
          0.08306979246734976,
          0.08280936601250934,
          0.08323729894382537,
          0.0827596216791415,
          0.08254312932582486,
          0.08260706905635237,
          0.08219159376324617,
          0.08332867404127005,
          0.08245010521755164,
          0.0830015532855319,
          0.08311543440998431,
          0.08354447071482189,
          0.08459523466839948,
          0.08279362737753312,
          0.0832079959920258,
          0.08271413443324369,
          0.08228669392391635,
          0.08746037507141037,
          0.08244453094845948,
          0.08267614433778225,
          0.08361087104719155,
          0.08254682461025294,
          0.08315199608854278,
          0.0831869149479296,
          0.0831621325240794,
          0.08241511203722858,
          0.08509378854913294,
          0.09446901061125146,
          0.0829485186897416,
          0.08256420848681219,
          0.08241588768524585,
          0.0832236338954678,
          0.08285732035700681,
          0.08516721649864492,
          0.08291222919437333,
          0.08335991669751068,
          0.08242883783270984,
          0.08266547354134668,
          0.08538467882794278,
          0.09503155262312986,
          0.08288635613635478,
          0.08264974876794147,
          0.08211371813595908,
          0.08657470992634171,
          0.08259439090255191,
          0.08312408247804894,
          0.08297372789394555,
          0.0824331510535781,
          0.08637316625377117,
          0.10319183305913875,
          0.08404118132533886,
          0.08263370202318032,
          0.0834927133525499,
          0.08558875960099446,
          0.0825962951162357,
          0.08525988219355811,
          0.08491634346648637,
          0.08277226913673,
          0.0821336454759914,
          0.08324338368912149,
          0.08389344305054196,
          0.0848766197803248,
          0.0828420257148326,
          0.08199056700519247,
          0.09088192756881397,
          0.08291160072327894,
          0.1049394814041883,
          0.0836179953257,
          0.08265678691618213,
          0.08329057337357557,
          0.08586042141522317,
          0.0831940183020074,
          0.08226786937139183,
          0.08258516295466148,
          0.08418431503900826,
          0.08378835210536532,
          0.08655320541884155,
          0.08267759915260924,
          0.10230479617375414,
          0.08253292069688178,
          0.08360373450105948,
          0.08306322233093005,
          0.08297395306054933,
          0.08502468501565282,
          0.08294150522200303,
          0.08291127575946386,
          0.0845033398062486,
          0.08229269394206133,
          0.08319980298285114,
          0.08355104320136941,
          0.08214051827823698,
          0.08368642417592195,
          0.10056914324523544,
          0.08742187178093777,
          0.10750592060617267,
          0.08342305906642027,
          0.08514749289202753,
          0.08306091591721386,
          0.08237992365506012,
          0.08504635372532161,
          0.08217331987854679,
          0.08579425456448286,
          0.08467105207276104,
          0.08242706667986897,
          0.08495171802357053,
          0.08436796376240073,
          0.08324510960531548,
          0.08348012821167246,
          0.08222864482506259,
          0.08360830242929047,
          0.08218972123363966,
          0.0846592336480432,
          0.08296534942665079,
          0.08385594691626566,
          0.08428881746489056,
          0.08514809023965,
          0.08517640762635152,
          0.08374766448669568,
          0.08346478111354153,
          0.08345120363017818,
          0.08341290855897499,
          0.08363018486181899,
          0.08310301346123908,
          0.09272379644340216,
          0.08378574735051028,
          0.0852733325696883,
          0.08216595188325272,
          0.10378265171249632,
          0.08294003021589308,
          0.08326020405580013,
          0.08228238847757531,
          0.08428079796872202,
          0.08311972393512462,
          0.08338968730295256,
          0.08293213450970785,
          0.09393946362590899,
          0.08327466350181753,
          0.08506553342131629,
          0.0826984353792733,
          0.08311464356185652,
          0.08507777430539948,
          0.10664077240786737,
          0.08308505363475249,
          0.0840521759461056,
          0.0984829737053229,
          0.08409671431676492,
          0.08343552651457424,
          0.08218376867519202,
          0.0826176725634861,
          0.087404024246948,
          0.0824579868517901,
          0.08343538851576185,
          0.08632526446250707,
          0.10113468346246568,
          0.08408297959141789,
          0.08455967169326965,
          0.08290135112828734,
          0.08622599585167161,
          0.08256680193136179,
          0.08279304994854082,
          0.08298208106969547,
          0.086487997927399,
          0.08451040655479528,
          0.08312387237224324,
          0.08279913946855727,
          0.08525800338496867,
          0.08306589903344415,
          0.08263009693292171,
          0.08444546632016169,
          0.08282584835510597,
          0.08275273275273365,
          0.08324783184800626,
          0.08293670250894221,
          0.08291566242729534,
          0.08471320065483626,
          0.08267044541148255,
          0.08447697754338505,
          0.08390700147066862,
          0.08333214843043357,
          0.0828179712937066,
          0.08299945475657355,
          0.08281322887122904,
          0.08395017481762637,
          0.08241984428417812,
          0.08213717563466298,
          0.10076673879446381,
          0.0832436787503004,
          0.08252665324010452,
          0.08320587493050441,
          0.08776937049652797,
          0.08227879636452218,
          0.08310102230104396,
          0.08312565513737012,
          0.08321513334889388,
          0.08299056674606672,
          0.08303414146151361,
          0.083011182298674,
          0.08365498360084109,
          0.08588668701466871,
          0.08384000289415564,
          0.0833124264339247,
          0.08238305561835071,
          0.09203839346356635,
          0.08349258535191846,
          0.08215443768884524,
          0.08349214873884861,
          0.08403318361265764,
          0.0824595433892342,
          0.08413298882527163,
          0.08435908750491991,
          0.08767755459365653,
          0.0827298801062617,
          0.09582911899764356,
          0.08268836609716766,
          0.08217887190324438,
          0.08456203431506822,
          0.08395906740770928,
          0.08721133673682199,
          0.08326946965557938,
          0.08277857338975389,
          0.08516307424345448,
          0.08874462697002515,
          0.08388595508820527,
          0.10162837463997096,
          0.08349279022803455,
          0.08273657163673434,
          0.08376690305749647,
          0.08253757560419685,
          0.10402306282405352,
          0.08277068997162221,
          0.08764543348113721,
          0.08260209306314711,
          0.08246848587072822,
          0.08308283039199962,
          0.08247267673486194,
          0.09189560105049469,
          0.08462355983006513,
          0.08329194609971924,
          0.08316037595639489,
          0.08317869616338323,
          0.08316144874053474,
          0.08959409565848214,
          0.08402716720523631,
          0.08400077230926636,
          0.0829619142270787,
          0.08283674928845328,
          0.0847778461277469,
          0.08344920849250995,
          0.08523696176791473,
          0.08725620307003272,
          0.10397492307026904,
          0.08595226907323969,
          0.09921985101954633,
          0.09533855013187104,
          0.08309507191276091,
          0.0826759786006139,
          0.08225593586524416,
          0.08740274630438884,
          0.08315660205916471,
          0.08608837841090153,
          0.08467202290989553,
          0.097137117730765,
          0.08275036333514421,
          0.08386434363894248,
          0.08188325999628746,
          0.08557863261436732,
          0.08446231710143276,
          0.08368820469053169,
          0.08285472367859309,
          0.08303516672857214,
          0.0833906914083489,
          0.08366981604698515,
          0.08793276147551224,
          0.08952814150800537,
          0.08308096878529299,
          0.08256381892015496,
          0.08317662056064302,
          0.08710183821325071,
          0.0834428219383049,
          0.0830258565559016,
          0.08355652692921657,
          0.08424998322924931,
          0.08334819052906109,
          0.08573110756005994,
          0.08488722805270349,
          0.08380901110932767,
          0.09618439324678967,
          0.0826032479888251,
          0.0820830552828102,
          0.0862818168369496,
          0.08401134731429585,
          0.08295871971481268,
          0.08240986881096399,
          0.0824353229340784,
          0.08462405213082154,
          0.08262857330685547,
          0.09748615573746822,
          0.08613884514448175,
          0.08814720651422511,
          0.0824457607071543,
          0.08331839738519489,
          0.0821804083798195,
          0.08350042182975628,
          0.08311370306065799,
          0.08886475197480367,
          0.08374134130419003,
          0.08466248680311633,
          0.083197319026209,
          0.0870576233729369,
          0.10433184556727466,
          0.08395723755226109,
          0.08220024890163888,
          0.08265994797689089,
          0.10548729044772294,
          0.08270016460923824,
          0.08263693091573175,
          0.08408285162449515,
          0.08247965927491127,
          0.08510520859519778,
          0.08393599427281197,
          0.08298244561042319,
          0.08562942857561037,
          0.08615259273943957,
          0.08253164483660641,
          0.0834048056413013,
          0.0836444464588196,
          0.08644759051217846,
          0.10161932762356388,
          0.0860720895428612,
          0.08284569478174883,
          0.08371538532284166,
          0.08264559635056794,
          0.08284416313727293,
          0.08391010104815012,
          0.08260172216704732,
          0.08581525248878688,
          0.08305984930644876,
          0.08274444052530283,
          0.08488695733354572,
          0.08243008456223479,
          0.0829948282257483,
          0.08495372244600471,
          0.08321977121587064,
          0.08635692506489748,
          0.08264673692125761,
          0.08186846040844238,
          0.08263059881485445,
          0.10441472501698376,
          0.10528485609977607,
          0.08499971771788298,
          0.09259959919221114,
          0.08261318952776894,
          0.08385959899234947,
          0.0826226315654416,
          0.08258107343944868,
          0.08331235626742295,
          0.0834970814550906,
          0.08322184354859442,
          0.08366762990690224,
          0.08232107545407749,
          0.08502953198831671,
          0.08469003944748557,
          0.08222412784889119,
          0.08312009935117741,
          0.08571363180020228,
          0.08237007825013624,
          0.0835774816385849,
          0.08347237759742218,
          0.08308469365588271,
          0.08238220838082617,
          0.08422243198098708,
          0.08466688799410906,
          0.08329245959958595,
          0.0831207423118143,
          0.08315877480983573,
          0.0852580342864113,
          0.08307430873764746,
          0.08793018395955414,
          0.0830730472577487,
          0.08393387458472819,
          0.08540937207454335,
          0.08348777746597265,
          0.08265909677459007,
          0.08387453848047574,
          0.08635036092591931,
          0.0829349762863024,
          0.08241289957264089,
          0.10413855843091069,
          0.08253266605259721,
          0.08648364062869808,
          0.08191460734977042,
          0.08310340315683169,
          0.0829548528835559,
          0.0864318137649662,
          0.0829293169593892,
          0.10305693039776236,
          0.08356895669343908,
          0.08244647842569405,
          0.08415434846452788,
          0.0830010827587181,
          0.08438485467873562,
          0.08223489665962366,
          0.0827530514167065,
          0.08357849234207144,
          0.08273626391953012,
          0.08404602275072952,
          0.08231460044689605,
          0.08210942459292067,
          0.08321989252062911,
          0.08319843094976215,
          0.08535502837758135,
          0.083684048231146,
          0.0829440056016863,
          0.08306201265224991,
          0.08321963632160116,
          0.08523513451119122,
          0.08957199257037188,
          0.10218071104046678,
          0.08365996670182504,
          0.08288414315622594,
          0.0831609409532332,
          0.08563748131796066,
          0.08267228190331319,
          0.08351072308999888,
          0.08595073621997369,
          0.0830822013310413,
          0.09276760202112158,
          0.08320761930510705,
          0.08320375706140715,
          0.087980450153498,
          0.08257120222868822,
          0.08382900630949686,
          0.08347497757570743,
          0.09168496044283138,
          0.08287356761286935,
          0.09610522792691246,
          0.0840937911692142,
          0.08320281863107817,
          0.0840821741688485,
          0.08296261071055012,
          0.08702358037168506,
          0.08304670690396758,
          0.08282960813412675,
          0.08504832917070236,
          0.08280821131692603,
          0.08357559868071203,
          0.10030268846476795,
          0.0830115284099357,
          0.08538684115689459,
          0.08316314519172648,
          0.08674664611182713,
          0.0833060353078631,
          0.08276759290831337,
          0.082636621675815,
          0.08519768869200897,
          0.08343585614273649,
          0.08348041210697296,
          0.08479695608461686,
          0.10471228685959055,
          0.08411816094404848,
          0.08253897607087386,
          0.08282121749834048,
          0.08301176176181067,
          0.08422872819064456,
          0.08248307926521628,
          0.0873169601423071,
          0.08403867263588274,
          0.08360671456693249,
          0.0873387381930901,
          0.08303636122381383,
          0.08429013910580663,
          0.08245037943250491,
          0.08731979299476997,
          0.0831649244330491,
          0.08299715585650622,
          0.08383338746534745,
          0.08300374574868034,
          0.08221742281473433,
          0.08328189460011386,
          0.08380170444242878,
          0.08724155313411035,
          0.08266658783360512,
          0.0827461062704319,
          0.08366220360416872,
          0.08926097460933158,
          0.0820128908958929,
          0.08239676192015702,
          0.08335814330410822,
          0.08271122899355499,
          0.09320347156581618,
          0.08244578693077234,
          0.08455687464846962,
          0.0827144043953543,
          0.09735407733563475,
          0.08311742873296009,
          0.08253193283787222,
          0.08503616705876,
          0.08229462409623334,
          0.10227328262969504,
          0.08244098270982371,
          0.08458546705476075,
          0.08245023856627462,
          0.08390096654345602,
          0.08261727710396506,
          0.08296341485333819,
          0.0843856743061751,
          0.0826403264007882,
          0.08236202033659863,
          0.08491342896501539,
          0.08249553332142545,
          0.08248501382813546,
          0.08325999242968649,
          0.08317749236739873,
          0.08683336062238954,
          0.0816765138371928,
          0.08259875051057868,
          0.08402034285954971,
          0.08252324178034955,
          0.09427206101801941,
          0.08361224594033768,
          0.08780454576834186,
          0.08306918463313627,
          0.08294936107813836,
          0.08226373331746348,
          0.08270158075217962,
          0.09324708168090719,
          0.08375970454230489,
          0.08278135784160373,
          0.08603363354510356,
          0.1045542831789172,
          0.0826534965564436,
          0.08254708094217993,
          0.08678866035047919,
          0.08257299826086026,
          0.10524439261200529,
          0.08462142450028,
          0.09693374076406427,
          0.10746782345354618,
          0.0865524613434759,
          0.08328747282232361,
          0.08874189978266406,
          0.08327021589551106,
          0.09380548783775086,
          0.08239314017106542,
          0.08329407297801958,
          0.08299425059418443,
          0.08576349535115597,
          0.0833380400574528,
          0.08329158468354203,
          0.09177749989897299,
          0.08283706581670945,
          0.08460100921379025,
          0.08298000691791643,
          0.0826500350927559,
          0.08338863250547998,
          0.08284291699085977,
          0.08353928226982023,
          0.0828191079128297,
          0.08425831915679907,
          0.08394784695156927,
          0.1048112802167831,
          0.08349310994313272,
          0.08527538963067192,
          0.08234989201226825,
          0.08602489920911367,
          0.08372643663163837,
          0.08333347635755216,
          0.0831372896782915,
          0.08252173787708882,
          0.0830012992386513,
          0.08352099809410667,
          0.08484631339069673,
          0.08376725637774293,
          0.08248257995669252,
          0.10291475756183441,
          0.08266324199933343,
          0.08417132121165083,
          0.08288839769821313,
          0.08226414886960809,
          0.10099639770365246,
          0.08272441754739207,
          0.08387966155676363,
          0.09299240361592506,
          0.08444045941099101,
          0.0834123863076146,
          0.08412272761003801,
          0.08446355118849863,
          0.08334290321166601,
          0.08245797628427717,
          0.09368903580642896,
          0.08579221761627058,
          0.0833885768570585,
          0.08239055214740282,
          0.08249961918811777,
          0.08306052813844458,
          0.0831357568439862,
          0.08409036395262161,
          0.08337417185758891,
          0.0829387506742927,
          0.08260896282855443,
          0.08498237910796849,
          0.08300392716098079,
          0.08195714197156158,
          0.08418743298567412,
          0.10038129945318305,
          0.08327494977087521,
          0.08373973990748947,
          0.08308164190378595,
          0.09100211356259427,
          0.08563965910037585,
          0.08583124169737358,
          0.08392212825027276,
          0.08279077028629671,
          0.08236456924978328,
          0.10629128661382152,
          0.08250091564787407,
          0.08713988911407138,
          0.08433474922626143,
          0.0826654757872344,
          0.08395989014513086,
          0.08574106211056873,
          0.08285930018085841,
          0.08299692922839406,
          0.08441498679660339,
          0.08389846280538288,
          0.0848405257616334,
          0.0834616221668075,
          0.08584030665022686,
          0.08322891696303439,
          0.08353011074735918,
          0.08309285294850441,
          0.08651283933196181,
          0.08310693915881492,
          0.08290063321512871,
          0.08320239035774854,
          0.08601050254872308,
          0.08393616543478491,
          0.08260553336789768,
          0.085613102481566,
          0.0825841065356284,
          0.08324735921670097,
          0.08332004573388377,
          0.0931690848266701,
          0.0859662930507487,
          0.08373161134596022,
          0.08301017264797066,
          0.08336194886702872,
          0.08255570842720951,
          0.08764987998468635,
          0.08336795546281142,
          0.08398129766152383,
          0.0848594980991578,
          0.08266155455990679,
          0.08247719720819359,
          0.08374440729884888,
          0.08310656364763773,
          0.10226158936330351,
          0.09595609196563701,
          0.08276011433173194,
          0.08717349665672955,
          0.08246932495623632,
          0.08319389051401872,
          0.0824771668600773,
          0.08573726649146093,
          0.08725387877287781,
          0.08304789224788486,
          0.08434879957739697,
          0.08363829132457859,
          0.10460715119245341,
          0.08220427027112753,
          0.08271754200267924,
          0.08689833135454574,
          0.0834464881164965,
          0.08562707073378686,
          0.08321486883375731,
          0.08404642113923066,
          0.08519422596037746,
          0.08574035101850674,
          0.08260114456740034,
          0.08226617978994574,
          0.09218102338526904,
          0.08182862624074039,
          0.08255383901034316,
          0.08343164795439631,
          0.09953847346653258,
          0.08232191407943026,
          0.0817069837129533,
          0.0867796506853938,
          0.08460698583192258,
          0.0817647500281331,
          0.08327299992035239,
          0.08248162883909414,
          0.0840224012952457,
          0.08418530669104944,
          0.087973293685031,
          0.08268698123591149,
          0.08437574712624792,
          0.08327755337916884,
          0.08214275629720465,
          0.08414232814750208,
          0.08329582745550315,
          0.08720531957157558,
          0.08479025950591201,
          0.08275803518911087,
          0.08291628786089525,
          0.08393236306890406,
          0.10079997009926882,
          0.08716367263376307,
          0.08492904714119187,
          0.08386324725465437,
          0.08271281080959585,
          0.08308604866601085,
          0.0834212191163988,
          0.08595553137476505,
          0.08234046626972627,
          0.08227295477044481,
          0.08406219882785552,
          0.08224580632195619,
          0.08345248633800766,
          0.08819583941980899,
          0.08470703788496611,
          0.08301319642691471,
          0.08769757652837515,
          0.0843761094860751,
          0.08335234029658274,
          0.08313066301315937,
          0.0833117535252543,
          0.08264534952309555,
          0.08355628770407349,
          0.08442672436586694,
          0.08236143286177203,
          0.083264741051013,
          0.08688689989921365,
          0.08449092200401603,
          0.08576533320175621,
          0.08308582311144246,
          0.09109043997091111,
          0.08254199390917345,
          0.0821983819056997,
          0.08240965082024795,
          0.08636383225110883,
          0.10300281038002383,
          0.08214853769173854,
          0.08268008983563277,
          0.08370669105344264,
          0.08268520080035745,
          0.08279714856339851,
          0.08713031875136815,
          0.08250872961798147,
          0.08243085229276693,
          0.08238836573806105,
          0.0825027710727266,
          0.08298343614744504,
          0.08356331086152853,
          0.09050036013864263,
          0.08511173574036432,
          0.08341313343305887,
          0.08258826362434266,
          0.0829828554386322,
          0.08276858925895582,
          0.08433108819297482,
          0.08237576200573207
         ],
         "yaxis": "y2"
        },
        {
         "marker": {
          "color": [
           0,
           1,
           2,
           3,
           4,
           5,
           6,
           7,
           8,
           9,
           10,
           11,
           12,
           13,
           14,
           15,
           16,
           17,
           18,
           19,
           20,
           21,
           22,
           23,
           24,
           25,
           26,
           27,
           28,
           29,
           30,
           31,
           32,
           33,
           34,
           35,
           36,
           37,
           38,
           39,
           40,
           41,
           42,
           43,
           44,
           45,
           46,
           47,
           48,
           49,
           50,
           51,
           52,
           53,
           54,
           55,
           56,
           57,
           58,
           59,
           60,
           61,
           62,
           63,
           64,
           65,
           66,
           67,
           68,
           69,
           70,
           71,
           72,
           73,
           74,
           75,
           76,
           77,
           78,
           79,
           80,
           81,
           82,
           83,
           84,
           85,
           86,
           87,
           88,
           89,
           90,
           91,
           92,
           93,
           94,
           95,
           96,
           97,
           98,
           99,
           100,
           101,
           102,
           103,
           104,
           105,
           106,
           107,
           108,
           109,
           110,
           111,
           112,
           113,
           114,
           115,
           116,
           117,
           118,
           119,
           120,
           121,
           122,
           123,
           124,
           125,
           126,
           127,
           128,
           129,
           130,
           131,
           132,
           133,
           134,
           135,
           136,
           137,
           138,
           139,
           140,
           141,
           142,
           143,
           144,
           145,
           146,
           147,
           148,
           149,
           150,
           151,
           152,
           153,
           154,
           155,
           156,
           157,
           158,
           159,
           160,
           161,
           162,
           163,
           164,
           165,
           166,
           167,
           168,
           169,
           170,
           171,
           172,
           173,
           174,
           175,
           176,
           177,
           178,
           179,
           180,
           181,
           182,
           183,
           184,
           185,
           186,
           187,
           188,
           189,
           190,
           191,
           192,
           193,
           194,
           195,
           196,
           197,
           198,
           199,
           200,
           201,
           202,
           203,
           204,
           205,
           206,
           207,
           208,
           209,
           210,
           211,
           212,
           213,
           214,
           215,
           216,
           217,
           218,
           219,
           220,
           221,
           222,
           223,
           224,
           225,
           226,
           227,
           228,
           229,
           230,
           231,
           232,
           233,
           234,
           235,
           236,
           237,
           238,
           239,
           240,
           241,
           242,
           243,
           244,
           245,
           246,
           247,
           248,
           249,
           250,
           251,
           252,
           253,
           254,
           255,
           256,
           257,
           258,
           259,
           260,
           261,
           262,
           263,
           264,
           265,
           266,
           267,
           268,
           269,
           270,
           271,
           272,
           273,
           274,
           275,
           276,
           277,
           278,
           279,
           280,
           281,
           282,
           283,
           284,
           285,
           286,
           287,
           288,
           289,
           290,
           291,
           292,
           293,
           294,
           295,
           296,
           297,
           298,
           299,
           300,
           301,
           302,
           303,
           304,
           305,
           306,
           307,
           308,
           309,
           310,
           311,
           312,
           313,
           314,
           315,
           316,
           317,
           318,
           319,
           320,
           321,
           322,
           323,
           324,
           325,
           326,
           327,
           328,
           329,
           330,
           331,
           332,
           333,
           334,
           335,
           336,
           337,
           338,
           339,
           340,
           341,
           342,
           343,
           344,
           345,
           346,
           347,
           348,
           349,
           350,
           351,
           352,
           353,
           354,
           355,
           356,
           357,
           358,
           359,
           360,
           361,
           362,
           363,
           364,
           365,
           366,
           367,
           368,
           369,
           370,
           371,
           372,
           373,
           374,
           375,
           376,
           377,
           378,
           379,
           380,
           381,
           382,
           383,
           384,
           385,
           386,
           387,
           388,
           389,
           390,
           391,
           392,
           393,
           394,
           395,
           396,
           397,
           398,
           399,
           400,
           401,
           402,
           403,
           404,
           405,
           406,
           407,
           408,
           409,
           410,
           411,
           412,
           413,
           414,
           415,
           416,
           417,
           418,
           419,
           420,
           421,
           422,
           423,
           424,
           425,
           426,
           427,
           428,
           429,
           430,
           431,
           432,
           433,
           434,
           435,
           436,
           437,
           438,
           439,
           440,
           441,
           442,
           443,
           444,
           445,
           446,
           447,
           448,
           449,
           450,
           451,
           452,
           453,
           454,
           455,
           456,
           457,
           458,
           459,
           460,
           461,
           462,
           463,
           464,
           465,
           466,
           467,
           468,
           469,
           470,
           471,
           472,
           473,
           474,
           475,
           476,
           477,
           478,
           479,
           480,
           481,
           482,
           483,
           484,
           485,
           486,
           487,
           488,
           489,
           490,
           491,
           492,
           493,
           494,
           495,
           496,
           497,
           498,
           499,
           500,
           501,
           502,
           503,
           504,
           505,
           506,
           507,
           508,
           509,
           510,
           511,
           512,
           513,
           514,
           515,
           516,
           517,
           518,
           519,
           520,
           521,
           522,
           523,
           524,
           525,
           526,
           527,
           528,
           529,
           530,
           531,
           532,
           533,
           534,
           535,
           536,
           537,
           538,
           539,
           540,
           541,
           542,
           543,
           544,
           545,
           546,
           547,
           548,
           549,
           550,
           551,
           552,
           553,
           554,
           555,
           556,
           557,
           558,
           559,
           560,
           561,
           562,
           563,
           564,
           565,
           566,
           567,
           568,
           569,
           570,
           571,
           572,
           573,
           574,
           575,
           576,
           577,
           578,
           579,
           580,
           581,
           582,
           583,
           584,
           585,
           586,
           587,
           588,
           589,
           590,
           591,
           592,
           593,
           594,
           595,
           596,
           597,
           598,
           599,
           600,
           601,
           602,
           603,
           604,
           605,
           606,
           607,
           608,
           609,
           610,
           611,
           612,
           613,
           614,
           615,
           616,
           617,
           618,
           619,
           620,
           621,
           622,
           623,
           624,
           625,
           626,
           627,
           628,
           629,
           630,
           631,
           632,
           633,
           634,
           635,
           636,
           637,
           638,
           639,
           640,
           641,
           642,
           643,
           644,
           645,
           646,
           647,
           648,
           649,
           650,
           651,
           652,
           653,
           654,
           655,
           656,
           657,
           658,
           659,
           660,
           661,
           662,
           663,
           664,
           665,
           666,
           667,
           668,
           669,
           670,
           671,
           672,
           673,
           674,
           675,
           676,
           677,
           678,
           679,
           680,
           681,
           682,
           683,
           684,
           685,
           686,
           687,
           688,
           689,
           690,
           691,
           692,
           693,
           694,
           695,
           696,
           697,
           698,
           699,
           700,
           701,
           702,
           703,
           704,
           705,
           706,
           707,
           708,
           709,
           710,
           711,
           712,
           713,
           714,
           715,
           716,
           717,
           718,
           719,
           720,
           721,
           722,
           723,
           724,
           725,
           726,
           727,
           728,
           729,
           730,
           731,
           732,
           733,
           734,
           735,
           736,
           737,
           738,
           739,
           740,
           741,
           742,
           743,
           744,
           745,
           746,
           747,
           748,
           749,
           750,
           751,
           752,
           753,
           754,
           755,
           756,
           757,
           758,
           759,
           760,
           761,
           762,
           763,
           764,
           765,
           766,
           767,
           768,
           769,
           770,
           771,
           772,
           773,
           774,
           775,
           776,
           777,
           778,
           779,
           780,
           781,
           782,
           783,
           784,
           785,
           786,
           787,
           788,
           789,
           790,
           791,
           792,
           793,
           794,
           795,
           796,
           797,
           798,
           799,
           800,
           801,
           802,
           803,
           804,
           805,
           806,
           807,
           808,
           809,
           810,
           811,
           812,
           813,
           814,
           815,
           816,
           817,
           818,
           819,
           820,
           821,
           822,
           823,
           824,
           825,
           826,
           827,
           828,
           829,
           830,
           831,
           832,
           833,
           834,
           835,
           836,
           837,
           838,
           839,
           840,
           841,
           842,
           843,
           844,
           845,
           846,
           847,
           848,
           849,
           850,
           851,
           852,
           853,
           854,
           855,
           856,
           857,
           858,
           859,
           860,
           861,
           862,
           863,
           864,
           865,
           866,
           867,
           868,
           869,
           870,
           871,
           872,
           873,
           874,
           875,
           876,
           877,
           878,
           879,
           880,
           881,
           882,
           883,
           884,
           885,
           886,
           887,
           888,
           889,
           890,
           891,
           892,
           893,
           894,
           895,
           896,
           897,
           898,
           899,
           900,
           901,
           902,
           903,
           904,
           905,
           906,
           907,
           908,
           909,
           910,
           911,
           912,
           913,
           914,
           915,
           916,
           917,
           918,
           919,
           920,
           921,
           922,
           923,
           924,
           925,
           926,
           927,
           928,
           929,
           930,
           931,
           932,
           933,
           934,
           935,
           936,
           937,
           938,
           939,
           940,
           941,
           942,
           943,
           944,
           945,
           946,
           947,
           948,
           949,
           950,
           951,
           952,
           953,
           954,
           955,
           956,
           957,
           958,
           959,
           960,
           961,
           962,
           963,
           964,
           965,
           966,
           967,
           968,
           969,
           970,
           971,
           972,
           973,
           974,
           975,
           976,
           977,
           978,
           979,
           980,
           981,
           982,
           983,
           984,
           985,
           986,
           987,
           988,
           989,
           990,
           991,
           992,
           993,
           994,
           995,
           996,
           997,
           998,
           999
          ],
          "colorbar": {
           "title": {
            "text": "Trial"
           },
           "x": 1,
           "xpad": 40
          },
          "colorscale": [
           [
            0,
            "rgb(247,251,255)"
           ],
           [
            0.125,
            "rgb(222,235,247)"
           ],
           [
            0.25,
            "rgb(198,219,239)"
           ],
           [
            0.375,
            "rgb(158,202,225)"
           ],
           [
            0.5,
            "rgb(107,174,214)"
           ],
           [
            0.625,
            "rgb(66,146,198)"
           ],
           [
            0.75,
            "rgb(33,113,181)"
           ],
           [
            0.875,
            "rgb(8,81,156)"
           ],
           [
            1,
            "rgb(8,48,107)"
           ]
          ],
          "line": {
           "color": "Grey",
           "width": 0.5
          },
          "showscale": false
         },
         "mode": "markers",
         "name": "Feasible Trial",
         "showlegend": false,
         "type": "scatter",
         "x": [
          8,
          9,
          10,
          9,
          8,
          5,
          6,
          9,
          8,
          2,
          2,
          5,
          4,
          5,
          4,
          6,
          4,
          1,
          5,
          7,
          7,
          7,
          3,
          3,
          3,
          7,
          3,
          6,
          1,
          4,
          2,
          2,
          2,
          2,
          1,
          1,
          2,
          1,
          2,
          1,
          1,
          1,
          1,
          1,
          2,
          3,
          10,
          2,
          1,
          2,
          1,
          1,
          1,
          2,
          1,
          3,
          2,
          2,
          4,
          3,
          2,
          1,
          1,
          2,
          3,
          2,
          2,
          3,
          2,
          9,
          1,
          1,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          3,
          1,
          1,
          2,
          1,
          1,
          2,
          1,
          2,
          1,
          1,
          5,
          1,
          1,
          2,
          1,
          1,
          2,
          2,
          4,
          1,
          1,
          1,
          1,
          1,
          2,
          1,
          1,
          3,
          3,
          3,
          2,
          2,
          2,
          2,
          6,
          2,
          4,
          2,
          8,
          3,
          2,
          2,
          2,
          2,
          3,
          3,
          3,
          3,
          3,
          4,
          4,
          3,
          3,
          4,
          4,
          4,
          4,
          5,
          3,
          3,
          3,
          3,
          4,
          5,
          3,
          2,
          3,
          2,
          2,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          3,
          3,
          2,
          4,
          3,
          2,
          2,
          6,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          5,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          5,
          4,
          4,
          4,
          4,
          4,
          2,
          4,
          5,
          4,
          4,
          4,
          3,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          10,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          7,
          2,
          2,
          2,
          2,
          2,
          9,
          2,
          2,
          2,
          2,
          2,
          6,
          2,
          5,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          8,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          6,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          7,
          1,
          1,
          1,
          5,
          1,
          1,
          2,
          1,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          1,
          2,
          2,
          1,
          2,
          1,
          2,
          2,
          1,
          2,
          2,
          1,
          2,
          2,
          1,
          2,
          1,
          5,
          2,
          3,
          2,
          1,
          2,
          2,
          1,
          2,
          3,
          1,
          2,
          10,
          2,
          3,
          1,
          2,
          2,
          1,
          3,
          2,
          1,
          2,
          2,
          1,
          2,
          1,
          2,
          2,
          1,
          2,
          2,
          1,
          2,
          1,
          2,
          3,
          3,
          3,
          2,
          2,
          1,
          2,
          2,
          3,
          1,
          2,
          2,
          1,
          2,
          1,
          2,
          8,
          2,
          9,
          2,
          3,
          1,
          2,
          1,
          2,
          1,
          2,
          3,
          2,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          2,
          2,
          3,
          1,
          2,
          2,
          1,
          3,
          2,
          1,
          2,
          1,
          2,
          2,
          1,
          2,
          2,
          2,
          1,
          6,
          2,
          1,
          3,
          2,
          2,
          3,
          1,
          1,
          2,
          2,
          2,
          1,
          2,
          2,
          1,
          2,
          3,
          2,
          1,
          2,
          1,
          5,
          2,
          2,
          3,
          1,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          3,
          2,
          2,
          3,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          4,
          2,
          2,
          2,
          3,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          3,
          2,
          3,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          4,
          3,
          2,
          2,
          2,
          2,
          2,
          3,
          2,
          2,
          2,
          2,
          6,
          2,
          2,
          3,
          2,
          2,
          2,
          5,
          7,
          3,
          2,
          2,
          2,
          2,
          2,
          4,
          2,
          3,
          2,
          2,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          3,
          1,
          2,
          1,
          3,
          4,
          2,
          2,
          10,
          2,
          2,
          1,
          2,
          2,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          3,
          3,
          5,
          3,
          2,
          2,
          2,
          2,
          2,
          3,
          2,
          2,
          2,
          2,
          3,
          2,
          2,
          2,
          4,
          2,
          2,
          2,
          3,
          2,
          3,
          9,
          2,
          7,
          2,
          2,
          2,
          5,
          2,
          1,
          3,
          2,
          6,
          4,
          2,
          8,
          1,
          2,
          2,
          1,
          3,
          2,
          2,
          4,
          1,
          2,
          3,
          1,
          3,
          2,
          2,
          1,
          2,
          2,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          3,
          2,
          2,
          3,
          2,
          2,
          2,
          3,
          2,
          2,
          2,
          2,
          2,
          3,
          2,
          2,
          2,
          3,
          2,
          4,
          2,
          2,
          2,
          2,
          3,
          2,
          2,
          2,
          2,
          2,
          4,
          5,
          3,
          3,
          2,
          2,
          2,
          2,
          2,
          1,
          3,
          2,
          2,
          2,
          1,
          2,
          2,
          3,
          1,
          2,
          2,
          1,
          2,
          3,
          4,
          2,
          1,
          2,
          2,
          2,
          1,
          3,
          1,
          2,
          2,
          2,
          3,
          2,
          1,
          5,
          2,
          2,
          1,
          2,
          3,
          1,
          2,
          2,
          2,
          1,
          2,
          3,
          2,
          4,
          1,
          2,
          2,
          3,
          2,
          6,
          1,
          2,
          2,
          1,
          2,
          3,
          2,
          2,
          1,
          2,
          1,
          2,
          3,
          2,
          4,
          2,
          1,
          2,
          3,
          1,
          2,
          2,
          2,
          1,
          3,
          2,
          2,
          1,
          2,
          2,
          1,
          2,
          2,
          2,
          3,
          1,
          2,
          4,
          2,
          7,
          3,
          2,
          1,
          2,
          1,
          2,
          2,
          10,
          3,
          2,
          5,
          1,
          2,
          4,
          2,
          2,
          3,
          1,
          2,
          1,
          2,
          2,
          1,
          3,
          2,
          3,
          2,
          1,
          2,
          2,
          2,
          2,
          1,
          2,
          1,
          2,
          2,
          3,
          2,
          1,
          2,
          3,
          2,
          1,
          2,
          2,
          1,
          3,
          4,
          8,
          2,
          1,
          2,
          2,
          2,
          1,
          2,
          2,
          3,
          2,
          3,
          1,
          2,
          9,
          2,
          1,
          2,
          5,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          6,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          4,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          5,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          4,
          2,
          1,
          2,
          2,
          1,
          2,
          2,
          2
         ],
         "xaxis": "x3",
         "y": [
          0.08539019019252586,
          0.09495197683593737,
          0.08561203246811563,
          0.10226171316013058,
          0.09436835513192324,
          0.08442866030728638,
          0.0882806271297641,
          0.09259418891274053,
          0.0936506610503796,
          0.08905616282810194,
          0.10070957535224619,
          0.0849354185113468,
          0.09538829459268595,
          0.08532982608602123,
          0.09802117955542877,
          0.08861113027563677,
          0.08782249655960989,
          0.09671218874633744,
          0.10056915412131459,
          0.08463494265724372,
          0.08550715408794303,
          0.08624021489163447,
          0.08421050029548485,
          0.0894877666245822,
          0.08522084111774414,
          0.08664087579640814,
          0.09277247714187424,
          0.08890925915171902,
          0.08679846383946271,
          0.08434924485492348,
          0.08332985614930138,
          0.08381487607339143,
          0.08381121898840153,
          0.08492498104511295,
          0.08351886332682175,
          0.1065370630575128,
          0.08407738487249838,
          0.10418987847943002,
          0.08315042884967586,
          0.08360267337033281,
          0.08344470964379247,
          0.08381392901444998,
          0.0857432511381672,
          0.08371432932413264,
          0.08615883393220121,
          0.08428574598953223,
          0.0897804337144529,
          0.10368442923134472,
          0.08419051036454686,
          0.08666228717253156,
          0.08539823364234111,
          0.08398322451750445,
          0.08324196548934296,
          0.08498755749166624,
          0.08592689223502362,
          0.08643366899645041,
          0.0835047463204986,
          0.09288435334111321,
          0.08424549859482734,
          0.08502893317695907,
          0.0890072310285759,
          0.08354751438906698,
          0.08481137014593011,
          0.08285306771082904,
          0.08435363262967716,
          0.08440017469452545,
          0.10234792567968147,
          0.08439105115445075,
          0.08367204082049616,
          0.08674702124077736,
          0.08364722764310903,
          0.08339834509471708,
          0.08458083145212651,
          0.08730137187390588,
          0.08347106168854283,
          0.10497466632612777,
          0.09500932690462209,
          0.08627061663570071,
          0.08361465461377242,
          0.0862346237864177,
          0.08576719136060315,
          0.08392691309042451,
          0.08667835916016219,
          0.0832016218464397,
          0.08360618571881825,
          0.10570571526403402,
          0.08912974625484643,
          0.08794829675893581,
          0.10369587843510582,
          0.08450059575739881,
          0.08564046243395204,
          0.08425998493872724,
          0.08328057039506313,
          0.08414219287159663,
          0.08939263864603345,
          0.0846428294515632,
          0.08165961576392188,
          0.08488512104472777,
          0.0864821733201315,
          0.0844953596275288,
          0.08312414300199703,
          0.08302348043185244,
          0.08349016143148591,
          0.08273551529377218,
          0.08712664607608565,
          0.08335508002367598,
          0.0833304450341904,
          0.08274977982394807,
          0.08723091108735959,
          0.10129893784474617,
          0.08328120589468982,
          0.08518771642746378,
          0.08417473164618214,
          0.08544436252688223,
          0.084259756018591,
          0.08477727688870212,
          0.0843732621596494,
          0.08238179744888834,
          0.0835201737237383,
          0.08440188316439623,
          0.0826963136261387,
          0.08325320855427412,
          0.08232280947837918,
          0.08313899416292488,
          0.0827606041580992,
          0.08345781219298537,
          0.082856239988547,
          0.08315158199964165,
          0.09996173825297669,
          0.08287132257374849,
          0.08399392866757854,
          0.08329801970738117,
          0.08218916379001612,
          0.08272626181044221,
          0.08356949324059257,
          0.08344809825525434,
          0.08412450105439928,
          0.08436250442465335,
          0.08543737126852116,
          0.08315415930339068,
          0.08328874654472859,
          0.08377063932211924,
          0.08374662836316327,
          0.08432312077904969,
          0.08334078110772783,
          0.10451366567669003,
          0.08396547011835669,
          0.09866471516554136,
          0.0840355188769745,
          0.08313315216040826,
          0.0833273896064651,
          0.08380846367977608,
          0.08301209213271031,
          0.08302693484535764,
          0.08513153125053821,
          0.08294988420046369,
          0.0828023224844447,
          0.08576151350462309,
          0.08313593893209155,
          0.0823890790084934,
          0.10233081625126154,
          0.08391457740581108,
          0.08346778197338994,
          0.08323672801444174,
          0.11016564627256034,
          0.08625986833194307,
          0.08308838128817571,
          0.08256675863644616,
          0.08687355439209934,
          0.08320375301071112,
          0.0831569856636962,
          0.0830861335286607,
          0.08373209848835807,
          0.08325772728529475,
          0.0840249429842295,
          0.08340173491254706,
          0.0841523490888042,
          0.08278875829740572,
          0.08588354015186887,
          0.08460117799571931,
          0.08271264948133934,
          0.08311842459534399,
          0.09775820704966276,
          0.08363667644538213,
          0.08518966582885326,
          0.08234088467720187,
          0.08300663028468404,
          0.08294792493806535,
          0.09374333088296935,
          0.08186637706985608,
          0.08286524474377754,
          0.08407084466100902,
          0.08513404749620522,
          0.08512809066960167,
          0.08317299290426386,
          0.08329165608201898,
          0.09249173931498986,
          0.08581289239318927,
          0.08311832057969164,
          0.08244091747412202,
          0.0859276097402228,
          0.08294365575003566,
          0.08291915631246957,
          0.08431553209465299,
          0.08316980756401962,
          0.08275119431371236,
          0.08259810148865966,
          0.08269710428895796,
          0.0825306928188557,
          0.08327358623423534,
          0.08337420215674948,
          0.10445665012752847,
          0.08334076815055066,
          0.08274338024429977,
          0.08251096914423196,
          0.08199902606831211,
          0.08235772891596732,
          0.08293553119582622,
          0.08253509322945916,
          0.08376735221276418,
          0.08242039156277536,
          0.08310408342648781,
          0.08236920723797173,
          0.08291592089843107,
          0.08237535912990515,
          0.08328242745865577,
          0.08330519127524712,
          0.08333766438105057,
          0.08348678243602832,
          0.08346972442740377,
          0.08291621932822964,
          0.08231996370906212,
          0.08231519235453469,
          0.08261364924345689,
          0.0825212172002789,
          0.08282661754470844,
          0.08280000274572497,
          0.0822068912312609,
          0.08306979246734976,
          0.08280936601250934,
          0.08323729894382537,
          0.0827596216791415,
          0.08254312932582486,
          0.08260706905635237,
          0.08219159376324617,
          0.08332867404127005,
          0.08245010521755164,
          0.0830015532855319,
          0.08311543440998431,
          0.08354447071482189,
          0.08459523466839948,
          0.08279362737753312,
          0.0832079959920258,
          0.08271413443324369,
          0.08228669392391635,
          0.08746037507141037,
          0.08244453094845948,
          0.08267614433778225,
          0.08361087104719155,
          0.08254682461025294,
          0.08315199608854278,
          0.0831869149479296,
          0.0831621325240794,
          0.08241511203722858,
          0.08509378854913294,
          0.09446901061125146,
          0.0829485186897416,
          0.08256420848681219,
          0.08241588768524585,
          0.0832236338954678,
          0.08285732035700681,
          0.08516721649864492,
          0.08291222919437333,
          0.08335991669751068,
          0.08242883783270984,
          0.08266547354134668,
          0.08538467882794278,
          0.09503155262312986,
          0.08288635613635478,
          0.08264974876794147,
          0.08211371813595908,
          0.08657470992634171,
          0.08259439090255191,
          0.08312408247804894,
          0.08297372789394555,
          0.0824331510535781,
          0.08637316625377117,
          0.10319183305913875,
          0.08404118132533886,
          0.08263370202318032,
          0.0834927133525499,
          0.08558875960099446,
          0.0825962951162357,
          0.08525988219355811,
          0.08491634346648637,
          0.08277226913673,
          0.0821336454759914,
          0.08324338368912149,
          0.08389344305054196,
          0.0848766197803248,
          0.0828420257148326,
          0.08199056700519247,
          0.09088192756881397,
          0.08291160072327894,
          0.1049394814041883,
          0.0836179953257,
          0.08265678691618213,
          0.08329057337357557,
          0.08586042141522317,
          0.0831940183020074,
          0.08226786937139183,
          0.08258516295466148,
          0.08418431503900826,
          0.08378835210536532,
          0.08655320541884155,
          0.08267759915260924,
          0.10230479617375414,
          0.08253292069688178,
          0.08360373450105948,
          0.08306322233093005,
          0.08297395306054933,
          0.08502468501565282,
          0.08294150522200303,
          0.08291127575946386,
          0.0845033398062486,
          0.08229269394206133,
          0.08319980298285114,
          0.08355104320136941,
          0.08214051827823698,
          0.08368642417592195,
          0.10056914324523544,
          0.08742187178093777,
          0.10750592060617267,
          0.08342305906642027,
          0.08514749289202753,
          0.08306091591721386,
          0.08237992365506012,
          0.08504635372532161,
          0.08217331987854679,
          0.08579425456448286,
          0.08467105207276104,
          0.08242706667986897,
          0.08495171802357053,
          0.08436796376240073,
          0.08324510960531548,
          0.08348012821167246,
          0.08222864482506259,
          0.08360830242929047,
          0.08218972123363966,
          0.0846592336480432,
          0.08296534942665079,
          0.08385594691626566,
          0.08428881746489056,
          0.08514809023965,
          0.08517640762635152,
          0.08374766448669568,
          0.08346478111354153,
          0.08345120363017818,
          0.08341290855897499,
          0.08363018486181899,
          0.08310301346123908,
          0.09272379644340216,
          0.08378574735051028,
          0.0852733325696883,
          0.08216595188325272,
          0.10378265171249632,
          0.08294003021589308,
          0.08326020405580013,
          0.08228238847757531,
          0.08428079796872202,
          0.08311972393512462,
          0.08338968730295256,
          0.08293213450970785,
          0.09393946362590899,
          0.08327466350181753,
          0.08506553342131629,
          0.0826984353792733,
          0.08311464356185652,
          0.08507777430539948,
          0.10664077240786737,
          0.08308505363475249,
          0.0840521759461056,
          0.0984829737053229,
          0.08409671431676492,
          0.08343552651457424,
          0.08218376867519202,
          0.0826176725634861,
          0.087404024246948,
          0.0824579868517901,
          0.08343538851576185,
          0.08632526446250707,
          0.10113468346246568,
          0.08408297959141789,
          0.08455967169326965,
          0.08290135112828734,
          0.08622599585167161,
          0.08256680193136179,
          0.08279304994854082,
          0.08298208106969547,
          0.086487997927399,
          0.08451040655479528,
          0.08312387237224324,
          0.08279913946855727,
          0.08525800338496867,
          0.08306589903344415,
          0.08263009693292171,
          0.08444546632016169,
          0.08282584835510597,
          0.08275273275273365,
          0.08324783184800626,
          0.08293670250894221,
          0.08291566242729534,
          0.08471320065483626,
          0.08267044541148255,
          0.08447697754338505,
          0.08390700147066862,
          0.08333214843043357,
          0.0828179712937066,
          0.08299945475657355,
          0.08281322887122904,
          0.08395017481762637,
          0.08241984428417812,
          0.08213717563466298,
          0.10076673879446381,
          0.0832436787503004,
          0.08252665324010452,
          0.08320587493050441,
          0.08776937049652797,
          0.08227879636452218,
          0.08310102230104396,
          0.08312565513737012,
          0.08321513334889388,
          0.08299056674606672,
          0.08303414146151361,
          0.083011182298674,
          0.08365498360084109,
          0.08588668701466871,
          0.08384000289415564,
          0.0833124264339247,
          0.08238305561835071,
          0.09203839346356635,
          0.08349258535191846,
          0.08215443768884524,
          0.08349214873884861,
          0.08403318361265764,
          0.0824595433892342,
          0.08413298882527163,
          0.08435908750491991,
          0.08767755459365653,
          0.0827298801062617,
          0.09582911899764356,
          0.08268836609716766,
          0.08217887190324438,
          0.08456203431506822,
          0.08395906740770928,
          0.08721133673682199,
          0.08326946965557938,
          0.08277857338975389,
          0.08516307424345448,
          0.08874462697002515,
          0.08388595508820527,
          0.10162837463997096,
          0.08349279022803455,
          0.08273657163673434,
          0.08376690305749647,
          0.08253757560419685,
          0.10402306282405352,
          0.08277068997162221,
          0.08764543348113721,
          0.08260209306314711,
          0.08246848587072822,
          0.08308283039199962,
          0.08247267673486194,
          0.09189560105049469,
          0.08462355983006513,
          0.08329194609971924,
          0.08316037595639489,
          0.08317869616338323,
          0.08316144874053474,
          0.08959409565848214,
          0.08402716720523631,
          0.08400077230926636,
          0.0829619142270787,
          0.08283674928845328,
          0.0847778461277469,
          0.08344920849250995,
          0.08523696176791473,
          0.08725620307003272,
          0.10397492307026904,
          0.08595226907323969,
          0.09921985101954633,
          0.09533855013187104,
          0.08309507191276091,
          0.0826759786006139,
          0.08225593586524416,
          0.08740274630438884,
          0.08315660205916471,
          0.08608837841090153,
          0.08467202290989553,
          0.097137117730765,
          0.08275036333514421,
          0.08386434363894248,
          0.08188325999628746,
          0.08557863261436732,
          0.08446231710143276,
          0.08368820469053169,
          0.08285472367859309,
          0.08303516672857214,
          0.0833906914083489,
          0.08366981604698515,
          0.08793276147551224,
          0.08952814150800537,
          0.08308096878529299,
          0.08256381892015496,
          0.08317662056064302,
          0.08710183821325071,
          0.0834428219383049,
          0.0830258565559016,
          0.08355652692921657,
          0.08424998322924931,
          0.08334819052906109,
          0.08573110756005994,
          0.08488722805270349,
          0.08380901110932767,
          0.09618439324678967,
          0.0826032479888251,
          0.0820830552828102,
          0.0862818168369496,
          0.08401134731429585,
          0.08295871971481268,
          0.08240986881096399,
          0.0824353229340784,
          0.08462405213082154,
          0.08262857330685547,
          0.09748615573746822,
          0.08613884514448175,
          0.08814720651422511,
          0.0824457607071543,
          0.08331839738519489,
          0.0821804083798195,
          0.08350042182975628,
          0.08311370306065799,
          0.08886475197480367,
          0.08374134130419003,
          0.08466248680311633,
          0.083197319026209,
          0.0870576233729369,
          0.10433184556727466,
          0.08395723755226109,
          0.08220024890163888,
          0.08265994797689089,
          0.10548729044772294,
          0.08270016460923824,
          0.08263693091573175,
          0.08408285162449515,
          0.08247965927491127,
          0.08510520859519778,
          0.08393599427281197,
          0.08298244561042319,
          0.08562942857561037,
          0.08615259273943957,
          0.08253164483660641,
          0.0834048056413013,
          0.0836444464588196,
          0.08644759051217846,
          0.10161932762356388,
          0.0860720895428612,
          0.08284569478174883,
          0.08371538532284166,
          0.08264559635056794,
          0.08284416313727293,
          0.08391010104815012,
          0.08260172216704732,
          0.08581525248878688,
          0.08305984930644876,
          0.08274444052530283,
          0.08488695733354572,
          0.08243008456223479,
          0.0829948282257483,
          0.08495372244600471,
          0.08321977121587064,
          0.08635692506489748,
          0.08264673692125761,
          0.08186846040844238,
          0.08263059881485445,
          0.10441472501698376,
          0.10528485609977607,
          0.08499971771788298,
          0.09259959919221114,
          0.08261318952776894,
          0.08385959899234947,
          0.0826226315654416,
          0.08258107343944868,
          0.08331235626742295,
          0.0834970814550906,
          0.08322184354859442,
          0.08366762990690224,
          0.08232107545407749,
          0.08502953198831671,
          0.08469003944748557,
          0.08222412784889119,
          0.08312009935117741,
          0.08571363180020228,
          0.08237007825013624,
          0.0835774816385849,
          0.08347237759742218,
          0.08308469365588271,
          0.08238220838082617,
          0.08422243198098708,
          0.08466688799410906,
          0.08329245959958595,
          0.0831207423118143,
          0.08315877480983573,
          0.0852580342864113,
          0.08307430873764746,
          0.08793018395955414,
          0.0830730472577487,
          0.08393387458472819,
          0.08540937207454335,
          0.08348777746597265,
          0.08265909677459007,
          0.08387453848047574,
          0.08635036092591931,
          0.0829349762863024,
          0.08241289957264089,
          0.10413855843091069,
          0.08253266605259721,
          0.08648364062869808,
          0.08191460734977042,
          0.08310340315683169,
          0.0829548528835559,
          0.0864318137649662,
          0.0829293169593892,
          0.10305693039776236,
          0.08356895669343908,
          0.08244647842569405,
          0.08415434846452788,
          0.0830010827587181,
          0.08438485467873562,
          0.08223489665962366,
          0.0827530514167065,
          0.08357849234207144,
          0.08273626391953012,
          0.08404602275072952,
          0.08231460044689605,
          0.08210942459292067,
          0.08321989252062911,
          0.08319843094976215,
          0.08535502837758135,
          0.083684048231146,
          0.0829440056016863,
          0.08306201265224991,
          0.08321963632160116,
          0.08523513451119122,
          0.08957199257037188,
          0.10218071104046678,
          0.08365996670182504,
          0.08288414315622594,
          0.0831609409532332,
          0.08563748131796066,
          0.08267228190331319,
          0.08351072308999888,
          0.08595073621997369,
          0.0830822013310413,
          0.09276760202112158,
          0.08320761930510705,
          0.08320375706140715,
          0.087980450153498,
          0.08257120222868822,
          0.08382900630949686,
          0.08347497757570743,
          0.09168496044283138,
          0.08287356761286935,
          0.09610522792691246,
          0.0840937911692142,
          0.08320281863107817,
          0.0840821741688485,
          0.08296261071055012,
          0.08702358037168506,
          0.08304670690396758,
          0.08282960813412675,
          0.08504832917070236,
          0.08280821131692603,
          0.08357559868071203,
          0.10030268846476795,
          0.0830115284099357,
          0.08538684115689459,
          0.08316314519172648,
          0.08674664611182713,
          0.0833060353078631,
          0.08276759290831337,
          0.082636621675815,
          0.08519768869200897,
          0.08343585614273649,
          0.08348041210697296,
          0.08479695608461686,
          0.10471228685959055,
          0.08411816094404848,
          0.08253897607087386,
          0.08282121749834048,
          0.08301176176181067,
          0.08422872819064456,
          0.08248307926521628,
          0.0873169601423071,
          0.08403867263588274,
          0.08360671456693249,
          0.0873387381930901,
          0.08303636122381383,
          0.08429013910580663,
          0.08245037943250491,
          0.08731979299476997,
          0.0831649244330491,
          0.08299715585650622,
          0.08383338746534745,
          0.08300374574868034,
          0.08221742281473433,
          0.08328189460011386,
          0.08380170444242878,
          0.08724155313411035,
          0.08266658783360512,
          0.0827461062704319,
          0.08366220360416872,
          0.08926097460933158,
          0.0820128908958929,
          0.08239676192015702,
          0.08335814330410822,
          0.08271122899355499,
          0.09320347156581618,
          0.08244578693077234,
          0.08455687464846962,
          0.0827144043953543,
          0.09735407733563475,
          0.08311742873296009,
          0.08253193283787222,
          0.08503616705876,
          0.08229462409623334,
          0.10227328262969504,
          0.08244098270982371,
          0.08458546705476075,
          0.08245023856627462,
          0.08390096654345602,
          0.08261727710396506,
          0.08296341485333819,
          0.0843856743061751,
          0.0826403264007882,
          0.08236202033659863,
          0.08491342896501539,
          0.08249553332142545,
          0.08248501382813546,
          0.08325999242968649,
          0.08317749236739873,
          0.08683336062238954,
          0.0816765138371928,
          0.08259875051057868,
          0.08402034285954971,
          0.08252324178034955,
          0.09427206101801941,
          0.08361224594033768,
          0.08780454576834186,
          0.08306918463313627,
          0.08294936107813836,
          0.08226373331746348,
          0.08270158075217962,
          0.09324708168090719,
          0.08375970454230489,
          0.08278135784160373,
          0.08603363354510356,
          0.1045542831789172,
          0.0826534965564436,
          0.08254708094217993,
          0.08678866035047919,
          0.08257299826086026,
          0.10524439261200529,
          0.08462142450028,
          0.09693374076406427,
          0.10746782345354618,
          0.0865524613434759,
          0.08328747282232361,
          0.08874189978266406,
          0.08327021589551106,
          0.09380548783775086,
          0.08239314017106542,
          0.08329407297801958,
          0.08299425059418443,
          0.08576349535115597,
          0.0833380400574528,
          0.08329158468354203,
          0.09177749989897299,
          0.08283706581670945,
          0.08460100921379025,
          0.08298000691791643,
          0.0826500350927559,
          0.08338863250547998,
          0.08284291699085977,
          0.08353928226982023,
          0.0828191079128297,
          0.08425831915679907,
          0.08394784695156927,
          0.1048112802167831,
          0.08349310994313272,
          0.08527538963067192,
          0.08234989201226825,
          0.08602489920911367,
          0.08372643663163837,
          0.08333347635755216,
          0.0831372896782915,
          0.08252173787708882,
          0.0830012992386513,
          0.08352099809410667,
          0.08484631339069673,
          0.08376725637774293,
          0.08248257995669252,
          0.10291475756183441,
          0.08266324199933343,
          0.08417132121165083,
          0.08288839769821313,
          0.08226414886960809,
          0.10099639770365246,
          0.08272441754739207,
          0.08387966155676363,
          0.09299240361592506,
          0.08444045941099101,
          0.0834123863076146,
          0.08412272761003801,
          0.08446355118849863,
          0.08334290321166601,
          0.08245797628427717,
          0.09368903580642896,
          0.08579221761627058,
          0.0833885768570585,
          0.08239055214740282,
          0.08249961918811777,
          0.08306052813844458,
          0.0831357568439862,
          0.08409036395262161,
          0.08337417185758891,
          0.0829387506742927,
          0.08260896282855443,
          0.08498237910796849,
          0.08300392716098079,
          0.08195714197156158,
          0.08418743298567412,
          0.10038129945318305,
          0.08327494977087521,
          0.08373973990748947,
          0.08308164190378595,
          0.09100211356259427,
          0.08563965910037585,
          0.08583124169737358,
          0.08392212825027276,
          0.08279077028629671,
          0.08236456924978328,
          0.10629128661382152,
          0.08250091564787407,
          0.08713988911407138,
          0.08433474922626143,
          0.0826654757872344,
          0.08395989014513086,
          0.08574106211056873,
          0.08285930018085841,
          0.08299692922839406,
          0.08441498679660339,
          0.08389846280538288,
          0.0848405257616334,
          0.0834616221668075,
          0.08584030665022686,
          0.08322891696303439,
          0.08353011074735918,
          0.08309285294850441,
          0.08651283933196181,
          0.08310693915881492,
          0.08290063321512871,
          0.08320239035774854,
          0.08601050254872308,
          0.08393616543478491,
          0.08260553336789768,
          0.085613102481566,
          0.0825841065356284,
          0.08324735921670097,
          0.08332004573388377,
          0.0931690848266701,
          0.0859662930507487,
          0.08373161134596022,
          0.08301017264797066,
          0.08336194886702872,
          0.08255570842720951,
          0.08764987998468635,
          0.08336795546281142,
          0.08398129766152383,
          0.0848594980991578,
          0.08266155455990679,
          0.08247719720819359,
          0.08374440729884888,
          0.08310656364763773,
          0.10226158936330351,
          0.09595609196563701,
          0.08276011433173194,
          0.08717349665672955,
          0.08246932495623632,
          0.08319389051401872,
          0.0824771668600773,
          0.08573726649146093,
          0.08725387877287781,
          0.08304789224788486,
          0.08434879957739697,
          0.08363829132457859,
          0.10460715119245341,
          0.08220427027112753,
          0.08271754200267924,
          0.08689833135454574,
          0.0834464881164965,
          0.08562707073378686,
          0.08321486883375731,
          0.08404642113923066,
          0.08519422596037746,
          0.08574035101850674,
          0.08260114456740034,
          0.08226617978994574,
          0.09218102338526904,
          0.08182862624074039,
          0.08255383901034316,
          0.08343164795439631,
          0.09953847346653258,
          0.08232191407943026,
          0.0817069837129533,
          0.0867796506853938,
          0.08460698583192258,
          0.0817647500281331,
          0.08327299992035239,
          0.08248162883909414,
          0.0840224012952457,
          0.08418530669104944,
          0.087973293685031,
          0.08268698123591149,
          0.08437574712624792,
          0.08327755337916884,
          0.08214275629720465,
          0.08414232814750208,
          0.08329582745550315,
          0.08720531957157558,
          0.08479025950591201,
          0.08275803518911087,
          0.08291628786089525,
          0.08393236306890406,
          0.10079997009926882,
          0.08716367263376307,
          0.08492904714119187,
          0.08386324725465437,
          0.08271281080959585,
          0.08308604866601085,
          0.0834212191163988,
          0.08595553137476505,
          0.08234046626972627,
          0.08227295477044481,
          0.08406219882785552,
          0.08224580632195619,
          0.08345248633800766,
          0.08819583941980899,
          0.08470703788496611,
          0.08301319642691471,
          0.08769757652837515,
          0.0843761094860751,
          0.08335234029658274,
          0.08313066301315937,
          0.0833117535252543,
          0.08264534952309555,
          0.08355628770407349,
          0.08442672436586694,
          0.08236143286177203,
          0.083264741051013,
          0.08688689989921365,
          0.08449092200401603,
          0.08576533320175621,
          0.08308582311144246,
          0.09109043997091111,
          0.08254199390917345,
          0.0821983819056997,
          0.08240965082024795,
          0.08636383225110883,
          0.10300281038002383,
          0.08214853769173854,
          0.08268008983563277,
          0.08370669105344264,
          0.08268520080035745,
          0.08279714856339851,
          0.08713031875136815,
          0.08250872961798147,
          0.08243085229276693,
          0.08238836573806105,
          0.0825027710727266,
          0.08298343614744504,
          0.08356331086152853,
          0.09050036013864263,
          0.08511173574036432,
          0.08341313343305887,
          0.08258826362434266,
          0.0829828554386322,
          0.08276858925895582,
          0.08433108819297482,
          0.08237576200573207
         ],
         "yaxis": "y3"
        },
        {
         "marker": {
          "color": [
           0,
           1,
           2,
           3,
           4,
           5,
           6,
           7,
           8,
           9,
           10,
           11,
           12,
           13,
           14,
           15,
           16,
           17,
           18,
           19,
           20,
           21,
           22,
           23,
           24,
           25,
           26,
           27,
           28,
           29,
           30,
           31,
           32,
           33,
           34,
           35,
           36,
           37,
           38,
           39,
           40,
           41,
           42,
           43,
           44,
           45,
           46,
           47,
           48,
           49,
           50,
           51,
           52,
           53,
           54,
           55,
           56,
           57,
           58,
           59,
           60,
           61,
           62,
           63,
           64,
           65,
           66,
           67,
           68,
           69,
           70,
           71,
           72,
           73,
           74,
           75,
           76,
           77,
           78,
           79,
           80,
           81,
           82,
           83,
           84,
           85,
           86,
           87,
           88,
           89,
           90,
           91,
           92,
           93,
           94,
           95,
           96,
           97,
           98,
           99,
           100,
           101,
           102,
           103,
           104,
           105,
           106,
           107,
           108,
           109,
           110,
           111,
           112,
           113,
           114,
           115,
           116,
           117,
           118,
           119,
           120,
           121,
           122,
           123,
           124,
           125,
           126,
           127,
           128,
           129,
           130,
           131,
           132,
           133,
           134,
           135,
           136,
           137,
           138,
           139,
           140,
           141,
           142,
           143,
           144,
           145,
           146,
           147,
           148,
           149,
           150,
           151,
           152,
           153,
           154,
           155,
           156,
           157,
           158,
           159,
           160,
           161,
           162,
           163,
           164,
           165,
           166,
           167,
           168,
           169,
           170,
           171,
           172,
           173,
           174,
           175,
           176,
           177,
           178,
           179,
           180,
           181,
           182,
           183,
           184,
           185,
           186,
           187,
           188,
           189,
           190,
           191,
           192,
           193,
           194,
           195,
           196,
           197,
           198,
           199,
           200,
           201,
           202,
           203,
           204,
           205,
           206,
           207,
           208,
           209,
           210,
           211,
           212,
           213,
           214,
           215,
           216,
           217,
           218,
           219,
           220,
           221,
           222,
           223,
           224,
           225,
           226,
           227,
           228,
           229,
           230,
           231,
           232,
           233,
           234,
           235,
           236,
           237,
           238,
           239,
           240,
           241,
           242,
           243,
           244,
           245,
           246,
           247,
           248,
           249,
           250,
           251,
           252,
           253,
           254,
           255,
           256,
           257,
           258,
           259,
           260,
           261,
           262,
           263,
           264,
           265,
           266,
           267,
           268,
           269,
           270,
           271,
           272,
           273,
           274,
           275,
           276,
           277,
           278,
           279,
           280,
           281,
           282,
           283,
           284,
           285,
           286,
           287,
           288,
           289,
           290,
           291,
           292,
           293,
           294,
           295,
           296,
           297,
           298,
           299,
           300,
           301,
           302,
           303,
           304,
           305,
           306,
           307,
           308,
           309,
           310,
           311,
           312,
           313,
           314,
           315,
           316,
           317,
           318,
           319,
           320,
           321,
           322,
           323,
           324,
           325,
           326,
           327,
           328,
           329,
           330,
           331,
           332,
           333,
           334,
           335,
           336,
           337,
           338,
           339,
           340,
           341,
           342,
           343,
           344,
           345,
           346,
           347,
           348,
           349,
           350,
           351,
           352,
           353,
           354,
           355,
           356,
           357,
           358,
           359,
           360,
           361,
           362,
           363,
           364,
           365,
           366,
           367,
           368,
           369,
           370,
           371,
           372,
           373,
           374,
           375,
           376,
           377,
           378,
           379,
           380,
           381,
           382,
           383,
           384,
           385,
           386,
           387,
           388,
           389,
           390,
           391,
           392,
           393,
           394,
           395,
           396,
           397,
           398,
           399,
           400,
           401,
           402,
           403,
           404,
           405,
           406,
           407,
           408,
           409,
           410,
           411,
           412,
           413,
           414,
           415,
           416,
           417,
           418,
           419,
           420,
           421,
           422,
           423,
           424,
           425,
           426,
           427,
           428,
           429,
           430,
           431,
           432,
           433,
           434,
           435,
           436,
           437,
           438,
           439,
           440,
           441,
           442,
           443,
           444,
           445,
           446,
           447,
           448,
           449,
           450,
           451,
           452,
           453,
           454,
           455,
           456,
           457,
           458,
           459,
           460,
           461,
           462,
           463,
           464,
           465,
           466,
           467,
           468,
           469,
           470,
           471,
           472,
           473,
           474,
           475,
           476,
           477,
           478,
           479,
           480,
           481,
           482,
           483,
           484,
           485,
           486,
           487,
           488,
           489,
           490,
           491,
           492,
           493,
           494,
           495,
           496,
           497,
           498,
           499,
           500,
           501,
           502,
           503,
           504,
           505,
           506,
           507,
           508,
           509,
           510,
           511,
           512,
           513,
           514,
           515,
           516,
           517,
           518,
           519,
           520,
           521,
           522,
           523,
           524,
           525,
           526,
           527,
           528,
           529,
           530,
           531,
           532,
           533,
           534,
           535,
           536,
           537,
           538,
           539,
           540,
           541,
           542,
           543,
           544,
           545,
           546,
           547,
           548,
           549,
           550,
           551,
           552,
           553,
           554,
           555,
           556,
           557,
           558,
           559,
           560,
           561,
           562,
           563,
           564,
           565,
           566,
           567,
           568,
           569,
           570,
           571,
           572,
           573,
           574,
           575,
           576,
           577,
           578,
           579,
           580,
           581,
           582,
           583,
           584,
           585,
           586,
           587,
           588,
           589,
           590,
           591,
           592,
           593,
           594,
           595,
           596,
           597,
           598,
           599,
           600,
           601,
           602,
           603,
           604,
           605,
           606,
           607,
           608,
           609,
           610,
           611,
           612,
           613,
           614,
           615,
           616,
           617,
           618,
           619,
           620,
           621,
           622,
           623,
           624,
           625,
           626,
           627,
           628,
           629,
           630,
           631,
           632,
           633,
           634,
           635,
           636,
           637,
           638,
           639,
           640,
           641,
           642,
           643,
           644,
           645,
           646,
           647,
           648,
           649,
           650,
           651,
           652,
           653,
           654,
           655,
           656,
           657,
           658,
           659,
           660,
           661,
           662,
           663,
           664,
           665,
           666,
           667,
           668,
           669,
           670,
           671,
           672,
           673,
           674,
           675,
           676,
           677,
           678,
           679,
           680,
           681,
           682,
           683,
           684,
           685,
           686,
           687,
           688,
           689,
           690,
           691,
           692,
           693,
           694,
           695,
           696,
           697,
           698,
           699,
           700,
           701,
           702,
           703,
           704,
           705,
           706,
           707,
           708,
           709,
           710,
           711,
           712,
           713,
           714,
           715,
           716,
           717,
           718,
           719,
           720,
           721,
           722,
           723,
           724,
           725,
           726,
           727,
           728,
           729,
           730,
           731,
           732,
           733,
           734,
           735,
           736,
           737,
           738,
           739,
           740,
           741,
           742,
           743,
           744,
           745,
           746,
           747,
           748,
           749,
           750,
           751,
           752,
           753,
           754,
           755,
           756,
           757,
           758,
           759,
           760,
           761,
           762,
           763,
           764,
           765,
           766,
           767,
           768,
           769,
           770,
           771,
           772,
           773,
           774,
           775,
           776,
           777,
           778,
           779,
           780,
           781,
           782,
           783,
           784,
           785,
           786,
           787,
           788,
           789,
           790,
           791,
           792,
           793,
           794,
           795,
           796,
           797,
           798,
           799,
           800,
           801,
           802,
           803,
           804,
           805,
           806,
           807,
           808,
           809,
           810,
           811,
           812,
           813,
           814,
           815,
           816,
           817,
           818,
           819,
           820,
           821,
           822,
           823,
           824,
           825,
           826,
           827,
           828,
           829,
           830,
           831,
           832,
           833,
           834,
           835,
           836,
           837,
           838,
           839,
           840,
           841,
           842,
           843,
           844,
           845,
           846,
           847,
           848,
           849,
           850,
           851,
           852,
           853,
           854,
           855,
           856,
           857,
           858,
           859,
           860,
           861,
           862,
           863,
           864,
           865,
           866,
           867,
           868,
           869,
           870,
           871,
           872,
           873,
           874,
           875,
           876,
           877,
           878,
           879,
           880,
           881,
           882,
           883,
           884,
           885,
           886,
           887,
           888,
           889,
           890,
           891,
           892,
           893,
           894,
           895,
           896,
           897,
           898,
           899,
           900,
           901,
           902,
           903,
           904,
           905,
           906,
           907,
           908,
           909,
           910,
           911,
           912,
           913,
           914,
           915,
           916,
           917,
           918,
           919,
           920,
           921,
           922,
           923,
           924,
           925,
           926,
           927,
           928,
           929,
           930,
           931,
           932,
           933,
           934,
           935,
           936,
           937,
           938,
           939,
           940,
           941,
           942,
           943,
           944,
           945,
           946,
           947,
           948,
           949,
           950,
           951,
           952,
           953,
           954,
           955,
           956,
           957,
           958,
           959,
           960,
           961,
           962,
           963,
           964,
           965,
           966,
           967,
           968,
           969,
           970,
           971,
           972,
           973,
           974,
           975,
           976,
           977,
           978,
           979,
           980,
           981,
           982,
           983,
           984,
           985,
           986,
           987,
           988,
           989,
           990,
           991,
           992,
           993,
           994,
           995,
           996,
           997,
           998,
           999
          ],
          "colorbar": {
           "title": {
            "text": "Trial"
           },
           "x": 1,
           "xpad": 40
          },
          "colorscale": [
           [
            0,
            "rgb(247,251,255)"
           ],
           [
            0.125,
            "rgb(222,235,247)"
           ],
           [
            0.25,
            "rgb(198,219,239)"
           ],
           [
            0.375,
            "rgb(158,202,225)"
           ],
           [
            0.5,
            "rgb(107,174,214)"
           ],
           [
            0.625,
            "rgb(66,146,198)"
           ],
           [
            0.75,
            "rgb(33,113,181)"
           ],
           [
            0.875,
            "rgb(8,81,156)"
           ],
           [
            1,
            "rgb(8,48,107)"
           ]
          ],
          "line": {
           "color": "Grey",
           "width": 0.5
          },
          "showscale": false
         },
         "mode": "markers",
         "name": "Feasible Trial",
         "showlegend": false,
         "type": "scatter",
         "x": [
          425,
          662,
          390,
          911,
          753,
          419,
          684,
          915,
          335,
          195,
          110,
          473,
          518,
          276,
          530,
          460,
          645,
          273,
          587,
          805,
          756,
          853,
          492,
          591,
          975,
          780,
          369,
          310,
          482,
          412,
          423,
          431,
          433,
          215,
          580,
          628,
          569,
          372,
          692,
          716,
          720,
          711,
          700,
          732,
          836,
          626,
          678,
          893,
          667,
          546,
          607,
          731,
          765,
          778,
          831,
          644,
          762,
          894,
          762,
          517,
          861,
          795,
          817,
          800,
          976,
          746,
          693,
          929,
          805,
          859,
          650,
          793,
          786,
          758,
          577,
          726,
          105,
          689,
          771,
          829,
          451,
          577,
          511,
          560,
          536,
          604,
          493,
          745,
          562,
          620,
          667,
          552,
          405,
          393,
          364,
          448,
          708,
          249,
          303,
          391,
          711,
          707,
          709,
          731,
          349,
          740,
          676,
          650,
          688,
          635,
          651,
          704,
          718,
          663,
          654,
          603,
          420,
          697,
          733,
          700,
          642,
          642,
          670,
          624,
          619,
          619,
          601,
          594,
          683,
          617,
          622,
          664,
          675,
          635,
          642,
          674,
          587,
          706,
          605,
          675,
          653,
          633,
          618,
          721,
          640,
          687,
          613,
          132,
          584,
          750,
          751,
          777,
          731,
          727,
          726,
          696,
          695,
          662,
          691,
          698,
          671,
          694,
          700,
          653,
          676,
          707,
          741,
          656,
          638,
          600,
          665,
          683,
          719,
          652,
          628,
          700,
          761,
          681,
          653,
          810,
          684,
          672,
          684,
          638,
          736,
          664,
          618,
          651,
          658,
          632,
          629,
          593,
          608,
          619,
          633,
          567,
          636,
          670,
          659,
          684,
          682,
          637,
          674,
          710,
          687,
          605,
          650,
          660,
          662,
          662,
          695,
          659,
          649,
          686,
          680,
          680,
          672,
          718,
          680,
          670,
          707,
          716,
          698,
          705,
          686,
          699,
          674,
          720,
          686,
          706,
          668,
          685,
          691,
          688,
          688,
          697,
          667,
          684,
          691,
          711,
          685,
          654,
          668,
          675,
          677,
          675,
          706,
          711,
          729,
          701,
          701,
          722,
          674,
          687,
          660,
          742,
          708,
          719,
          703,
          694,
          736,
          674,
          714,
          685,
          692,
          705,
          728,
          682,
          665,
          666,
          748,
          711,
          653,
          694,
          666,
          692,
          716,
          645,
          679,
          731,
          703,
          708,
          665,
          703,
          673,
          727,
          752,
          731,
          722,
          743,
          695,
          716,
          648,
          702,
          758,
          665,
          689,
          646,
          686,
          951,
          666,
          685,
          639,
          680,
          662,
          687,
          660,
          681,
          726,
          641,
          697,
          699,
          718,
          696,
          999,
          669,
          653,
          737,
          773,
          711,
          678,
          694,
          635,
          660,
          731,
          699,
          679,
          696,
          670,
          703,
          679,
          652,
          630,
          712,
          693,
          670,
          686,
          650,
          681,
          685,
          685,
          699,
          720,
          672,
          656,
          693,
          677,
          641,
          714,
          744,
          718,
          672,
          627,
          656,
          713,
          682,
          703,
          674,
          645,
          742,
          722,
          146,
          696,
          667,
          707,
          761,
          712,
          688,
          732,
          740,
          722,
          756,
          729,
          656,
          707,
          671,
          785,
          631,
          701,
          736,
          713,
          655,
          683,
          531,
          671,
          699,
          725,
          703,
          743,
          623,
          697,
          642,
          483,
          718,
          664,
          772,
          687,
          709,
          673,
          249,
          651,
          755,
          733,
          694,
          681,
          710,
          659,
          693,
          640,
          671,
          729,
          705,
          684,
          616,
          719,
          659,
          677,
          696,
          743,
          639,
          714,
          665,
          694,
          678,
          697,
          647,
          729,
          660,
          681,
          709,
          690,
          723,
          749,
          681,
          628,
          883,
          703,
          667,
          685,
          654,
          716,
          696,
          763,
          805,
          790,
          770,
          507,
          746,
          774,
          759,
          737,
          731,
          714,
          759,
          721,
          740,
          709,
          725,
          822,
          707,
          763,
          689,
          728,
          711,
          743,
          785,
          688,
          699,
          676,
          717,
          699,
          680,
          730,
          752,
          706,
          670,
          692,
          643,
          726,
          179,
          678,
          711,
          657,
          694,
          742,
          675,
          708,
          658,
          695,
          725,
          679,
          628,
          712,
          655,
          758,
          766,
          759,
          798,
          754,
          737,
          769,
          776,
          737,
          460,
          792,
          745,
          747,
          731,
          785,
          759,
          724,
          740,
          846,
          719,
          714,
          768,
          600,
          701,
          551,
          744,
          730,
          698,
          575,
          670,
          812,
          715,
          647,
          631,
          649,
          608,
          615,
          639,
          660,
          639,
          669,
          684,
          774,
          646,
          667,
          755,
          800,
          746,
          785,
          758,
          758,
          780,
          739,
          728,
          755,
          720,
          712,
          697,
          688,
          718,
          702,
          684,
          726,
          773,
          713,
          682,
          739,
          702,
          673,
          747,
          696,
          728,
          314,
          668,
          710,
          687,
          656,
          587,
          764,
          721,
          684,
          705,
          621,
          742,
          644,
          672,
          721,
          688,
          658,
          704,
          733,
          705,
          746,
          760,
          719,
          702,
          789,
          725,
          693,
          712,
          737,
          766,
          677,
          694,
          753,
          714,
          671,
          629,
          652,
          665,
          821,
          666,
          647,
          676,
          632,
          778,
          694,
          678,
          729,
          660,
          614,
          707,
          741,
          683,
          798,
          644,
          714,
          695,
          660,
          727,
          679,
          745,
          708,
          693,
          757,
          671,
          641,
          721,
          701,
          663,
          683,
          738,
          708,
          650,
          773,
          728,
          689,
          670,
          755,
          919,
          615,
          713,
          734,
          715,
          752,
          435,
          718,
          702,
          769,
          732,
          214,
          700,
          744,
          708,
          786,
          723,
          690,
          728,
          754,
          517,
          373,
          687,
          714,
          676,
          696,
          591,
          657,
          737,
          638,
          707,
          670,
          774,
          720,
          690,
          745,
          808,
          653,
          705,
          675,
          725,
          628,
          758,
          266,
          690,
          712,
          664,
          686,
          734,
          705,
          677,
          650,
          564,
          747,
          720,
          695,
          668,
          734,
          764,
          689,
          783,
          703,
          628,
          541,
          720,
          655,
          676,
          705,
          744,
          640,
          688,
          727,
          763,
          809,
          783,
          830,
          765,
          797,
          772,
          755,
          758,
          748,
          737,
          779,
          729,
          755,
          719,
          740,
          125,
          713,
          791,
          704,
          736,
          720,
          769,
          698,
          752,
          712,
          684,
          868,
          603,
          732,
          697,
          751,
          721,
          667,
          689,
          705,
          735,
          769,
          808,
          798,
          791,
          765,
          775,
          790,
          765,
          750,
          783,
          752,
          770,
          731,
          751,
          839,
          737,
          777,
          725,
          711,
          821,
          742,
          766,
          716,
          697,
          751,
          497,
          651,
          683,
          718,
          666,
          732,
          806,
          700,
          762,
          783,
          332,
          674,
          713,
          733,
          688,
          618,
          640,
          749,
          702,
          665,
          720,
          692,
          741,
          658,
          681,
          768,
          715,
          704,
          731,
          792,
          682,
          647,
          754,
          403,
          699,
          716,
          673,
          738,
          630,
          659,
          763,
          693,
          726,
          576,
          779,
          705,
          674,
          744,
          716,
          689,
          654,
          722,
          609,
          757,
          965,
          679,
          707,
          733,
          687,
          797,
          641,
          668,
          700,
          745,
          758,
          779,
          776,
          817,
          744,
          158,
          767,
          750,
          747,
          732,
          658,
          790,
          630,
          706,
          726,
          768,
          679,
          688,
          660,
          714,
          742,
          696,
          644,
          799,
          761,
          673,
          735,
          717,
          703,
          778,
          600,
          682,
          751,
          727,
          662,
          700,
          631,
          719,
          683,
          740,
          760,
          666,
          702,
          654,
          726,
          694,
          784,
          472,
          680,
          712,
          751,
          807,
          645,
          729,
          615,
          671,
          766,
          697,
          712,
          738,
          687,
          711,
          667,
          740,
          774,
          647,
          691,
          723,
          751,
          677,
          705,
          732,
          529,
          660,
          627,
          699,
          788,
          718,
          766,
          767,
          821,
          806,
          787,
          793,
          797,
          840,
          834,
          863,
          857,
          815,
          787,
          836,
          805,
          840,
          889,
          828,
          823,
          816,
          866,
          845,
          841,
          827,
          820,
          828,
          813,
          884,
          844,
          805,
          832,
          795,
          810,
          865,
          853,
          873,
          793,
          852,
          900,
          824,
          786,
          785,
          794,
          770,
          806,
          837,
          766,
          828,
          772,
          802,
          781,
          935,
          851,
          765,
          819,
          791,
          776,
          761,
          813,
          750,
          758,
          776,
          778,
          813,
          789,
          838,
          810,
          802,
          772,
          798,
          826,
          779,
          758,
          780,
          874,
          854,
          798,
          750,
          768,
          746,
          799
         ],
         "xaxis": "x4",
         "y": [
          0.08539019019252586,
          0.09495197683593737,
          0.08561203246811563,
          0.10226171316013058,
          0.09436835513192324,
          0.08442866030728638,
          0.0882806271297641,
          0.09259418891274053,
          0.0936506610503796,
          0.08905616282810194,
          0.10070957535224619,
          0.0849354185113468,
          0.09538829459268595,
          0.08532982608602123,
          0.09802117955542877,
          0.08861113027563677,
          0.08782249655960989,
          0.09671218874633744,
          0.10056915412131459,
          0.08463494265724372,
          0.08550715408794303,
          0.08624021489163447,
          0.08421050029548485,
          0.0894877666245822,
          0.08522084111774414,
          0.08664087579640814,
          0.09277247714187424,
          0.08890925915171902,
          0.08679846383946271,
          0.08434924485492348,
          0.08332985614930138,
          0.08381487607339143,
          0.08381121898840153,
          0.08492498104511295,
          0.08351886332682175,
          0.1065370630575128,
          0.08407738487249838,
          0.10418987847943002,
          0.08315042884967586,
          0.08360267337033281,
          0.08344470964379247,
          0.08381392901444998,
          0.0857432511381672,
          0.08371432932413264,
          0.08615883393220121,
          0.08428574598953223,
          0.0897804337144529,
          0.10368442923134472,
          0.08419051036454686,
          0.08666228717253156,
          0.08539823364234111,
          0.08398322451750445,
          0.08324196548934296,
          0.08498755749166624,
          0.08592689223502362,
          0.08643366899645041,
          0.0835047463204986,
          0.09288435334111321,
          0.08424549859482734,
          0.08502893317695907,
          0.0890072310285759,
          0.08354751438906698,
          0.08481137014593011,
          0.08285306771082904,
          0.08435363262967716,
          0.08440017469452545,
          0.10234792567968147,
          0.08439105115445075,
          0.08367204082049616,
          0.08674702124077736,
          0.08364722764310903,
          0.08339834509471708,
          0.08458083145212651,
          0.08730137187390588,
          0.08347106168854283,
          0.10497466632612777,
          0.09500932690462209,
          0.08627061663570071,
          0.08361465461377242,
          0.0862346237864177,
          0.08576719136060315,
          0.08392691309042451,
          0.08667835916016219,
          0.0832016218464397,
          0.08360618571881825,
          0.10570571526403402,
          0.08912974625484643,
          0.08794829675893581,
          0.10369587843510582,
          0.08450059575739881,
          0.08564046243395204,
          0.08425998493872724,
          0.08328057039506313,
          0.08414219287159663,
          0.08939263864603345,
          0.0846428294515632,
          0.08165961576392188,
          0.08488512104472777,
          0.0864821733201315,
          0.0844953596275288,
          0.08312414300199703,
          0.08302348043185244,
          0.08349016143148591,
          0.08273551529377218,
          0.08712664607608565,
          0.08335508002367598,
          0.0833304450341904,
          0.08274977982394807,
          0.08723091108735959,
          0.10129893784474617,
          0.08328120589468982,
          0.08518771642746378,
          0.08417473164618214,
          0.08544436252688223,
          0.084259756018591,
          0.08477727688870212,
          0.0843732621596494,
          0.08238179744888834,
          0.0835201737237383,
          0.08440188316439623,
          0.0826963136261387,
          0.08325320855427412,
          0.08232280947837918,
          0.08313899416292488,
          0.0827606041580992,
          0.08345781219298537,
          0.082856239988547,
          0.08315158199964165,
          0.09996173825297669,
          0.08287132257374849,
          0.08399392866757854,
          0.08329801970738117,
          0.08218916379001612,
          0.08272626181044221,
          0.08356949324059257,
          0.08344809825525434,
          0.08412450105439928,
          0.08436250442465335,
          0.08543737126852116,
          0.08315415930339068,
          0.08328874654472859,
          0.08377063932211924,
          0.08374662836316327,
          0.08432312077904969,
          0.08334078110772783,
          0.10451366567669003,
          0.08396547011835669,
          0.09866471516554136,
          0.0840355188769745,
          0.08313315216040826,
          0.0833273896064651,
          0.08380846367977608,
          0.08301209213271031,
          0.08302693484535764,
          0.08513153125053821,
          0.08294988420046369,
          0.0828023224844447,
          0.08576151350462309,
          0.08313593893209155,
          0.0823890790084934,
          0.10233081625126154,
          0.08391457740581108,
          0.08346778197338994,
          0.08323672801444174,
          0.11016564627256034,
          0.08625986833194307,
          0.08308838128817571,
          0.08256675863644616,
          0.08687355439209934,
          0.08320375301071112,
          0.0831569856636962,
          0.0830861335286607,
          0.08373209848835807,
          0.08325772728529475,
          0.0840249429842295,
          0.08340173491254706,
          0.0841523490888042,
          0.08278875829740572,
          0.08588354015186887,
          0.08460117799571931,
          0.08271264948133934,
          0.08311842459534399,
          0.09775820704966276,
          0.08363667644538213,
          0.08518966582885326,
          0.08234088467720187,
          0.08300663028468404,
          0.08294792493806535,
          0.09374333088296935,
          0.08186637706985608,
          0.08286524474377754,
          0.08407084466100902,
          0.08513404749620522,
          0.08512809066960167,
          0.08317299290426386,
          0.08329165608201898,
          0.09249173931498986,
          0.08581289239318927,
          0.08311832057969164,
          0.08244091747412202,
          0.0859276097402228,
          0.08294365575003566,
          0.08291915631246957,
          0.08431553209465299,
          0.08316980756401962,
          0.08275119431371236,
          0.08259810148865966,
          0.08269710428895796,
          0.0825306928188557,
          0.08327358623423534,
          0.08337420215674948,
          0.10445665012752847,
          0.08334076815055066,
          0.08274338024429977,
          0.08251096914423196,
          0.08199902606831211,
          0.08235772891596732,
          0.08293553119582622,
          0.08253509322945916,
          0.08376735221276418,
          0.08242039156277536,
          0.08310408342648781,
          0.08236920723797173,
          0.08291592089843107,
          0.08237535912990515,
          0.08328242745865577,
          0.08330519127524712,
          0.08333766438105057,
          0.08348678243602832,
          0.08346972442740377,
          0.08291621932822964,
          0.08231996370906212,
          0.08231519235453469,
          0.08261364924345689,
          0.0825212172002789,
          0.08282661754470844,
          0.08280000274572497,
          0.0822068912312609,
          0.08306979246734976,
          0.08280936601250934,
          0.08323729894382537,
          0.0827596216791415,
          0.08254312932582486,
          0.08260706905635237,
          0.08219159376324617,
          0.08332867404127005,
          0.08245010521755164,
          0.0830015532855319,
          0.08311543440998431,
          0.08354447071482189,
          0.08459523466839948,
          0.08279362737753312,
          0.0832079959920258,
          0.08271413443324369,
          0.08228669392391635,
          0.08746037507141037,
          0.08244453094845948,
          0.08267614433778225,
          0.08361087104719155,
          0.08254682461025294,
          0.08315199608854278,
          0.0831869149479296,
          0.0831621325240794,
          0.08241511203722858,
          0.08509378854913294,
          0.09446901061125146,
          0.0829485186897416,
          0.08256420848681219,
          0.08241588768524585,
          0.0832236338954678,
          0.08285732035700681,
          0.08516721649864492,
          0.08291222919437333,
          0.08335991669751068,
          0.08242883783270984,
          0.08266547354134668,
          0.08538467882794278,
          0.09503155262312986,
          0.08288635613635478,
          0.08264974876794147,
          0.08211371813595908,
          0.08657470992634171,
          0.08259439090255191,
          0.08312408247804894,
          0.08297372789394555,
          0.0824331510535781,
          0.08637316625377117,
          0.10319183305913875,
          0.08404118132533886,
          0.08263370202318032,
          0.0834927133525499,
          0.08558875960099446,
          0.0825962951162357,
          0.08525988219355811,
          0.08491634346648637,
          0.08277226913673,
          0.0821336454759914,
          0.08324338368912149,
          0.08389344305054196,
          0.0848766197803248,
          0.0828420257148326,
          0.08199056700519247,
          0.09088192756881397,
          0.08291160072327894,
          0.1049394814041883,
          0.0836179953257,
          0.08265678691618213,
          0.08329057337357557,
          0.08586042141522317,
          0.0831940183020074,
          0.08226786937139183,
          0.08258516295466148,
          0.08418431503900826,
          0.08378835210536532,
          0.08655320541884155,
          0.08267759915260924,
          0.10230479617375414,
          0.08253292069688178,
          0.08360373450105948,
          0.08306322233093005,
          0.08297395306054933,
          0.08502468501565282,
          0.08294150522200303,
          0.08291127575946386,
          0.0845033398062486,
          0.08229269394206133,
          0.08319980298285114,
          0.08355104320136941,
          0.08214051827823698,
          0.08368642417592195,
          0.10056914324523544,
          0.08742187178093777,
          0.10750592060617267,
          0.08342305906642027,
          0.08514749289202753,
          0.08306091591721386,
          0.08237992365506012,
          0.08504635372532161,
          0.08217331987854679,
          0.08579425456448286,
          0.08467105207276104,
          0.08242706667986897,
          0.08495171802357053,
          0.08436796376240073,
          0.08324510960531548,
          0.08348012821167246,
          0.08222864482506259,
          0.08360830242929047,
          0.08218972123363966,
          0.0846592336480432,
          0.08296534942665079,
          0.08385594691626566,
          0.08428881746489056,
          0.08514809023965,
          0.08517640762635152,
          0.08374766448669568,
          0.08346478111354153,
          0.08345120363017818,
          0.08341290855897499,
          0.08363018486181899,
          0.08310301346123908,
          0.09272379644340216,
          0.08378574735051028,
          0.0852733325696883,
          0.08216595188325272,
          0.10378265171249632,
          0.08294003021589308,
          0.08326020405580013,
          0.08228238847757531,
          0.08428079796872202,
          0.08311972393512462,
          0.08338968730295256,
          0.08293213450970785,
          0.09393946362590899,
          0.08327466350181753,
          0.08506553342131629,
          0.0826984353792733,
          0.08311464356185652,
          0.08507777430539948,
          0.10664077240786737,
          0.08308505363475249,
          0.0840521759461056,
          0.0984829737053229,
          0.08409671431676492,
          0.08343552651457424,
          0.08218376867519202,
          0.0826176725634861,
          0.087404024246948,
          0.0824579868517901,
          0.08343538851576185,
          0.08632526446250707,
          0.10113468346246568,
          0.08408297959141789,
          0.08455967169326965,
          0.08290135112828734,
          0.08622599585167161,
          0.08256680193136179,
          0.08279304994854082,
          0.08298208106969547,
          0.086487997927399,
          0.08451040655479528,
          0.08312387237224324,
          0.08279913946855727,
          0.08525800338496867,
          0.08306589903344415,
          0.08263009693292171,
          0.08444546632016169,
          0.08282584835510597,
          0.08275273275273365,
          0.08324783184800626,
          0.08293670250894221,
          0.08291566242729534,
          0.08471320065483626,
          0.08267044541148255,
          0.08447697754338505,
          0.08390700147066862,
          0.08333214843043357,
          0.0828179712937066,
          0.08299945475657355,
          0.08281322887122904,
          0.08395017481762637,
          0.08241984428417812,
          0.08213717563466298,
          0.10076673879446381,
          0.0832436787503004,
          0.08252665324010452,
          0.08320587493050441,
          0.08776937049652797,
          0.08227879636452218,
          0.08310102230104396,
          0.08312565513737012,
          0.08321513334889388,
          0.08299056674606672,
          0.08303414146151361,
          0.083011182298674,
          0.08365498360084109,
          0.08588668701466871,
          0.08384000289415564,
          0.0833124264339247,
          0.08238305561835071,
          0.09203839346356635,
          0.08349258535191846,
          0.08215443768884524,
          0.08349214873884861,
          0.08403318361265764,
          0.0824595433892342,
          0.08413298882527163,
          0.08435908750491991,
          0.08767755459365653,
          0.0827298801062617,
          0.09582911899764356,
          0.08268836609716766,
          0.08217887190324438,
          0.08456203431506822,
          0.08395906740770928,
          0.08721133673682199,
          0.08326946965557938,
          0.08277857338975389,
          0.08516307424345448,
          0.08874462697002515,
          0.08388595508820527,
          0.10162837463997096,
          0.08349279022803455,
          0.08273657163673434,
          0.08376690305749647,
          0.08253757560419685,
          0.10402306282405352,
          0.08277068997162221,
          0.08764543348113721,
          0.08260209306314711,
          0.08246848587072822,
          0.08308283039199962,
          0.08247267673486194,
          0.09189560105049469,
          0.08462355983006513,
          0.08329194609971924,
          0.08316037595639489,
          0.08317869616338323,
          0.08316144874053474,
          0.08959409565848214,
          0.08402716720523631,
          0.08400077230926636,
          0.0829619142270787,
          0.08283674928845328,
          0.0847778461277469,
          0.08344920849250995,
          0.08523696176791473,
          0.08725620307003272,
          0.10397492307026904,
          0.08595226907323969,
          0.09921985101954633,
          0.09533855013187104,
          0.08309507191276091,
          0.0826759786006139,
          0.08225593586524416,
          0.08740274630438884,
          0.08315660205916471,
          0.08608837841090153,
          0.08467202290989553,
          0.097137117730765,
          0.08275036333514421,
          0.08386434363894248,
          0.08188325999628746,
          0.08557863261436732,
          0.08446231710143276,
          0.08368820469053169,
          0.08285472367859309,
          0.08303516672857214,
          0.0833906914083489,
          0.08366981604698515,
          0.08793276147551224,
          0.08952814150800537,
          0.08308096878529299,
          0.08256381892015496,
          0.08317662056064302,
          0.08710183821325071,
          0.0834428219383049,
          0.0830258565559016,
          0.08355652692921657,
          0.08424998322924931,
          0.08334819052906109,
          0.08573110756005994,
          0.08488722805270349,
          0.08380901110932767,
          0.09618439324678967,
          0.0826032479888251,
          0.0820830552828102,
          0.0862818168369496,
          0.08401134731429585,
          0.08295871971481268,
          0.08240986881096399,
          0.0824353229340784,
          0.08462405213082154,
          0.08262857330685547,
          0.09748615573746822,
          0.08613884514448175,
          0.08814720651422511,
          0.0824457607071543,
          0.08331839738519489,
          0.0821804083798195,
          0.08350042182975628,
          0.08311370306065799,
          0.08886475197480367,
          0.08374134130419003,
          0.08466248680311633,
          0.083197319026209,
          0.0870576233729369,
          0.10433184556727466,
          0.08395723755226109,
          0.08220024890163888,
          0.08265994797689089,
          0.10548729044772294,
          0.08270016460923824,
          0.08263693091573175,
          0.08408285162449515,
          0.08247965927491127,
          0.08510520859519778,
          0.08393599427281197,
          0.08298244561042319,
          0.08562942857561037,
          0.08615259273943957,
          0.08253164483660641,
          0.0834048056413013,
          0.0836444464588196,
          0.08644759051217846,
          0.10161932762356388,
          0.0860720895428612,
          0.08284569478174883,
          0.08371538532284166,
          0.08264559635056794,
          0.08284416313727293,
          0.08391010104815012,
          0.08260172216704732,
          0.08581525248878688,
          0.08305984930644876,
          0.08274444052530283,
          0.08488695733354572,
          0.08243008456223479,
          0.0829948282257483,
          0.08495372244600471,
          0.08321977121587064,
          0.08635692506489748,
          0.08264673692125761,
          0.08186846040844238,
          0.08263059881485445,
          0.10441472501698376,
          0.10528485609977607,
          0.08499971771788298,
          0.09259959919221114,
          0.08261318952776894,
          0.08385959899234947,
          0.0826226315654416,
          0.08258107343944868,
          0.08331235626742295,
          0.0834970814550906,
          0.08322184354859442,
          0.08366762990690224,
          0.08232107545407749,
          0.08502953198831671,
          0.08469003944748557,
          0.08222412784889119,
          0.08312009935117741,
          0.08571363180020228,
          0.08237007825013624,
          0.0835774816385849,
          0.08347237759742218,
          0.08308469365588271,
          0.08238220838082617,
          0.08422243198098708,
          0.08466688799410906,
          0.08329245959958595,
          0.0831207423118143,
          0.08315877480983573,
          0.0852580342864113,
          0.08307430873764746,
          0.08793018395955414,
          0.0830730472577487,
          0.08393387458472819,
          0.08540937207454335,
          0.08348777746597265,
          0.08265909677459007,
          0.08387453848047574,
          0.08635036092591931,
          0.0829349762863024,
          0.08241289957264089,
          0.10413855843091069,
          0.08253266605259721,
          0.08648364062869808,
          0.08191460734977042,
          0.08310340315683169,
          0.0829548528835559,
          0.0864318137649662,
          0.0829293169593892,
          0.10305693039776236,
          0.08356895669343908,
          0.08244647842569405,
          0.08415434846452788,
          0.0830010827587181,
          0.08438485467873562,
          0.08223489665962366,
          0.0827530514167065,
          0.08357849234207144,
          0.08273626391953012,
          0.08404602275072952,
          0.08231460044689605,
          0.08210942459292067,
          0.08321989252062911,
          0.08319843094976215,
          0.08535502837758135,
          0.083684048231146,
          0.0829440056016863,
          0.08306201265224991,
          0.08321963632160116,
          0.08523513451119122,
          0.08957199257037188,
          0.10218071104046678,
          0.08365996670182504,
          0.08288414315622594,
          0.0831609409532332,
          0.08563748131796066,
          0.08267228190331319,
          0.08351072308999888,
          0.08595073621997369,
          0.0830822013310413,
          0.09276760202112158,
          0.08320761930510705,
          0.08320375706140715,
          0.087980450153498,
          0.08257120222868822,
          0.08382900630949686,
          0.08347497757570743,
          0.09168496044283138,
          0.08287356761286935,
          0.09610522792691246,
          0.0840937911692142,
          0.08320281863107817,
          0.0840821741688485,
          0.08296261071055012,
          0.08702358037168506,
          0.08304670690396758,
          0.08282960813412675,
          0.08504832917070236,
          0.08280821131692603,
          0.08357559868071203,
          0.10030268846476795,
          0.0830115284099357,
          0.08538684115689459,
          0.08316314519172648,
          0.08674664611182713,
          0.0833060353078631,
          0.08276759290831337,
          0.082636621675815,
          0.08519768869200897,
          0.08343585614273649,
          0.08348041210697296,
          0.08479695608461686,
          0.10471228685959055,
          0.08411816094404848,
          0.08253897607087386,
          0.08282121749834048,
          0.08301176176181067,
          0.08422872819064456,
          0.08248307926521628,
          0.0873169601423071,
          0.08403867263588274,
          0.08360671456693249,
          0.0873387381930901,
          0.08303636122381383,
          0.08429013910580663,
          0.08245037943250491,
          0.08731979299476997,
          0.0831649244330491,
          0.08299715585650622,
          0.08383338746534745,
          0.08300374574868034,
          0.08221742281473433,
          0.08328189460011386,
          0.08380170444242878,
          0.08724155313411035,
          0.08266658783360512,
          0.0827461062704319,
          0.08366220360416872,
          0.08926097460933158,
          0.0820128908958929,
          0.08239676192015702,
          0.08335814330410822,
          0.08271122899355499,
          0.09320347156581618,
          0.08244578693077234,
          0.08455687464846962,
          0.0827144043953543,
          0.09735407733563475,
          0.08311742873296009,
          0.08253193283787222,
          0.08503616705876,
          0.08229462409623334,
          0.10227328262969504,
          0.08244098270982371,
          0.08458546705476075,
          0.08245023856627462,
          0.08390096654345602,
          0.08261727710396506,
          0.08296341485333819,
          0.0843856743061751,
          0.0826403264007882,
          0.08236202033659863,
          0.08491342896501539,
          0.08249553332142545,
          0.08248501382813546,
          0.08325999242968649,
          0.08317749236739873,
          0.08683336062238954,
          0.0816765138371928,
          0.08259875051057868,
          0.08402034285954971,
          0.08252324178034955,
          0.09427206101801941,
          0.08361224594033768,
          0.08780454576834186,
          0.08306918463313627,
          0.08294936107813836,
          0.08226373331746348,
          0.08270158075217962,
          0.09324708168090719,
          0.08375970454230489,
          0.08278135784160373,
          0.08603363354510356,
          0.1045542831789172,
          0.0826534965564436,
          0.08254708094217993,
          0.08678866035047919,
          0.08257299826086026,
          0.10524439261200529,
          0.08462142450028,
          0.09693374076406427,
          0.10746782345354618,
          0.0865524613434759,
          0.08328747282232361,
          0.08874189978266406,
          0.08327021589551106,
          0.09380548783775086,
          0.08239314017106542,
          0.08329407297801958,
          0.08299425059418443,
          0.08576349535115597,
          0.0833380400574528,
          0.08329158468354203,
          0.09177749989897299,
          0.08283706581670945,
          0.08460100921379025,
          0.08298000691791643,
          0.0826500350927559,
          0.08338863250547998,
          0.08284291699085977,
          0.08353928226982023,
          0.0828191079128297,
          0.08425831915679907,
          0.08394784695156927,
          0.1048112802167831,
          0.08349310994313272,
          0.08527538963067192,
          0.08234989201226825,
          0.08602489920911367,
          0.08372643663163837,
          0.08333347635755216,
          0.0831372896782915,
          0.08252173787708882,
          0.0830012992386513,
          0.08352099809410667,
          0.08484631339069673,
          0.08376725637774293,
          0.08248257995669252,
          0.10291475756183441,
          0.08266324199933343,
          0.08417132121165083,
          0.08288839769821313,
          0.08226414886960809,
          0.10099639770365246,
          0.08272441754739207,
          0.08387966155676363,
          0.09299240361592506,
          0.08444045941099101,
          0.0834123863076146,
          0.08412272761003801,
          0.08446355118849863,
          0.08334290321166601,
          0.08245797628427717,
          0.09368903580642896,
          0.08579221761627058,
          0.0833885768570585,
          0.08239055214740282,
          0.08249961918811777,
          0.08306052813844458,
          0.0831357568439862,
          0.08409036395262161,
          0.08337417185758891,
          0.0829387506742927,
          0.08260896282855443,
          0.08498237910796849,
          0.08300392716098079,
          0.08195714197156158,
          0.08418743298567412,
          0.10038129945318305,
          0.08327494977087521,
          0.08373973990748947,
          0.08308164190378595,
          0.09100211356259427,
          0.08563965910037585,
          0.08583124169737358,
          0.08392212825027276,
          0.08279077028629671,
          0.08236456924978328,
          0.10629128661382152,
          0.08250091564787407,
          0.08713988911407138,
          0.08433474922626143,
          0.0826654757872344,
          0.08395989014513086,
          0.08574106211056873,
          0.08285930018085841,
          0.08299692922839406,
          0.08441498679660339,
          0.08389846280538288,
          0.0848405257616334,
          0.0834616221668075,
          0.08584030665022686,
          0.08322891696303439,
          0.08353011074735918,
          0.08309285294850441,
          0.08651283933196181,
          0.08310693915881492,
          0.08290063321512871,
          0.08320239035774854,
          0.08601050254872308,
          0.08393616543478491,
          0.08260553336789768,
          0.085613102481566,
          0.0825841065356284,
          0.08324735921670097,
          0.08332004573388377,
          0.0931690848266701,
          0.0859662930507487,
          0.08373161134596022,
          0.08301017264797066,
          0.08336194886702872,
          0.08255570842720951,
          0.08764987998468635,
          0.08336795546281142,
          0.08398129766152383,
          0.0848594980991578,
          0.08266155455990679,
          0.08247719720819359,
          0.08374440729884888,
          0.08310656364763773,
          0.10226158936330351,
          0.09595609196563701,
          0.08276011433173194,
          0.08717349665672955,
          0.08246932495623632,
          0.08319389051401872,
          0.0824771668600773,
          0.08573726649146093,
          0.08725387877287781,
          0.08304789224788486,
          0.08434879957739697,
          0.08363829132457859,
          0.10460715119245341,
          0.08220427027112753,
          0.08271754200267924,
          0.08689833135454574,
          0.0834464881164965,
          0.08562707073378686,
          0.08321486883375731,
          0.08404642113923066,
          0.08519422596037746,
          0.08574035101850674,
          0.08260114456740034,
          0.08226617978994574,
          0.09218102338526904,
          0.08182862624074039,
          0.08255383901034316,
          0.08343164795439631,
          0.09953847346653258,
          0.08232191407943026,
          0.0817069837129533,
          0.0867796506853938,
          0.08460698583192258,
          0.0817647500281331,
          0.08327299992035239,
          0.08248162883909414,
          0.0840224012952457,
          0.08418530669104944,
          0.087973293685031,
          0.08268698123591149,
          0.08437574712624792,
          0.08327755337916884,
          0.08214275629720465,
          0.08414232814750208,
          0.08329582745550315,
          0.08720531957157558,
          0.08479025950591201,
          0.08275803518911087,
          0.08291628786089525,
          0.08393236306890406,
          0.10079997009926882,
          0.08716367263376307,
          0.08492904714119187,
          0.08386324725465437,
          0.08271281080959585,
          0.08308604866601085,
          0.0834212191163988,
          0.08595553137476505,
          0.08234046626972627,
          0.08227295477044481,
          0.08406219882785552,
          0.08224580632195619,
          0.08345248633800766,
          0.08819583941980899,
          0.08470703788496611,
          0.08301319642691471,
          0.08769757652837515,
          0.0843761094860751,
          0.08335234029658274,
          0.08313066301315937,
          0.0833117535252543,
          0.08264534952309555,
          0.08355628770407349,
          0.08442672436586694,
          0.08236143286177203,
          0.083264741051013,
          0.08688689989921365,
          0.08449092200401603,
          0.08576533320175621,
          0.08308582311144246,
          0.09109043997091111,
          0.08254199390917345,
          0.0821983819056997,
          0.08240965082024795,
          0.08636383225110883,
          0.10300281038002383,
          0.08214853769173854,
          0.08268008983563277,
          0.08370669105344264,
          0.08268520080035745,
          0.08279714856339851,
          0.08713031875136815,
          0.08250872961798147,
          0.08243085229276693,
          0.08238836573806105,
          0.0825027710727266,
          0.08298343614744504,
          0.08356331086152853,
          0.09050036013864263,
          0.08511173574036432,
          0.08341313343305887,
          0.08258826362434266,
          0.0829828554386322,
          0.08276858925895582,
          0.08433108819297482,
          0.08237576200573207
         ],
         "yaxis": "y4"
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Slice Plot"
        },
        "width": 1200,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.2125
         ],
         "title": {
          "text": "learning_rate"
         },
         "type": "log"
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.2625,
          0.475
         ],
         "title": {
          "text": "max_depth"
         }
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0.525,
          0.7375
         ],
         "title": {
          "text": "min_child_weight"
         }
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0.7875,
          1
         ],
         "title": {
          "text": "n_estimators"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Objective Value"
         }
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0,
          1
         ],
         "matches": "y",
         "showticklabels": false
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0,
          1
         ],
         "matches": "y",
         "showticklabels": false
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0,
          1
         ],
         "matches": "y",
         "showticklabels": false
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optuna.visualization.plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9848aa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3c0f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_n_estimators = best_params['n_estimators']\n",
    "best_max_depth = best_params['max_depth']\n",
    "best_learning_rate = best_params['learning_rate']\n",
    "best_min_child_weight = best_params['min_child_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a80b7ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = XGBRegressor(\n",
    "    n_estimators=best_n_estimators,\n",
    "    max_depth=best_max_depth,\n",
    "    learning_rate=best_learning_rate,\n",
    "    min_child_weight=best_min_child_weight,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8de99a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "}\n",
       "\n",
       "#sk-container-id-2.light {\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: black;\n",
       "  --sklearn-color-background: white;\n",
       "  --sklearn-color-border-box: black;\n",
       "  --sklearn-color-icon: #696969;\n",
       "}\n",
       "\n",
       "#sk-container-id-2.dark {\n",
       "  --sklearn-color-text-on-default-background: white;\n",
       "  --sklearn-color-background: #111;\n",
       "  --sklearn-color-border-box: white;\n",
       "  --sklearn-color-icon: #878787;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: center;\n",
       "  justify-content: center;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-3) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  border: var(--sklearn-color-fitted-level-0) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-0);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  border: var(--sklearn-color-fitted-level-0) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-0);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table {\n",
       "    font-family: monospace;\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table summary::marker {\n",
       "    font-size: 0.7rem;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "    margin-top: 0;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       "/*\n",
       "    `table td`is set in notebook with right text-align.\n",
       "    We need to overwrite it.\n",
       "*/\n",
       ".estimator-table table td.param {\n",
       "    text-align: left;\n",
       "    position: relative;\n",
       "    padding: 0;\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left !important;\n",
       "}\n",
       "\n",
       ".user-set td.value {\n",
       "    color:rgb(255, 94, 0);\n",
       "    background-color: transparent;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left !important;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       "/*\n",
       "    Styles for parameter documentation links\n",
       "    We need styling for visited so jupyter doesn't overwrite it\n",
       "*/\n",
       "a.param-doc-link,\n",
       "a.param-doc-link:link,\n",
       "a.param-doc-link:visited {\n",
       "    text-decoration: underline dashed;\n",
       "    text-underline-offset: .3em;\n",
       "    color: inherit;\n",
       "    display: block;\n",
       "    padding: .5em;\n",
       "}\n",
       "\n",
       "/* \"hack\" to make the entire area of the cell containing the link clickable */\n",
       "a.param-doc-link::before {\n",
       "    position: absolute;\n",
       "    content: \"\";\n",
       "    inset: 0;\n",
       "}\n",
       "\n",
       ".param-doc-description {\n",
       "    display: none;\n",
       "    position: absolute;\n",
       "    z-index: 9999;\n",
       "    left: 0;\n",
       "    padding: .5ex;\n",
       "    margin-left: 1.5em;\n",
       "    color: var(--sklearn-color-text);\n",
       "    box-shadow: .3em .3em .4em #999;\n",
       "    width: max-content;\n",
       "    text-align: left;\n",
       "    max-height: 10em;\n",
       "    overflow-y: auto;\n",
       "\n",
       "    /* unfitted */\n",
       "    background: var(--sklearn-color-unfitted-level-0);\n",
       "    border: thin solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       "/* Fitted state for parameter tooltips */\n",
       ".fitted .param-doc-description {\n",
       "    /* fitted */\n",
       "    background: var(--sklearn-color-fitted-level-0);\n",
       "    border: thin solid var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".param-doc-link:hover .param-doc-description {\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             feature_weights=None, gamma=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=0.1305406029660337, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "             min_child_weight=2, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=708, n_jobs=None,\n",
       "             num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>XGBRegressor</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor\">?<span>Documentation for XGBRegressor</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('objective',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=objective,-typing.Union%5Bstr%2C%20xgboost.sklearn._SklObjWProto%2C%20typing.Callable%5B%5Btyping.Any%2C%20typing.Any%5D%2C%20typing.Tuple%5Bnumpy.ndarray%2C%20numpy.ndarray%5D%5D%2C%20NoneType%5D\">\n",
       "            objective\n",
       "            <span class=\"param-doc-description\">objective: typing.Union[str, xgboost.sklearn._SklObjWProto, typing.Callable[[typing.Any, typing.Any], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]<br><br>Specify the learning task and the corresponding learning objective or a custom<br>objective function to be used.<br><br>For custom objective, see :doc:`/tutorials/custom_metric_obj` and<br>:ref:`custom-obj-metric` for more information, along with the end note for<br>function signatures.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;reg:squarederror&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('base_score',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=base_score,-typing.Union%5Bfloat%2C%20typing.List%5Bfloat%5D%2C%20NoneType%5D\">\n",
       "            base_score\n",
       "            <span class=\"param-doc-description\">base_score: typing.Union[float, typing.List[float], NoneType]<br><br>The initial prediction score of all instances, global bias.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('booster',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">booster</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('callbacks',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=callbacks,-typing.Optional%5Btyping.List%5Bxgboost.callback.TrainingCallback%5D%5D\">\n",
       "            callbacks\n",
       "            <span class=\"param-doc-description\">callbacks: typing.Optional[typing.List[xgboost.callback.TrainingCallback]]<br><br>List of callback functions that are applied at end of each iteration.<br>It is possible to use predefined callbacks by using<br>:ref:`Callback API <callback_api>`.<br><br>.. note::<br><br>   States in callback are not preserved during training, which means callback<br>   objects can not be reused for multiple training sessions without<br>   reinitialization or deepcopy.<br><br>.. code-block:: python<br><br>    for params in parameters_grid:<br>        # be sure to (re)initialize the callbacks before each run<br>        callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]<br>        reg = xgboost.XGBRegressor(**params, callbacks=callbacks)<br>        reg.fit(X, y)</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('colsample_bylevel',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=colsample_bylevel,-typing.Optional%5Bfloat%5D\">\n",
       "            colsample_bylevel\n",
       "            <span class=\"param-doc-description\">colsample_bylevel: typing.Optional[float]<br><br>Subsample ratio of columns for each level.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('colsample_bynode',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=colsample_bynode,-typing.Optional%5Bfloat%5D\">\n",
       "            colsample_bynode\n",
       "            <span class=\"param-doc-description\">colsample_bynode: typing.Optional[float]<br><br>Subsample ratio of columns for each split.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('colsample_bytree',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=colsample_bytree,-typing.Optional%5Bfloat%5D\">\n",
       "            colsample_bytree\n",
       "            <span class=\"param-doc-description\">colsample_bytree: typing.Optional[float]<br><br>Subsample ratio of columns when constructing each tree.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('device',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=device,-typing.Optional%5Bstr%5D\">\n",
       "            device\n",
       "            <span class=\"param-doc-description\">device: typing.Optional[str]<br><br>.. versionadded:: 2.0.0<br><br>Device ordinal, available options are `cpu`, `cuda`, and `gpu`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('early_stopping_rounds',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=early_stopping_rounds,-typing.Optional%5Bint%5D\">\n",
       "            early_stopping_rounds\n",
       "            <span class=\"param-doc-description\">early_stopping_rounds: typing.Optional[int]<br><br>.. versionadded:: 1.6.0<br><br>- Activates early stopping. Validation metric needs to improve at least once in<br>  every **early_stopping_rounds** round(s) to continue training.  Requires at<br>  least one item in **eval_set** in :py:meth:`fit`.<br><br>- If early stopping occurs, the model will have two additional attributes:<br>  :py:attr:`best_score` and :py:attr:`best_iteration`. These are used by the<br>  :py:meth:`predict` and :py:meth:`apply` methods to determine the optimal<br>  number of trees during inference. If users want to access the full model<br>  (including trees built after early stopping), they can specify the<br>  `iteration_range` in these inference methods. In addition, other utilities<br>  like model plotting can also use the entire model.<br><br>- If you prefer to discard the trees after `best_iteration`, consider using the<br>  callback function :py:class:`xgboost.callback.EarlyStopping`.<br><br>- If there's more than one item in **eval_set**, the last entry will be used for<br>  early stopping.  If there's more than one metric in **eval_metric**, the last<br>  metric will be used for early stopping.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('enable_categorical',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=enable_categorical,-bool\">\n",
       "            enable_categorical\n",
       "            <span class=\"param-doc-description\">enable_categorical: bool<br><br>See the same parameter of :py:class:`DMatrix` for details.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('eval_metric',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=eval_metric,-typing.Union%5Bstr%2C%20typing.List%5Btyping.Union%5Bstr%2C%20typing.Callable%5D%5D%2C%20typing.Callable%2C%20NoneType%5D\">\n",
       "            eval_metric\n",
       "            <span class=\"param-doc-description\">eval_metric: typing.Union[str, typing.List[typing.Union[str, typing.Callable]], typing.Callable, NoneType]<br><br>.. versionadded:: 1.6.0<br><br>Metric used for monitoring the training result and early stopping.  It can be a<br>string or list of strings as names of predefined metric in XGBoost (See<br>:doc:`/parameter`), one of the metrics in :py:mod:`sklearn.metrics`, or any<br>other user defined metric that looks like `sklearn.metrics`.<br><br>If custom objective is also provided, then custom metric should implement the<br>corresponding reverse link function.<br><br>Unlike the `scoring` parameter commonly used in scikit-learn, when a callable<br>object is provided, it's assumed to be a cost function and by default XGBoost<br>will minimize the result during early stopping.<br><br>For advanced usage on Early stopping like directly choosing to maximize instead<br>of minimize, see :py:obj:`xgboost.callback.EarlyStopping`.<br><br>See :doc:`/tutorials/custom_metric_obj` and :ref:`custom-obj-metric` for more<br>information.<br><br>.. code-block:: python<br><br>    from sklearn.datasets import load_diabetes<br>    from sklearn.metrics import mean_absolute_error<br>    X, y = load_diabetes(return_X_y=True)<br>    reg = xgb.XGBRegressor(<br>        tree_method=\"hist\",<br>        eval_metric=mean_absolute_error,<br>    )<br>    reg.fit(X, y, eval_set=[(X, y)])</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('feature_types',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=feature_types,-typing.Optional%5Btyping.Sequence%5Bstr%5D%5D\">\n",
       "            feature_types\n",
       "            <span class=\"param-doc-description\">feature_types: typing.Optional[typing.Sequence[str]]<br><br>.. versionadded:: 1.7.0<br><br>Used for specifying feature types without constructing a dataframe. See<br>the :py:class:`DMatrix` for details.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('feature_weights',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=feature_weights,-Optional%5BArrayLike%5D\">\n",
       "            feature_weights\n",
       "            <span class=\"param-doc-description\">feature_weights: Optional[ArrayLike]<br><br>Weight for each feature, defines the probability of each feature being selected<br>when colsample is being used.  All values must be greater than 0, otherwise a<br>`ValueError` is thrown.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('gamma',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=gamma,-typing.Optional%5Bfloat%5D\">\n",
       "            gamma\n",
       "            <span class=\"param-doc-description\">gamma: typing.Optional[float]<br><br>(min_split_loss) Minimum loss reduction required to make a further partition on<br>a leaf node of the tree.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('grow_policy',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=grow_policy,-typing.Optional%5Bstr%5D\">\n",
       "            grow_policy\n",
       "            <span class=\"param-doc-description\">grow_policy: typing.Optional[str]<br><br>Tree growing policy.<br><br>- depthwise: Favors splitting at nodes closest to the node,<br>- lossguide: Favors splitting at nodes with highest loss change.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('importance_type',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">importance_type</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('interaction_constraints',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=interaction_constraints,-typing.Union%5Bstr%2C%20typing.List%5Btyping.Tuple%5Bstr%5D%5D%2C%20NoneType%5D\">\n",
       "            interaction_constraints\n",
       "            <span class=\"param-doc-description\">interaction_constraints: typing.Union[str, typing.List[typing.Tuple[str]], NoneType]<br><br>Constraints for interaction representing permitted interactions.  The<br>constraints must be specified in the form of a nested list, e.g. ``[[0, 1], [2,<br>3, 4]]``, where each inner list is a group of indices of features that are<br>allowed to interact with each other.  See :doc:`tutorial<br></tutorials/feature_interaction_constraint>` for more information</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('learning_rate',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=learning_rate,-typing.Optional%5Bfloat%5D\">\n",
       "            learning_rate\n",
       "            <span class=\"param-doc-description\">learning_rate: typing.Optional[float]<br><br>Boosting learning rate (xgb's \"eta\")</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">0.1305406029660337</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_bin',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=max_bin,-typing.Optional%5Bint%5D\">\n",
       "            max_bin\n",
       "            <span class=\"param-doc-description\">max_bin: typing.Optional[int]<br><br>If using histogram-based algorithm, maximum number of bins per feature</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_cat_threshold',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=max_cat_threshold,-typing.Optional%5Bint%5D\">\n",
       "            max_cat_threshold\n",
       "            <span class=\"param-doc-description\">max_cat_threshold: typing.Optional[int]<br><br>.. versionadded:: 1.7.0<br><br>.. note:: This parameter is experimental<br><br>Maximum number of categories considered for each split. Used only by<br>partition-based splits for preventing over-fitting. Also, `enable_categorical`<br>needs to be set to have categorical feature support. See :doc:`Categorical Data<br></tutorials/categorical>` and :ref:`cat-param` for details.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_cat_to_onehot',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=max_cat_to_onehot,-Optional%5Bint%5D\">\n",
       "            max_cat_to_onehot\n",
       "            <span class=\"param-doc-description\">max_cat_to_onehot: Optional[int]<br><br>.. versionadded:: 1.6.0<br><br>.. note:: This parameter is experimental<br><br>A threshold for deciding whether XGBoost should use one-hot encoding based split<br>for categorical data.  When number of categories is lesser than the threshold<br>then one-hot encoding is chosen, otherwise the categories will be partitioned<br>into children nodes. Also, `enable_categorical` needs to be set to have<br>categorical feature support. See :doc:`Categorical Data<br></tutorials/categorical>` and :ref:`cat-param` for details.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_delta_step',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=max_delta_step,-typing.Optional%5Bfloat%5D\">\n",
       "            max_delta_step\n",
       "            <span class=\"param-doc-description\">max_delta_step: typing.Optional[float]<br><br>Maximum delta step we allow each tree's weight estimation to be.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_depth',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=max_depth,-%20typing.Optional%5Bint%5D\">\n",
       "            max_depth\n",
       "            <span class=\"param-doc-description\">max_depth:  typing.Optional[int]<br><br>Maximum tree depth for base learners.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">3</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_leaves',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=max_leaves,-typing.Optional%5Bint%5D\">\n",
       "            max_leaves\n",
       "            <span class=\"param-doc-description\">max_leaves: typing.Optional[int]<br><br>Maximum number of leaves; 0 indicates no limit.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('min_child_weight',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=min_child_weight,-typing.Optional%5Bfloat%5D\">\n",
       "            min_child_weight\n",
       "            <span class=\"param-doc-description\">min_child_weight: typing.Optional[float]<br><br>Minimum sum of instance weight(hessian) needed in a child.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">2</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('missing',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=missing,-float\">\n",
       "            missing\n",
       "            <span class=\"param-doc-description\">missing: float<br><br>Value in the data which needs to be present as a missing value. Default to<br>:py:data:`numpy.nan`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">nan</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('monotone_constraints',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=monotone_constraints,-typing.Union%5Btyping.Dict%5Bstr%2C%20int%5D%2C%20str%2C%20NoneType%5D\">\n",
       "            monotone_constraints\n",
       "            <span class=\"param-doc-description\">monotone_constraints: typing.Union[typing.Dict[str, int], str, NoneType]<br><br>Constraint of variable monotonicity.  See :doc:`tutorial </tutorials/monotonic>`<br>for more information.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('multi_strategy',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=multi_strategy,-typing.Optional%5Bstr%5D\">\n",
       "            multi_strategy\n",
       "            <span class=\"param-doc-description\">multi_strategy: typing.Optional[str]<br><br>.. versionadded:: 2.0.0<br><br>.. note:: This parameter is working-in-progress.<br><br>The strategy used for training multi-target models, including multi-target<br>regression and multi-class classification. See :doc:`/tutorials/multioutput` for<br>more information.<br><br>- ``one_output_per_tree``: One model for each target.<br>- ``multi_output_tree``:  Use multi-target trees.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_estimators',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=n_estimators,-typing.Optional%5Bint%5D\">\n",
       "            n_estimators\n",
       "            <span class=\"param-doc-description\">n_estimators: typing.Optional[int]<br><br>Number of gradient boosted trees.  Equivalent to number of boosting<br>rounds.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">708</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_jobs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=n_jobs,-typing.Optional%5Bint%5D\">\n",
       "            n_jobs\n",
       "            <span class=\"param-doc-description\">n_jobs: typing.Optional[int]<br><br>Number of parallel threads used to run xgboost.  When used with other<br>Scikit-Learn algorithms like grid search, you may choose which algorithm to<br>parallelize and balance the threads.  Creating thread contention will<br>significantly slow down both algorithms.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('num_parallel_tree',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">num_parallel_tree</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('random_state',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=random_state,-typing.Union%5Bnumpy.random.mtrand.RandomState%2C%20numpy.random._generator.Generator%2C%20int%2C%20NoneType%5D\">\n",
       "            random_state\n",
       "            <span class=\"param-doc-description\">random_state: typing.Union[numpy.random.mtrand.RandomState, numpy.random._generator.Generator, int, NoneType]<br><br>Random number seed.<br><br>.. note::<br><br>   Using gblinear booster with shotgun updater is nondeterministic as<br>   it uses Hogwild algorithm.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">42</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('reg_alpha',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=reg_alpha,-typing.Optional%5Bfloat%5D\">\n",
       "            reg_alpha\n",
       "            <span class=\"param-doc-description\">reg_alpha: typing.Optional[float]<br><br>L1 regularization term on weights (xgb's alpha).</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('reg_lambda',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=reg_lambda,-typing.Optional%5Bfloat%5D\">\n",
       "            reg_lambda\n",
       "            <span class=\"param-doc-description\">reg_lambda: typing.Optional[float]<br><br>L2 regularization term on weights (xgb's lambda).</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('sampling_method',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=sampling_method,-typing.Optional%5Bstr%5D\">\n",
       "            sampling_method\n",
       "            <span class=\"param-doc-description\">sampling_method: typing.Optional[str]<br><br>Sampling method. Used only by the GPU version of ``hist`` tree method.<br><br>- ``uniform``: Select random training instances uniformly.<br>- ``gradient_based``: Select random training instances with higher probability<br>    when the gradient and hessian are larger. (cf. CatBoost)</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('scale_pos_weight',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=scale_pos_weight,-typing.Optional%5Bfloat%5D\">\n",
       "            scale_pos_weight\n",
       "            <span class=\"param-doc-description\">scale_pos_weight: typing.Optional[float]<br><br>Balancing of positive and negative weights.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('subsample',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=subsample,-typing.Optional%5Bfloat%5D\">\n",
       "            subsample\n",
       "            <span class=\"param-doc-description\">subsample: typing.Optional[float]<br><br>Subsample ratio of the training instance.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('tree_method',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=tree_method,-typing.Optional%5Bstr%5D\">\n",
       "            tree_method\n",
       "            <span class=\"param-doc-description\">tree_method: typing.Optional[str]<br><br>Specify which tree method to use.  Default to auto.  If this parameter is set to<br>default, XGBoost will choose the most conservative option available.  It's<br>recommended to study this option from the parameters document :doc:`tree method<br></treemethod>`</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('validate_parameters',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=validate_parameters,-typing.Optional%5Bbool%5D\">\n",
       "            validate_parameters\n",
       "            <span class=\"param-doc-description\">validate_parameters: typing.Optional[bool]<br><br>Give warnings for unknown parameter.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbosity',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor#:~:text=verbosity,-typing.Optional%5Bint%5D\">\n",
       "            verbosity\n",
       "            <span class=\"param-doc-description\">verbosity: typing.Optional[int]<br><br>The degree of verbosity. Valid values are 0 (silent) - 3 (debug).</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.copy-paste-icon').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling\n",
       "        .textContent.trim().split(' ')[0];\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "\n",
       "\n",
       "/**\n",
       " * Adapted from Skrub\n",
       " * https://github.com/skrub-data/skrub/blob/403466d1d5d4dc76a7ef569b3f8228db59a31dc3/skrub/_reporting/_data/templates/report.js#L789\n",
       " * @returns \"light\" or \"dark\"\n",
       " */\n",
       "function detectTheme(element) {\n",
       "    const body = document.querySelector('body');\n",
       "\n",
       "    // Check VSCode theme\n",
       "    const themeKindAttr = body.getAttribute('data-vscode-theme-kind');\n",
       "    const themeNameAttr = body.getAttribute('data-vscode-theme-name');\n",
       "\n",
       "    if (themeKindAttr && themeNameAttr) {\n",
       "        const themeKind = themeKindAttr.toLowerCase();\n",
       "        const themeName = themeNameAttr.toLowerCase();\n",
       "\n",
       "        if (themeKind.includes(\"dark\") || themeName.includes(\"dark\")) {\n",
       "            return \"dark\";\n",
       "        }\n",
       "        if (themeKind.includes(\"light\") || themeName.includes(\"light\")) {\n",
       "            return \"light\";\n",
       "        }\n",
       "    }\n",
       "\n",
       "    // Check Jupyter theme\n",
       "    if (body.getAttribute('data-jp-theme-light') === 'false') {\n",
       "        return 'dark';\n",
       "    } else if (body.getAttribute('data-jp-theme-light') === 'true') {\n",
       "        return 'light';\n",
       "    }\n",
       "\n",
       "    // Guess based on a parent element's color\n",
       "    const color = window.getComputedStyle(element.parentNode, null).getPropertyValue('color');\n",
       "    const match = color.match(/^rgb\\s*\\(\\s*(\\d+)\\s*,\\s*(\\d+)\\s*,\\s*(\\d+)\\s*\\)\\s*$/i);\n",
       "    if (match) {\n",
       "        const [r, g, b] = [\n",
       "            parseFloat(match[1]),\n",
       "            parseFloat(match[2]),\n",
       "            parseFloat(match[3])\n",
       "        ];\n",
       "\n",
       "        // https://en.wikipedia.org/wiki/HSL_and_HSV#Lightness\n",
       "        const luma = 0.299 * r + 0.587 * g + 0.114 * b;\n",
       "\n",
       "        if (luma > 180) {\n",
       "            // If the text is very bright we have a dark theme\n",
       "            return 'dark';\n",
       "        }\n",
       "        if (luma < 75) {\n",
       "            // If the text is very dark we have a light theme\n",
       "            return 'light';\n",
       "        }\n",
       "        // Otherwise fall back to the next heuristic.\n",
       "    }\n",
       "\n",
       "    // Fallback to system preference\n",
       "    return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';\n",
       "}\n",
       "\n",
       "\n",
       "function forceTheme(elementId) {\n",
       "    const estimatorElement = document.querySelector(`#${elementId}`);\n",
       "    if (estimatorElement === null) {\n",
       "        console.error(`Element with id ${elementId} not found.`);\n",
       "    } else {\n",
       "        const theme = detectTheme(estimatorElement);\n",
       "        estimatorElement.classList.add(theme);\n",
       "    }\n",
       "}\n",
       "\n",
       "forceTheme('sk-container-id-2');</script></body>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             feature_weights=None, gamma=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=0.1305406029660337, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "             min_child_weight=2, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=708, n_jobs=None,\n",
       "             num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d53ab7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "080649c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2609738229239942"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc8a1ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9d7abb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = root_mean_squared_error(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d686727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "car-price-prediction-model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
